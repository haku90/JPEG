#include "xTransform.h"

namespace AVlib {

_declspec(align(32)) const int16 xTransform::m_TrM4x4[4][4] = 
{
  { 64, 64, 64, 64},
  { 83, 36,-36,-83},
  { 64,-64,-64, 64},
  { 36,-83, 83,-36}
};

_declspec(align(32)) const int16 xTransform::m_TrM8x8[8][8] =
{
  { 64, 64, 64, 64, 64, 64, 64, 64},
  { 89, 75, 50, 18,-18,-50,-75,-89},
  { 83, 36,-36,-83,-83,-36, 36, 83},
  { 75,-18,-89,-50, 50, 89, 18,-75},
  { 64,-64,-64, 64, 64,-64,-64, 64},
  { 50,-89, 18, 75,-75,-18, 89,-50},
  { 36,-83, 83,-36,-36, 83,-83, 36},
  { 18,-50, 75,-89, 89,-75, 50,-18}
};

_declspec(align(32)) const int16 xTransform::m_TrM16x16[16][16] =
{
  { 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64},
  { 90, 87, 80, 70, 57, 43, 25,  9, -9,-25,-43,-57,-70,-80,-87,-90},
  { 89, 75, 50, 18,-18,-50,-75,-89,-89,-75,-50,-18, 18, 50, 75, 89},
  { 87, 57,  9,-43,-80,-90,-70,-25, 25, 70, 90, 80, 43, -9,-57,-87},
  { 83, 36,-36,-83,-83,-36, 36, 83, 83, 36,-36,-83,-83,-36, 36, 83},
  { 80,  9,-70,-87,-25, 57, 90, 43,-43,-90,-57, 25, 87, 70, -9,-80},
  { 75,-18,-89,-50, 50, 89, 18,-75,-75, 18, 89, 50,-50,-89,-18, 75},
  { 70,-43,-87,  9, 90, 25,-80,-57, 57, 80,-25,-90, -9, 87, 43,-70},
  { 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64},
  { 57,-80,-25, 90, -9,-87, 43, 70,-70,-43, 87,  9,-90, 25, 80,-57},
  { 50,-89, 18, 75,-75,-18, 89,-50,-50, 89,-18,-75, 75, 18,-89, 50},
  { 43,-90, 57, 25,-87, 70,  9,-80, 80, -9,-70, 87,-25,-57, 90,-43},
  { 36,-83, 83,-36,-36, 83,-83, 36, 36,-83, 83,-36,-36, 83,-83, 36},
  { 25,-70, 90,-80, 43,  9,-57, 87,-87, 57, -9,-43, 80,-90, 70,-25},
  { 18,-50, 75,-89, 89,-75, 50,-18,-18, 50,-75, 89,-89, 75,-50, 18},
  {  9,-25, 43,-57, 70,-80, 87,-90, 90,-87, 80,-70, 57,-43, 25, -9}
};

_declspec(align(32)) const int16 xTransform::m_TrM32x32[32][32] =
{
  { 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64},
  { 90, 90, 88, 85, 82, 78, 73, 67, 61, 54, 46, 38, 31, 22, 13,  4, -4,-13,-22,-31,-38,-46,-54,-61,-67,-73,-78,-82,-85,-88,-90,-90},
  { 90, 87, 80, 70, 57, 43, 25,  9, -9,-25,-43,-57,-70,-80,-87,-90,-90,-87,-80,-70,-57,-43,-25, -9,  9, 25, 43, 57, 70, 80, 87, 90},
  { 90, 82, 67, 46, 22, -4,-31,-54,-73,-85,-90,-88,-78,-61,-38,-13, 13, 38, 61, 78, 88, 90, 85, 73, 54, 31,  4,-22,-46,-67,-82,-90},
  { 89, 75, 50, 18,-18,-50,-75,-89,-89,-75,-50,-18, 18, 50, 75, 89, 89, 75, 50, 18,-18,-50,-75,-89,-89,-75,-50,-18, 18, 50, 75, 89},
  { 88, 67, 31,-13,-54,-82,-90,-78,-46, -4, 38, 73, 90, 85, 61, 22,-22,-61,-85,-90,-73,-38,  4, 46, 78, 90, 82, 54, 13,-31,-67,-88},
  { 87, 57,  9,-43,-80,-90,-70,-25, 25, 70, 90, 80, 43, -9,-57,-87,-87,-57, -9, 43, 80, 90, 70, 25,-25,-70,-90,-80,-43,  9, 57, 87},
  { 85, 46,-13,-67,-90,-73,-22, 38, 82, 88, 54, -4,-61,-90,-78,-31, 31, 78, 90, 61,  4,-54,-88,-82,-38, 22, 73, 90, 67, 13,-46,-85},
  { 83, 36,-36,-83,-83,-36, 36, 83, 83, 36,-36,-83,-83,-36, 36, 83, 83, 36,-36,-83,-83,-36, 36, 83, 83, 36,-36,-83,-83,-36, 36, 83},
  { 82, 22,-54,-90,-61, 13, 78, 85, 31,-46,-90,-67,  4, 73, 88, 38,-38,-88,-73, -4, 67, 90, 46,-31,-85,-78,-13, 61, 90, 54,-22,-82},
  { 80,  9,-70,-87,-25, 57, 90, 43,-43,-90,-57, 25, 87, 70, -9,-80,-80, -9, 70, 87, 25,-57,-90,-43, 43, 90, 57,-25,-87,-70,  9, 80},
  { 78, -4,-82,-73, 13, 85, 67,-22,-88,-61, 31, 90, 54,-38,-90,-46, 46, 90, 38,-54,-90,-31, 61, 88, 22,-67,-85,-13, 73, 82,  4,-78},
  { 75,-18,-89,-50, 50, 89, 18,-75,-75, 18, 89, 50,-50,-89,-18, 75, 75,-18,-89,-50, 50, 89, 18,-75,-75, 18, 89, 50,-50,-89,-18, 75},
  { 73,-31,-90,-22, 78, 67,-38,-90,-13, 82, 61,-46,-88, -4, 85, 54,-54,-85,  4, 88, 46,-61,-82, 13, 90, 38,-67,-78, 22, 90, 31,-73},
  { 70,-43,-87,  9, 90, 25,-80,-57, 57, 80,-25,-90, -9, 87, 43,-70,-70, 43, 87, -9,-90,-25, 80, 57,-57,-80, 25, 90,  9,-87,-43, 70},
  { 67,-54,-78, 38, 85,-22,-90,  4, 90, 13,-88,-31, 82, 46,-73,-61, 61, 73,-46,-82, 31, 88,-13,-90, -4, 90, 22,-85,-38, 78, 54,-67},
  { 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64},
  { 61,-73,-46, 82, 31,-88,-13, 90, -4,-90, 22, 85,-38,-78, 54, 67,-67,-54, 78, 38,-85,-22, 90,  4,-90, 13, 88,-31,-82, 46, 73,-61},
  { 57,-80,-25, 90, -9,-87, 43, 70,-70,-43, 87,  9,-90, 25, 80,-57,-57, 80, 25,-90,  9, 87,-43,-70, 70, 43,-87, -9, 90,-25,-80, 57},
  { 54,-85, -4, 88,-46,-61, 82, 13,-90, 38, 67,-78,-22, 90,-31,-73, 73, 31,-90, 22, 78,-67,-38, 90,-13,-82, 61, 46,-88,  4, 85,-54},
  { 50,-89, 18, 75,-75,-18, 89,-50,-50, 89,-18,-75, 75, 18,-89, 50, 50,-89, 18, 75,-75,-18, 89,-50,-50, 89,-18,-75, 75, 18,-89, 50},
  { 46,-90, 38, 54,-90, 31, 61,-88, 22, 67,-85, 13, 73,-82,  4, 78,-78, -4, 82,-73,-13, 85,-67,-22, 88,-61,-31, 90,-54,-38, 90,-46},
  { 43,-90, 57, 25,-87, 70,  9,-80, 80, -9,-70, 87,-25,-57, 90,-43,-43, 90,-57,-25, 87,-70, -9, 80,-80,  9, 70,-87, 25, 57,-90, 43},
  { 38,-88, 73, -4,-67, 90,-46,-31, 85,-78, 13, 61,-90, 54, 22,-82, 82,-22,-54, 90,-61,-13, 78,-85, 31, 46,-90, 67,  4,-73, 88,-38},
  { 36,-83, 83,-36,-36, 83,-83, 36, 36,-83, 83,-36,-36, 83,-83, 36, 36,-83, 83,-36,-36, 83,-83, 36, 36,-83, 83,-36,-36, 83,-83, 36},
  { 31,-78, 90,-61,  4, 54,-88, 82,-38,-22, 73,-90, 67,-13,-46, 85,-85, 46, 13,-67, 90,-73, 22, 38,-82, 88,-54, -4, 61,-90, 78,-31},
  { 25,-70, 90,-80, 43,  9,-57, 87,-87, 57, -9,-43, 80,-90, 70,-25,-25, 70,-90, 80,-43, -9, 57,-87, 87,-57,  9, 43,-80, 90,-70, 25},
  { 22,-61, 85,-90, 73,-38, -4, 46,-78, 90,-82, 54,-13,-31, 67,-88, 88,-67, 31, 13,-54, 82,-90, 78,-46,  4, 38,-73, 90,-85, 61,-22},
  { 18,-50, 75,-89, 89,-75, 50,-18,-18, 50,-75, 89,-89, 75,-50, 18, 18,-50, 75,-89, 89,-75, 50,-18,-18, 50,-75, 89,-89, 75,-50, 18},
  { 13,-38, 61,-78, 88,-90, 85,-73, 54,-31,  4, 22,-46, 67,-82, 90,-90, 82,-67, 46,-22, -4, 31,-54, 73,-85, 90,-88, 78,-61, 38,-13},
  {  9,-25, 43,-57, 70,-80, 87,-90, 90,-87, 80,-70, 57,-43, 25, -9, -9, 25,-43, 57,-70, 80,-87, 90,-90, 87,-80, 70,-57, 43,-25,  9},
  {  4,-13, 22,-31, 38,-46, 54,-61, 67,-73, 78,-82, 85,-88, 90,-90, 90,-90, 88,-85, 82,-78, 73,-67, 61,-54, 46,-38, 31,-22, 13, -4}
};

_declspec(align(32)) const int16 xTransform::m_TrM64x64[64][64] = 
{
  { 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64},
  { 91, 90, 90, 89, 88, 87, 86, 84, 83, 81, 79, 76, 74, 71, 69, 66, 62, 59, 56, 52, 48, 45, 41, 37, 33, 28, 24, 20, 16, 11,  7,  2, -2, -7,-11,-16,-20,-24,-28,-33,-37,-41,-45,-48,-52,-56,-59,-62,-66,-69,-71,-74,-76,-79,-81,-83,-84,-86,-87,-88,-89,-90,-90,-91},
  { 90, 90, 88, 85, 82, 78, 73, 67, 61, 54, 46, 38, 31, 22, 13,  4, -4,-13,-22,-31,-38,-46,-54,-61,-67,-73,-78,-82,-85,-88,-90,-90,-90,-90,-88,-85,-82,-78,-73,-67,-61,-54,-46,-38,-31,-22,-13, -4,  4, 13, 22, 31, 38, 46, 54, 61, 67, 73, 78, 82, 85, 88, 90, 90},
  { 90, 88, 84, 79, 71, 62, 52, 41, 28, 16,  2,-11,-24,-37,-48,-59,-69,-76,-83,-87,-90,-91,-89,-86,-81,-74,-66,-56,-45,-33,-20, -7,  7, 20, 33, 45, 56, 66, 74, 81, 86, 89, 91, 90, 87, 83, 76, 69, 59, 48, 37, 24, 11, -2,-16,-28,-41,-52,-62,-71,-79,-84,-88,-90},
  { 90, 87, 80, 70, 57, 43, 25,  9, -9,-25,-43,-57,-70,-80,-87,-90,-90,-87,-80,-70,-57,-43,-25, -9,  9, 25, 43, 57, 70, 80, 87, 90, 90, 87, 80, 70, 57, 43, 25,  9, -9,-25,-43,-57,-70,-80,-87,-90,-90,-87,-80,-70,-57,-43,-25, -9,  9, 25, 43, 57, 70, 80, 87, 90},
  { 90, 84, 74, 59, 41, 20, -2,-24,-45,-62,-76,-86,-90,-89,-83,-71,-56,-37,-16,  7, 28, 48, 66, 79, 87, 91, 88, 81, 69, 52, 33, 11,-11,-33,-52,-69,-81,-88,-91,-87,-79,-66,-48,-28, -7, 16, 37, 56, 71, 83, 89, 90, 86, 76, 62, 45, 24,  2,-20,-41,-59,-74,-84,-90},
  { 90, 82, 67, 46, 22, -4,-31,-54,-73,-85,-90,-88,-78,-61,-38,-13, 13, 38, 61, 78, 88, 90, 85, 73, 54, 31,  4,-22,-46,-67,-82,-90,-90,-82,-67,-46,-22,  4, 31, 54, 73, 85, 90, 88, 78, 61, 38, 13,-13,-38,-61,-78,-88,-90,-85,-73,-54,-31, -4, 22, 46, 67, 82, 90},
  { 89, 79, 59, 33,  2,-28,-56,-76,-88,-90,-81,-62,-37, -7, 24, 52, 74, 87, 90, 83, 66, 41, 11,-20,-48,-71,-86,-91,-84,-69,-45,-16, 16, 45, 69, 84, 91, 86, 71, 48, 20,-11,-41,-66,-83,-90,-87,-74,-52,-24,  7, 37, 62, 81, 90, 88, 76, 56, 28, -2,-33,-59,-79,-89},
  { 89, 75, 50, 18,-18,-50,-75,-89,-89,-75,-50,-18, 18, 50, 75, 89, 89, 75, 50, 18,-18,-50,-75,-89,-89,-75,-50,-18, 18, 50, 75, 89, 89, 75, 50, 18,-18,-50,-75,-89,-89,-75,-50,-18, 18, 50, 75, 89, 89, 75, 50, 18,-18,-50,-75,-89,-89,-75,-50,-18, 18, 50, 75, 89},
  { 88, 71, 41,  2,-37,-69,-87,-89,-74,-45, -7, 33, 66, 86, 90, 76, 48, 11,-28,-62,-84,-90,-79,-52,-16, 24, 59, 83, 91, 81, 56, 20,-20,-56,-81,-91,-83,-59,-24, 16, 52, 79, 90, 84, 62, 28,-11,-48,-76,-90,-86,-66,-33,  7, 45, 74, 89, 87, 69, 37, -2,-41,-71,-88},
  { 88, 67, 31,-13,-54,-82,-90,-78,-46, -4, 38, 73, 90, 85, 61, 22,-22,-61,-85,-90,-73,-38,  4, 46, 78, 90, 82, 54, 13,-31,-67,-88,-88,-67,-31, 13, 54, 82, 90, 78, 46,  4,-38,-73,-90,-85,-61,-22, 22, 61, 85, 90, 73, 38, -4,-46,-78,-90,-82,-54,-13, 31, 67, 88},
  { 87, 62, 20,-28,-69,-89,-84,-56,-11, 37, 74, 90, 81, 48,  2,-45,-79,-91,-76,-41,  7, 52, 83, 90, 71, 33,-16,-59,-86,-88,-66,-24, 24, 66, 88, 86, 59, 16,-33,-71,-90,-83,-52, -7, 41, 76, 91, 79, 45, -2,-48,-81,-90,-74,-37, 11, 56, 84, 89, 69, 28,-20,-62,-87},
  { 87, 57,  9,-43,-80,-90,-70,-25, 25, 70, 90, 80, 43, -9,-57,-87,-87,-57, -9, 43, 80, 90, 70, 25,-25,-70,-90,-80,-43,  9, 57, 87, 87, 57,  9,-43,-80,-90,-70,-25, 25, 70, 90, 80, 43, -9,-57,-87,-87,-57, -9, 43, 80, 90, 70, 25,-25,-70,-90,-80,-43,  9, 57, 87},
  { 86, 52, -2,-56,-87,-84,-48,  7, 59, 88, 83, 45,-11,-62,-89,-81,-41, 16, 66, 90, 79, 37,-20,-69,-90,-76,-33, 24, 71, 91, 74, 28,-28,-74,-91,-71,-24, 33, 76, 90, 69, 20,-37,-79,-90,-66,-16, 41, 81, 89, 62, 11,-45,-83,-88,-59, -7, 48, 84, 87, 56,  2,-52,-86},
  { 85, 46,-13,-67,-90,-73,-22, 38, 82, 88, 54, -4,-61,-90,-78,-31, 31, 78, 90, 61,  4,-54,-88,-82,-38, 22, 73, 90, 67, 13,-46,-85,-85,-46, 13, 67, 90, 73, 22,-38,-82,-88,-54,  4, 61, 90, 78, 31,-31,-78,-90,-61, -4, 54, 88, 82, 38,-22,-73,-90,-67,-13, 46, 85},
  { 84, 41,-24,-76,-89,-56,  7, 66, 91, 69, 11,-52,-88,-79,-28, 37, 83, 86, 45,-20,-74,-90,-59,  2, 62, 90, 71, 16,-48,-87,-81,-33, 33, 81, 87, 48,-16,-71,-90,-62, -2, 59, 90, 74, 20,-45,-86,-83,-37, 28, 79, 88, 52,-11,-69,-91,-66, -7, 56, 89, 76, 24,-41,-84},
  { 83, 36,-36,-83,-83,-36, 36, 83, 83, 36,-36,-83,-83,-36, 36, 83, 83, 36,-36,-83,-83,-36, 36, 83, 83, 36,-36,-83,-83,-36, 36, 83, 83, 36,-36,-83,-83,-36, 36, 83, 83, 36,-36,-83,-83,-36, 36, 83, 83, 36,-36,-83,-83,-36, 36, 83, 83, 36,-36,-83,-83,-36, 36, 83},
  { 83, 28,-45,-88,-74,-11, 59, 91, 62, -7,-71,-89,-48, 24, 81, 84, 33,-41,-87,-76,-16, 56, 90, 66, -2,-69,-90,-52, 20, 79, 86, 37,-37,-86,-79,-20, 52, 90, 69,  2,-66,-90,-56, 16, 76, 87, 41,-33,-84,-81,-24, 48, 89, 71,  7,-62,-91,-59, 11, 74, 88, 45,-28,-83},
  { 82, 22,-54,-90,-61, 13, 78, 85, 31,-46,-90,-67,  4, 73, 88, 38,-38,-88,-73, -4, 67, 90, 46,-31,-85,-78,-13, 61, 90, 54,-22,-82,-82,-22, 54, 90, 61,-13,-78,-85,-31, 46, 90, 67, -4,-73,-88,-38, 38, 88, 73,  4,-67,-90,-46, 31, 85, 78, 13,-61,-90,-54, 22, 82},
  { 81, 16,-62,-90,-45, 37, 88, 69, -7,-76,-84,-24, 56, 91, 52,-28,-86,-74, -2, 71, 87, 33,-48,-90,-59, 20, 83, 79, 11,-66,-89,-41, 41, 89, 66,-11,-79,-83,-20, 59, 90, 48,-33,-87,-71,  2, 74, 86, 28,-52,-91,-56, 24, 84, 76,  7,-69,-88,-37, 45, 90, 62,-16,-81},
  { 80,  9,-70,-87,-25, 57, 90, 43,-43,-90,-57, 25, 87, 70, -9,-80,-80, -9, 70, 87, 25,-57,-90,-43, 43, 90, 57,-25,-87,-70,  9, 80, 80,  9,-70,-87,-25, 57, 90, 43,-43,-90,-57, 25, 87, 70, -9,-80,-80, -9, 70, 87, 25,-57,-90,-43, 43, 90, 57,-25,-87,-70,  9, 80},
  { 79,  2,-76,-81, -7, 74, 83, 11,-71,-84,-16, 69, 86, 20,-66,-87,-24, 62, 88, 28,-59,-89,-33, 56, 90, 37,-52,-90,-41, 48, 91, 45,-45,-91,-48, 41, 90, 52,-37,-90,-56, 33, 89, 59,-28,-88,-62, 24, 87, 66,-20,-86,-69, 16, 84, 71,-11,-83,-74,  7, 81, 76, -2,-79},
  { 78, -4,-82,-73, 13, 85, 67,-22,-88,-61, 31, 90, 54,-38,-90,-46, 46, 90, 38,-54,-90,-31, 61, 88, 22,-67,-85,-13, 73, 82,  4,-78,-78,  4, 82, 73,-13,-85,-67, 22, 88, 61,-31,-90,-54, 38, 90, 46,-46,-90,-38, 54, 90, 31,-61,-88,-22, 67, 85, 13,-73,-82, -4, 78},
  { 76,-11,-86,-62, 33, 90, 45,-52,-89,-24, 69, 83,  2,-81,-71, 20, 88, 56,-41,-91,-37, 59, 87, 16,-74,-79,  7, 84, 66,-28,-90,-48, 48, 90, 28,-66,-84, -7, 79, 74,-16,-87,-59, 37, 91, 41,-56,-88,-20, 71, 81, -2,-83,-69, 24, 89, 52,-45,-90,-33, 62, 86, 11,-76},
  { 75,-18,-89,-50, 50, 89, 18,-75,-75, 18, 89, 50,-50,-89,-18, 75, 75,-18,-89,-50, 50, 89, 18,-75,-75, 18, 89, 50,-50,-89,-18, 75, 75,-18,-89,-50, 50, 89, 18,-75,-75, 18, 89, 50,-50,-89,-18, 75, 75,-18,-89,-50, 50, 89, 18,-75,-75, 18, 89, 50,-50,-89,-18, 75},
  { 74,-24,-90,-37, 66, 81,-11,-88,-48, 56, 86,  2,-84,-59, 45, 89, 16,-79,-69, 33, 91, 28,-71,-76, 20, 90, 41,-62,-83,  7, 87, 52,-52,-87, -7, 83, 62,-41,-90,-20, 76, 71,-28,-91,-33, 69, 79,-16,-89,-45, 59, 84, -2,-86,-56, 48, 88, 11,-81,-66, 37, 90, 24,-74},
  { 73,-31,-90,-22, 78, 67,-38,-90,-13, 82, 61,-46,-88, -4, 85, 54,-54,-85,  4, 88, 46,-61,-82, 13, 90, 38,-67,-78, 22, 90, 31,-73,-73, 31, 90, 22,-78,-67, 38, 90, 13,-82,-61, 46, 88,  4,-85,-54, 54, 85, -4,-88,-46, 61, 82,-13,-90,-38, 67, 78,-22,-90,-31, 73},
  { 71,-37,-89, -7, 86, 48,-62,-79, 24, 91, 20,-81,-59, 52, 84,-11,-90,-33, 74, 69,-41,-88, -2, 87, 45,-66,-76, 28, 90, 16,-83,-56, 56, 83,-16,-90,-28, 76, 66,-45,-87,  2, 88, 41,-69,-74, 33, 90, 11,-84,-52, 59, 81,-20,-91,-24, 79, 62,-48,-86,  7, 89, 37,-71},
  { 70,-43,-87,  9, 90, 25,-80,-57, 57, 80,-25,-90, -9, 87, 43,-70,-70, 43, 87, -9,-90,-25, 80, 57,-57,-80, 25, 90,  9,-87,-43, 70, 70,-43,-87,  9, 90, 25,-80,-57, 57, 80,-25,-90, -9, 87, 43,-70,-70, 43, 87, -9,-90,-25, 80, 57,-57,-80, 25, 90,  9,-87,-43, 70},
  { 69,-48,-83, 24, 90,  2,-89,-28, 81, 52,-66,-71, 45, 84,-20,-90, -7, 88, 33,-79,-56, 62, 74,-41,-86, 16, 91, 11,-87,-37, 76, 59,-59,-76, 37, 87,-11,-91,-16, 86, 41,-74,-62, 56, 79,-33,-88,  7, 90, 20,-84,-45, 71, 66,-52,-81, 28, 89, -2,-90,-24, 83, 48,-69},
  { 67,-54,-78, 38, 85,-22,-90,  4, 90, 13,-88,-31, 82, 46,-73,-61, 61, 73,-46,-82, 31, 88,-13,-90, -4, 90, 22,-85,-38, 78, 54,-67,-67, 54, 78,-38,-85, 22, 90, -4,-90,-13, 88, 31,-82,-46, 73, 61,-61,-73, 46, 82,-31,-88, 13, 90,  4,-90,-22, 85, 38,-78,-54, 67},
  { 66,-59,-71, 52, 76,-45,-81, 37, 84,-28,-87, 20, 89,-11,-90,  2, 91,  7,-90,-16, 88, 24,-86,-33, 83, 41,-79,-48, 74, 56,-69,-62, 62, 69,-56,-74, 48, 79,-41,-83, 33, 86,-24,-88, 16, 90, -7,-91, -2, 90, 11,-89,-20, 87, 28,-84,-37, 81, 45,-76,-52, 71, 59,-66},
  { 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64, 64,-64,-64, 64},
  { 62,-69,-56, 74, 48,-79,-41, 83, 33,-86,-24, 88, 16,-90, -7, 91, -2,-90, 11, 89,-20,-87, 28, 84,-37,-81, 45, 76,-52,-71, 59, 66,-66,-59, 71, 52,-76,-45, 81, 37,-84,-28, 87, 20,-89,-11, 90,  2,-91,  7, 90,-16,-88, 24, 86,-33,-83, 41, 79,-48,-74, 56, 69,-62},
  { 61,-73,-46, 82, 31,-88,-13, 90, -4,-90, 22, 85,-38,-78, 54, 67,-67,-54, 78, 38,-85,-22, 90,  4,-90, 13, 88,-31,-82, 46, 73,-61,-61, 73, 46,-82,-31, 88, 13,-90,  4, 90,-22,-85, 38, 78,-54,-67, 67, 54,-78,-38, 85, 22,-90, -4, 90,-13,-88, 31, 82,-46,-73, 61},
  { 59,-76,-37, 87, 11,-91, 16, 86,-41,-74, 62, 56,-79,-33, 88,  7,-90, 20, 84,-45,-71, 66, 52,-81,-28, 89,  2,-90, 24, 83,-48,-69, 69, 48,-83,-24, 90, -2,-89, 28, 81,-52,-66, 71, 45,-84,-20, 90, -7,-88, 33, 79,-56,-62, 74, 41,-86,-16, 91,-11,-87, 37, 76,-59},
  { 57,-80,-25, 90, -9,-87, 43, 70,-70,-43, 87,  9,-90, 25, 80,-57,-57, 80, 25,-90,  9, 87,-43,-70, 70, 43,-87, -9, 90,-25,-80, 57, 57,-80,-25, 90, -9,-87, 43, 70,-70,-43, 87,  9,-90, 25, 80,-57,-57, 80, 25,-90,  9, 87,-43,-70, 70, 43,-87, -9, 90,-25,-80, 57},
  { 56,-83,-16, 90,-28,-76, 66, 45,-87, -2, 88,-41,-69, 74, 33,-90, 11, 84,-52,-59, 81, 20,-91, 24, 79,-62,-48, 86,  7,-89, 37, 71,-71,-37, 89, -7,-86, 48, 62,-79,-24, 91,-20,-81, 59, 52,-84,-11, 90,-33,-74, 69, 41,-88,  2, 87,-45,-66, 76, 28,-90, 16, 83,-56},
  { 54,-85, -4, 88,-46,-61, 82, 13,-90, 38, 67,-78,-22, 90,-31,-73, 73, 31,-90, 22, 78,-67,-38, 90,-13,-82, 61, 46,-88,  4, 85,-54,-54, 85,  4,-88, 46, 61,-82,-13, 90,-38,-67, 78, 22,-90, 31, 73,-73,-31, 90,-22,-78, 67, 38,-90, 13, 82,-61,-46, 88, -4,-85, 54},
  { 52,-87,  7, 83,-62,-41, 90,-20,-76, 71, 28,-91, 33, 69,-79,-16, 89,-45,-59, 84,  2,-86, 56, 48,-88, 11, 81,-66,-37, 90,-24,-74, 74, 24,-90, 37, 66,-81,-11, 88,-48,-56, 86, -2,-84, 59, 45,-89, 16, 79,-69,-33, 91,-28,-71, 76, 20,-90, 41, 62,-83, -7, 87,-52},
  { 50,-89, 18, 75,-75,-18, 89,-50,-50, 89,-18,-75, 75, 18,-89, 50, 50,-89, 18, 75,-75,-18, 89,-50,-50, 89,-18,-75, 75, 18,-89, 50, 50,-89, 18, 75,-75,-18, 89,-50,-50, 89,-18,-75, 75, 18,-89, 50, 50,-89, 18, 75,-75,-18, 89,-50,-50, 89,-18,-75, 75, 18,-89, 50},
  { 48,-90, 28, 66,-84,  7, 79,-74,-16, 87,-59,-37, 91,-41,-56, 88,-20,-71, 81,  2,-83, 69, 24,-89, 52, 45,-90, 33, 62,-86, 11, 76,-76,-11, 86,-62,-33, 90,-45,-52, 89,-24,-69, 83, -2,-81, 71, 20,-88, 56, 41,-91, 37, 59,-87, 16, 74,-79, -7, 84,-66,-28, 90,-48},
  { 46,-90, 38, 54,-90, 31, 61,-88, 22, 67,-85, 13, 73,-82,  4, 78,-78, -4, 82,-73,-13, 85,-67,-22, 88,-61,-31, 90,-54,-38, 90,-46,-46, 90,-38,-54, 90,-31,-61, 88,-22,-67, 85,-13,-73, 82, -4,-78, 78,  4,-82, 73, 13,-85, 67, 22,-88, 61, 31,-90, 54, 38,-90, 46},
  { 45,-91, 48, 41,-90, 52, 37,-90, 56, 33,-89, 59, 28,-88, 62, 24,-87, 66, 20,-86, 69, 16,-84, 71, 11,-83, 74,  7,-81, 76,  2,-79, 79, -2,-76, 81, -7,-74, 83,-11,-71, 84,-16,-69, 86,-20,-66, 87,-24,-62, 88,-28,-59, 89,-33,-56, 90,-37,-52, 90,-41,-48, 91,-45},
  { 43,-90, 57, 25,-87, 70,  9,-80, 80, -9,-70, 87,-25,-57, 90,-43,-43, 90,-57,-25, 87,-70, -9, 80,-80,  9, 70,-87, 25, 57,-90, 43, 43,-90, 57, 25,-87, 70,  9,-80, 80, -9,-70, 87,-25,-57, 90,-43,-43, 90,-57,-25, 87,-70, -9, 80,-80,  9, 70,-87, 25, 57,-90, 43},
  { 41,-89, 66, 11,-79, 83,-20,-59, 90,-48,-33, 87,-71, -2, 74,-86, 28, 52,-91, 56, 24,-84, 76, -7,-69, 88,-37,-45, 90,-62,-16, 81,-81, 16, 62,-90, 45, 37,-88, 69,  7,-76, 84,-24,-56, 91,-52,-28, 86,-74,  2, 71,-87, 33, 48,-90, 59, 20,-83, 79,-11,-66, 89,-41},
  { 38,-88, 73, -4,-67, 90,-46,-31, 85,-78, 13, 61,-90, 54, 22,-82, 82,-22,-54, 90,-61,-13, 78,-85, 31, 46,-90, 67,  4,-73, 88,-38,-38, 88,-73,  4, 67,-90, 46, 31,-85, 78,-13,-61, 90,-54,-22, 82,-82, 22, 54,-90, 61, 13,-78, 85,-31,-46, 90,-67, -4, 73,-88, 38},
  { 37,-86, 79,-20,-52, 90,-69,  2, 66,-90, 56, 16,-76, 87,-41,-33, 84,-81, 24, 48,-89, 71, -7,-62, 91,-59,-11, 74,-88, 45, 28,-83, 83,-28,-45, 88,-74, 11, 59,-91, 62,  7,-71, 89,-48,-24, 81,-84, 33, 41,-87, 76,-16,-56, 90,-66, -2, 69,-90, 52, 20,-79, 86,-37},
  { 36,-83, 83,-36,-36, 83,-83, 36, 36,-83, 83,-36,-36, 83,-83, 36, 36,-83, 83,-36,-36, 83,-83, 36, 36,-83, 83,-36,-36, 83,-83, 36, 36,-83, 83,-36,-36, 83,-83, 36, 36,-83, 83,-36,-36, 83,-83, 36, 36,-83, 83,-36,-36, 83,-83, 36, 36,-83, 83,-36,-36, 83,-83, 36},
  { 33,-81, 87,-48,-16, 71,-90, 62, -2,-59, 90,-74, 20, 45,-86, 83,-37,-28, 79,-88, 52, 11,-69, 91,-66,  7, 56,-89, 76,-24,-41, 84,-84, 41, 24,-76, 89,-56, -7, 66,-91, 69,-11,-52, 88,-79, 28, 37,-83, 86,-45,-20, 74,-90, 59,  2,-62, 90,-71, 16, 48,-87, 81,-33},
  { 31,-78, 90,-61,  4, 54,-88, 82,-38,-22, 73,-90, 67,-13,-46, 85,-85, 46, 13,-67, 90,-73, 22, 38,-82, 88,-54, -4, 61,-90, 78,-31,-31, 78,-90, 61, -4,-54, 88,-82, 38, 22,-73, 90,-67, 13, 46,-85, 85,-46,-13, 67,-90, 73,-22,-38, 82,-88, 54,  4,-61, 90,-78, 31},
  { 28,-74, 91,-71, 24, 33,-76, 90,-69, 20, 37,-79, 90,-66, 16, 41,-81, 89,-62, 11, 45,-83, 88,-59,  7, 48,-84, 87,-56,  2, 52,-86, 86,-52, -2, 56,-87, 84,-48, -7, 59,-88, 83,-45,-11, 62,-89, 81,-41,-16, 66,-90, 79,-37,-20, 69,-90, 76,-33,-24, 71,-91, 74,-28},
  { 25,-70, 90,-80, 43,  9,-57, 87,-87, 57, -9,-43, 80,-90, 70,-25,-25, 70,-90, 80,-43, -9, 57,-87, 87,-57,  9, 43,-80, 90,-70, 25, 25,-70, 90,-80, 43,  9,-57, 87,-87, 57, -9,-43, 80,-90, 70,-25,-25, 70,-90, 80,-43, -9, 57,-87, 87,-57,  9, 43,-80, 90,-70, 25},
  { 24,-66, 88,-86, 59,-16,-33, 71,-90, 83,-52,  7, 41,-76, 91,-79, 45,  2,-48, 81,-90, 74,-37,-11, 56,-84, 89,-69, 28, 20,-62, 87,-87, 62,-20,-28, 69,-89, 84,-56, 11, 37,-74, 90,-81, 48, -2,-45, 79,-91, 76,-41, -7, 52,-83, 90,-71, 33, 16,-59, 86,-88, 66,-24},
  { 22,-61, 85,-90, 73,-38, -4, 46,-78, 90,-82, 54,-13,-31, 67,-88, 88,-67, 31, 13,-54, 82,-90, 78,-46,  4, 38,-73, 90,-85, 61,-22,-22, 61,-85, 90,-73, 38,  4,-46, 78,-90, 82,-54, 13, 31,-67, 88,-88, 67,-31,-13, 54,-82, 90,-78, 46, -4,-38, 73,-90, 85,-61, 22},
  { 20,-56, 81,-91, 83,-59, 24, 16,-52, 79,-90, 84,-62, 28, 11,-48, 76,-90, 86,-66, 33,  7,-45, 74,-89, 87,-69, 37,  2,-41, 71,-88, 88,-71, 41, -2,-37, 69,-87, 89,-74, 45, -7,-33, 66,-86, 90,-76, 48,-11,-28, 62,-84, 90,-79, 52,-16,-24, 59,-83, 91,-81, 56,-20},
  { 18,-50, 75,-89, 89,-75, 50,-18,-18, 50,-75, 89,-89, 75,-50, 18, 18,-50, 75,-89, 89,-75, 50,-18,-18, 50,-75, 89,-89, 75,-50, 18, 18,-50, 75,-89, 89,-75, 50,-18,-18, 50,-75, 89,-89, 75,-50, 18, 18,-50, 75,-89, 89,-75, 50,-18,-18, 50,-75, 89,-89, 75,-50, 18},
  { 16,-45, 69,-84, 91,-86, 71,-48, 20, 11,-41, 66,-83, 90,-87, 74,-52, 24,  7,-37, 62,-81, 90,-88, 76,-56, 28,  2,-33, 59,-79, 89,-89, 79,-59, 33, -2,-28, 56,-76, 88,-90, 81,-62, 37, -7,-24, 52,-74, 87,-90, 83,-66, 41,-11,-20, 48,-71, 86,-91, 84,-69, 45,-16},
  { 13,-38, 61,-78, 88,-90, 85,-73, 54,-31,  4, 22,-46, 67,-82, 90,-90, 82,-67, 46,-22, -4, 31,-54, 73,-85, 90,-88, 78,-61, 38,-13,-13, 38,-61, 78,-88, 90,-85, 73,-54, 31, -4,-22, 46,-67, 82,-90, 90,-82, 67,-46, 22,  4,-31, 54,-73, 85,-90, 88,-78, 61,-38, 13},
  { 11,-33, 52,-69, 81,-88, 91,-87, 79,-66, 48,-28,  7, 16,-37, 56,-71, 83,-89, 90,-86, 76,-62, 45,-24,  2, 20,-41, 59,-74, 84,-90, 90,-84, 74,-59, 41,-20, -2, 24,-45, 62,-76, 86,-90, 89,-83, 71,-56, 37,-16, -7, 28,-48, 66,-79, 87,-91, 88,-81, 69,-52, 33,-11},
  {  9,-25, 43,-57, 70,-80, 87,-90, 90,-87, 80,-70, 57,-43, 25, -9, -9, 25,-43, 57,-70, 80,-87, 90,-90, 87,-80, 70,-57, 43,-25,  9,  9,-25, 43,-57, 70,-80, 87,-90, 90,-87, 80,-70, 57,-43, 25, -9, -9, 25,-43, 57,-70, 80,-87, 90,-90, 87,-80, 70,-57, 43,-25,  9},
  {  7,-20, 33,-45, 56,-66, 74,-81, 86,-89, 91,-90, 87,-83, 76,-69, 59,-48, 37,-24, 11,  2,-16, 28,-41, 52,-62, 71,-79, 84,-88, 90,-90, 88,-84, 79,-71, 62,-52, 41,-28, 16, -2,-11, 24,-37, 48,-59, 69,-76, 83,-87, 90,-91, 89,-86, 81,-74, 66,-56, 45,-33, 20, -7},
  {  4,-13, 22,-31, 38,-46, 54,-61, 67,-73, 78,-82, 85,-88, 90,-90, 90,-90, 88,-85, 82,-78, 73,-67, 61,-54, 46,-38, 31,-22, 13, -4, -4, 13,-22, 31,-38, 46,-54, 61,-67, 73,-78, 82,-85, 88,-90, 90,-90, 90,-88, 85,-82, 78,-73, 67,-61, 54,-46, 38,-31, 22,-13,  4},
  {  2, -7, 11,-16, 20,-24, 28,-33, 37,-41, 45,-48, 52,-56, 59,-62, 66,-69, 71,-74, 76,-79, 81,-83, 84,-86, 87,-88, 89,-90, 90,-91, 91,-90, 90,-89, 88,-87, 86,-84, 83,-81, 79,-76, 74,-71, 69,-66, 62,-59, 56,-52, 48,-45, 41,-37, 33,-28, 24,-20, 16,-11,  7, -2}
};

void xTransform::TransformHadamard_4x4_STD(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 DstStride)
{
  int32 B[4];
  int32 Tmp[4][4];
  int32 DstStride_x3 = DstStride + (DstStride<<1);

  for(int32 j=0; j<4; j++) //horizontal transform
  {
    B[0] = Src[2] + Src[0];
    B[1] = Src[3] + Src[1];
    B[2] = Src[0] - Src[2];
    B[3] = Src[1] - Src[3];
    Src += SrcStride;

    Tmp[0][j] = B[1] + B[0]; //store with transposition
    Tmp[1][j] = B[0] - B[1];
    Tmp[2][j] = B[3] + B[2];
    Tmp[3][j] = B[2] - B[3];
  }

  for(int32 j=0; j<4; j++) //vertical transform
  {
    B[0] = Tmp[j][2] + Tmp[j][0];
    B[1] = Tmp[j][3] + Tmp[j][1];
    B[2] = Tmp[j][0] - Tmp[j][2];
    B[3] = Tmp[j][1] - Tmp[j][3];

    Dst[0           ] = (B[1] + B[0])<<3; //store with transposition
    Dst[DstStride   ] = (B[0] - B[1])<<3;
    Dst[DstStride<<1] = (B[3] + B[2])<<3;
    Dst[DstStride_x3] = (B[2] - B[3])<<3;
    Dst++;
  }
}
void xTransform::TransformHadamard_8x8_STD(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 DstStride)
{
  int32 B[8],C[8];
  int32 Tmp[8][8];

  for(int32 j=0; j<8; j++) //horizontal transform
  {
    B[0] = Src[4] + Src[0];
    B[1] = Src[5] + Src[1];
    B[2] = Src[6] + Src[2];
    B[3] = Src[7] + Src[3];
    B[4] = Src[0] - Src[4];
    B[5] = Src[1] - Src[5];
    B[6] = Src[2] - Src[6];
    B[7] = Src[3] - Src[7];
    Src += SrcStride;

    C[0] = B[2] + B[0];
    C[1] = B[3] + B[1];
    C[2] = B[0] - B[2];
    C[3] = B[1] - B[3];
    C[4] = B[6] + B[4];
    C[5] = B[7] + B[5];
    C[6] = B[4] - B[6];
    C[7] = B[5] - B[7];    

    Tmp[0][j] = C[1] + C[0]; //store with transposition
    Tmp[1][j] = C[0] - C[1];
    Tmp[2][j] = C[3] + C[2];
    Tmp[3][j] = C[2] - C[3];
    Tmp[4][j] = C[5] + C[4];
    Tmp[5][j] = C[4] - C[5];
    Tmp[6][j] = C[7] + C[6];
    Tmp[7][j] = C[6] - C[7];
  }

  for(int32 j=0; j<8; j++) //vertical transform
  {
    B[0] = Tmp[j][4] + Tmp[j][0];
    B[1] = Tmp[j][5] + Tmp[j][1];
    B[2] = Tmp[j][6] + Tmp[j][2];
    B[3] = Tmp[j][7] + Tmp[j][3];
    B[4] = Tmp[j][0] - Tmp[j][4];
    B[5] = Tmp[j][1] - Tmp[j][5];
    B[6] = Tmp[j][2] - Tmp[j][6];
    B[7] = Tmp[j][3] - Tmp[j][7];

    C[0] = B[2] + B[0];
    C[1] = B[3] + B[1];
    C[2] = B[0] - B[2];
    C[3] = B[1] - B[3];
    C[4] = B[6] + B[4];
    C[5] = B[7] + B[5];
    C[6] = B[4] - B[6];
    C[7] = B[5] - B[7];    

    Dst[0*DstStride] = (C[1] + C[0])<<1; //store with transposition
    Dst[1*DstStride] = (C[0] - C[1])<<1;
    Dst[2*DstStride] = (C[3] + C[2])<<1;
    Dst[3*DstStride] = (C[2] - C[3])<<1;
    Dst[4*DstStride] = (C[5] + C[4])<<1;
    Dst[5*DstStride] = (C[4] - C[5])<<1;
    Dst[6*DstStride] = (C[7] + C[6])<<1;
    Dst[7*DstStride] = (C[6] - C[7])<<1;
    Dst++;
  }
}
void xTransform::TransformHadamard_8x8_SSE(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 DstStride)
{
  __m128i Tmp[8];
  const __m128i Zero = _mm_setzero_si128();
  
  //load and horizontal transform
  for(int j=0; j<8; j++)
  {
    __m128i Line = _mm_loadu_si128((__m128i*)Src);
    Src += SrcStride;

    __m128i Line_0  = _mm_unpacklo_epi16(Zero, Line);
    __m128i Line_1  = _mm_unpackhi_epi16(Zero, Line);
    Line_0 = _mm_srai_epi32(Line_0, 16);
    Line_1 = _mm_srai_epi32(Line_1, 16);

    __m128i H_L0_0 = _mm_add_epi32(Line_0, Line_1);
    __m128i H_L0_1 = _mm_sub_epi32(Line_0, Line_1);
    __m128i H_L1_0 = _mm_unpacklo_epi64(H_L0_0, H_L0_1);
    __m128i H_L1_1 = _mm_unpackhi_epi64(H_L0_0, H_L0_1);
    __m128i H_L2_0 = _mm_add_epi32(H_L1_0, H_L1_1);
    __m128i H_L2_1 = _mm_sub_epi32(H_L1_0, H_L1_1);
    __m128i H_L3_0 = _mm_unpacklo_epi32(H_L2_0, H_L2_1);
    __m128i H_L3_1 = _mm_unpackhi_epi32(H_L2_0, H_L2_1);
    __m128i H_L4_0 = _mm_unpacklo_epi64(H_L3_0, H_L3_1);
    __m128i H_L4_1 = _mm_unpackhi_epi64(H_L3_0, H_L3_1);
    __m128i H_L5_0 = _mm_add_epi32(H_L4_0, H_L4_1);
    __m128i H_L5_1 = _mm_sub_epi32(H_L4_0, H_L4_1);
    __m128i H_L6_0 = _mm_unpacklo_epi32(H_L5_0, H_L5_1);
    __m128i H_L6_1 = _mm_unpackhi_epi32(H_L5_0, H_L5_1);

    Tmp[j] = _mm_packs_epi32(H_L6_0, H_L6_1);   
  } 
  
  //transpose  
  {
    __m128i Tr_A0 = _mm_unpacklo_epi16(Tmp[0], Tmp[4]);
    __m128i Tr_A1 = _mm_unpackhi_epi16(Tmp[0], Tmp[4]);
    __m128i Tr_A2 = _mm_unpacklo_epi16(Tmp[1], Tmp[5]);
    __m128i Tr_A3 = _mm_unpackhi_epi16(Tmp[1], Tmp[5]);
    __m128i Tr_A4 = _mm_unpacklo_epi16(Tmp[2], Tmp[6]);
    __m128i Tr_A5 = _mm_unpackhi_epi16(Tmp[2], Tmp[6]);
    __m128i Tr_A6 = _mm_unpacklo_epi16(Tmp[3], Tmp[7]);
    __m128i Tr_A7 = _mm_unpackhi_epi16(Tmp[3], Tmp[7]);

    __m128i Tr_B0 = _mm_unpacklo_epi16(Tr_A0, Tr_A4);
    __m128i Tr_B1 = _mm_unpackhi_epi16(Tr_A0, Tr_A4);
    __m128i Tr_B2 = _mm_unpacklo_epi16(Tr_A1, Tr_A5);
    __m128i Tr_B3 = _mm_unpackhi_epi16(Tr_A1, Tr_A5);
    __m128i Tr_B4 = _mm_unpacklo_epi16(Tr_A2, Tr_A6);
    __m128i Tr_B5 = _mm_unpackhi_epi16(Tr_A2, Tr_A6);
    __m128i Tr_B6 = _mm_unpacklo_epi16(Tr_A3, Tr_A7);
    __m128i Tr_B7 = _mm_unpackhi_epi16(Tr_A3, Tr_A7);

    Tmp[0] = _mm_unpacklo_epi16(Tr_B0, Tr_B4);
    Tmp[1] = _mm_unpackhi_epi16(Tr_B0, Tr_B4);
    Tmp[2] = _mm_unpacklo_epi16(Tr_B1, Tr_B5);
    Tmp[3] = _mm_unpackhi_epi16(Tr_B1, Tr_B5);
    Tmp[4] = _mm_unpacklo_epi16(Tr_B2, Tr_B6);
    Tmp[5] = _mm_unpackhi_epi16(Tr_B2, Tr_B6);
    Tmp[6] = _mm_unpacklo_epi16(Tr_B3, Tr_B7);
    Tmp[7] = _mm_unpackhi_epi16(Tr_B3, Tr_B7);
  }  

  //vertical transform
  for(int j=0; j<8; j++)
  {
    __m128i Line = Tmp[j]; 
    __m128i Line_0  = _mm_unpacklo_epi16(Zero, Line);
    __m128i Line_1  = _mm_unpackhi_epi16(Zero, Line);
    Line_0 = _mm_srai_epi32(Line_0, 16);
    Line_1 = _mm_srai_epi32(Line_1, 16);

    __m128i H_L0_0 = _mm_add_epi32(Line_0, Line_1);
    __m128i H_L0_1 = _mm_sub_epi32(Line_0, Line_1);
    __m128i H_L1_0 = _mm_unpacklo_epi64(H_L0_0, H_L0_1);
    __m128i H_L1_1 = _mm_unpackhi_epi64(H_L0_0, H_L0_1);
    __m128i H_L2_0 = _mm_add_epi32(H_L1_0, H_L1_1);
    __m128i H_L2_1 = _mm_sub_epi32(H_L1_0, H_L1_1);
    __m128i H_L3_0 = _mm_unpacklo_epi32(H_L2_0, H_L2_1);
    __m128i H_L3_1 = _mm_unpackhi_epi32(H_L2_0, H_L2_1);
    __m128i H_L4_0 = _mm_unpacklo_epi64(H_L3_0, H_L3_1);
    __m128i H_L4_1 = _mm_unpackhi_epi64(H_L3_0, H_L3_1);
    __m128i H_L5_0 = _mm_add_epi32(H_L4_0, H_L4_1);
    __m128i H_L5_1 = _mm_sub_epi32(H_L4_0, H_L4_1);
    __m128i H_L6_0 = _mm_unpacklo_epi32(H_L5_0, H_L5_1);
    __m128i H_L6_1 = _mm_unpackhi_epi32(H_L5_0, H_L5_1);

    Tmp[j] = _mm_packs_epi32(H_L6_0, H_L6_1);   
  }  

  //transpose 
  {
    __m128i Tr_A0 = _mm_unpacklo_epi16(Tmp[0], Tmp[4]);
    __m128i Tr_A1 = _mm_unpackhi_epi16(Tmp[0], Tmp[4]);
    __m128i Tr_A2 = _mm_unpacklo_epi16(Tmp[1], Tmp[5]);
    __m128i Tr_A3 = _mm_unpackhi_epi16(Tmp[1], Tmp[5]);
    __m128i Tr_A4 = _mm_unpacklo_epi16(Tmp[2], Tmp[6]);
    __m128i Tr_A5 = _mm_unpackhi_epi16(Tmp[2], Tmp[6]);
    __m128i Tr_A6 = _mm_unpacklo_epi16(Tmp[3], Tmp[7]);
    __m128i Tr_A7 = _mm_unpackhi_epi16(Tmp[3], Tmp[7]);

    __m128i Tr_B0 = _mm_unpacklo_epi16(Tr_A0, Tr_A4);
    __m128i Tr_B1 = _mm_unpackhi_epi16(Tr_A0, Tr_A4);
    __m128i Tr_B2 = _mm_unpacklo_epi16(Tr_A1, Tr_A5);
    __m128i Tr_B3 = _mm_unpackhi_epi16(Tr_A1, Tr_A5);
    __m128i Tr_B4 = _mm_unpacklo_epi16(Tr_A2, Tr_A6);
    __m128i Tr_B5 = _mm_unpackhi_epi16(Tr_A2, Tr_A6);
    __m128i Tr_B6 = _mm_unpacklo_epi16(Tr_A3, Tr_A7);
    __m128i Tr_B7 = _mm_unpackhi_epi16(Tr_A3, Tr_A7);

    Tmp[0] = _mm_unpacklo_epi16(Tr_B0, Tr_B4);
    Tmp[1] = _mm_unpackhi_epi16(Tr_B0, Tr_B4);
    Tmp[2] = _mm_unpacklo_epi16(Tr_B1, Tr_B5);
    Tmp[3] = _mm_unpackhi_epi16(Tr_B1, Tr_B5);
    Tmp[4] = _mm_unpacklo_epi16(Tr_B2, Tr_B6);
    Tmp[5] = _mm_unpackhi_epi16(Tr_B2, Tr_B6);
    Tmp[6] = _mm_unpacklo_epi16(Tr_B3, Tr_B7);
    Tmp[7] = _mm_unpackhi_epi16(Tr_B3, Tr_B7);
  }  

  //store
  for(int j=0; j<8; j++)
  {
    Tmp[j] =_mm_slli_epi16(Tmp[j], 1);
    _mm_storeu_si128((__m128i*)Dst, Tmp[j]);
    Dst += DstStride;   
  } 
}
void xTransform::TransformHadamard_16x16_STD   (int16* Src, int16* Dst, uint32 SrcStride, uint32 DstStride)
{
  int32 B[16], C[16];
  int32 Tmp[16][16];

  for(int32 j=0; j<16; j++) //horizontal transform
  {
    for(int32 i=0; i<8; i++)
    {
      B[i  ] = Src[i+8] + Src[i  ];
      B[i+8] = Src[i  ] - Src[i+8];
    }
    Src += SrcStride;

    for(int32 i=0; i<16; i+=8)
    {
      C[i  ] = B[i+4] + B[i  ];
      C[i+1] = B[i+5] + B[i+1];
      C[i+2] = B[i+6] + B[i+2];
      C[i+3] = B[i+7] + B[i+3];
      C[i+4] = B[i  ] - B[i+4];
      C[i+5] = B[i+1] - B[i+5];
      C[i+6] = B[i+2] - B[i+6];
      C[i+7] = B[i+3] - B[i+7];
    }  

    for(int32 i=0; i<16; i+=4)
    {
      B[i  ] = C[i+2] + C[i  ];
      B[i+1] = C[i+3] + C[i+1];
      B[i+2] = C[i  ] - C[i+2];
      B[i+3] = C[i+1] - C[i+3];
    }

    for(int32 i=0; i<16; i+=2)
    {
      Tmp[i  ][j] = B[i+1] + B[i  ]; //store with transposition
      Tmp[i+1][j] = B[i  ] - B[i+1];
    }
  }

  for(int32 j=0; j<16; j++) //vertical transform
  {
    for(int32 i=0; i<8; i++)
    {
      B[i  ] = Tmp[j][i+8] + Tmp[j][i  ];
      B[i+8] = Tmp[j][i  ] - Tmp[j][i+8];
    }

    for(int32 i=0; i<16; i+=8)
    {
      C[i  ] = B[i+4] + B[i  ];
      C[i+1] = B[i+5] + B[i+1];
      C[i+2] = B[i+6] + B[i+2];
      C[i+3] = B[i+7] + B[i+3];
      C[i+4] = B[i  ] - B[i+4];
      C[i+5] = B[i+1] - B[i+5];
      C[i+6] = B[i+2] - B[i+6];
      C[i+7] = B[i+3] - B[i+7];
    } 

    for(int32 i=0; i<16; i+=4)
    {
      B[i  ] = C[i+2] + C[i  ];
      B[i+1] = C[i+3] + C[i+1];
      B[i+2] = C[i  ] - C[i+2];
      B[i+3] = C[i+1] - C[i+3];
    }

    for(int32 i=0; i<16; i+=2)
    {
      Dst[    i*DstStride] = (B[i+1] + B[i  ] + 1)>>1; //store with transposition
      Dst[(i+1)*DstStride] = (B[i  ] - B[i+1] + 1)>>1;
    } 
    Dst++;
  }
}
void xTransform::TransformHadamard_32x32_STD   (int16* Src, int16* Dst, uint32 SrcStride, uint32 DstStride)
{
  int32 B[32],C[32];
  int32 Tmp[32][32];

  for(int32 j=0; j<32; j++) //horizontal transform
  {
    for(int32 i=0; i<16; i++)
    {
      B[i   ] = Src[i+16] + Src[i   ];
      B[i+16] = Src[i   ] - Src[i+16];
    }
    Src += SrcStride;

    for(int32 i=0; i<8; i++)
    {
      C[i   ] = B[i+8 ] + B[i   ];
      C[i+8 ] = B[i   ] - B[i+8 ];
      C[i+16] = B[i+24] + B[i+16];
      C[i+24] = B[i+16] - B[i+24];
    }  

    for(int32 i=0; i<4; i++)
    {
      B[i   ] = C[i+4 ] + C[i   ];
      B[i+4 ] = C[i   ] - C[i+4 ];
      B[i+8 ] = C[i+12] + C[i+8 ];
      B[i+12] = C[i+8 ] - C[i+12];
      B[i+16] = C[i+20] + C[i+16];
      B[i+20] = C[i+16] - C[i+20];
      B[i+24] = C[i+28] + C[i+24];
      B[i+28] = C[i+24] - C[i+28];
    }     

    for(int32 i=0; i<32; i+=4)
    {
      C[i  ] = B[i+2] + B[i  ];
      C[i+1] = B[i+3] + B[i+1];
      C[i+2] = B[i  ] - B[i+2];
      C[i+3] = B[i+1] - B[i+3];
    }

    for(int32 i=0; i<32; i+=2)
    {
      Tmp[i  ][j] = C[i+1] + C[i  ]; //store with transposition
      Tmp[i+1][j] = C[i  ] - C[i+1];
    }
  }

  for(int32 j=0; j<32; j++) //vertical transform
  {
    for(int32 i=0; i<16; i++)
    {
      B[i   ] = Tmp[j][i+16] + Tmp[j][i   ];
      B[i+16] = Tmp[j][i   ] - Tmp[j][i+16];
    }

    for(int32 i=0; i<8; i++)
    {
      C[i   ] = B[i+8 ] + B[i   ];
      C[i+8 ] = B[i   ] - B[i+8 ];
      C[i+16] = B[i+24] + B[i+16];
      C[i+24] = B[i+16] - B[i+24];
    }  

    for(int32 i=0; i<4; i++)
    {
      B[i   ] = C[i+4 ] + C[i   ];
      B[i+4 ] = C[i   ] - C[i+4 ];
      B[i+8 ] = C[i+12] + C[i+8 ];
      B[i+12] = C[i+8 ] - C[i+12];
      B[i+16] = C[i+20] + C[i+16];
      B[i+20] = C[i+16] - C[i+20];
      B[i+24] = C[i+28] + C[i+24];
      B[i+28] = C[i+24] - C[i+28];
    }     

    for(int32 i=0; i<32; i+=4)
    {
      C[i  ] = B[i+2] + B[i  ];
      C[i+1] = B[i+3] + B[i+1];
      C[i+2] = B[i  ] - B[i+2];
      C[i+3] = B[i+1] - B[i+3];
    }

    for(int32 i=0; i<32; i+=2)
    {
      Dst[    i*DstStride] = (C[i+1] + C[i  ] + 2)>>3; //store with transposition
      Dst[(i+1)*DstStride] = (C[i  ] - C[i+1] + 2)>>3;
    } 
    Dst++;
  }
}
void xTransform::InvTransformHadamard_4x4_STD  (int16* Src, int16* Dst, uint32 SrcStride, uint32 DstStride)
{
  int32 D[4];
  int32 Tmp[4][4];
  int32 SrcStride_x3 = SrcStride + (SrcStride<<1);

  for(int32 j=0; j<4; j++) //vertical transform
  {    
    D[0] = Src[SrcStride   ] + Src[0            ]; //read with transposition
    D[1] = Src[0            ] - Src[SrcStride   ];
    D[2] = Src[SrcStride_x3] + Src[SrcStride<<1];
    D[3] = Src[SrcStride<<1] - Src[SrcStride_x3];
    Src++;

    Tmp[j][0] = D[2] + D[0];
    Tmp[j][1] = D[3] + D[1];
    Tmp[j][2] = D[0] - D[2];
    Tmp[j][3] = D[1] - D[3];
  }

  for(int32 j=0; j<4; j++) //horizontal transform
  {
    D[0] = Tmp[1][j] + Tmp[0][j]; //read with transposition
    D[1] = Tmp[0][j] - Tmp[1][j];
    D[2] = Tmp[3][j] + Tmp[2][j];
    D[3] = Tmp[2][j] - Tmp[3][j];

    Dst[0] = (D[2] + D[0] + 64)>>7;
    Dst[1] = (D[3] + D[1] + 64)>>7;
    Dst[2] = (D[0] - D[2] + 64)>>7;
    Dst[3] = (D[1] - D[3] + 64)>>7;
    Dst += DstStride;
  }
}
void xTransform::InvTransformHadamard_8x8_STD  (int16* Src, int16* Dst, uint32 SrcStride, uint32 DstStride)
{
  int32 B[8],C[8];
  int32 Tmp[8][8];

  for(int32 j=0; j<8; j++) //vertical transform
  {    
    B[0] = Src[1*SrcStride] + Src[0*SrcStride]; //read with transposition
    B[1] = Src[0*SrcStride] - Src[1*SrcStride];
    B[2] = Src[3*SrcStride] + Src[2*SrcStride];
    B[3] = Src[2*SrcStride] - Src[3*SrcStride];
    B[4] = Src[5*SrcStride] + Src[4*SrcStride]; 
    B[5] = Src[4*SrcStride] - Src[5*SrcStride];
    B[6] = Src[7*SrcStride] + Src[6*SrcStride];
    B[7] = Src[6*SrcStride] - Src[7*SrcStride];
    Src++;

    C[0] = B[2] + B[0];
    C[1] = B[3] + B[1];
    C[2] = B[0] - B[2];
    C[3] = B[1] - B[3];
    C[4] = B[6] + B[4]; 
    C[5] = B[7] + B[5];
    C[6] = B[4] - B[6];
    C[7] = B[5] - B[7];

    Tmp[j][0] = C[4] + C[0];
    Tmp[j][1] = C[5] + C[1];
    Tmp[j][2] = C[6] + C[2];
    Tmp[j][3] = C[7] + C[3];
    Tmp[j][4] = C[0] - C[4];
    Tmp[j][5] = C[1] - C[5];
    Tmp[j][6] = C[2] - C[6];
    Tmp[j][7] = C[3] - C[7];
  }

  for(int32 j=0; j<8; j++) //horizontal transform
  {
    B[0] = Tmp[1][j] + Tmp[0][j]; //read with transposition
    B[1] = Tmp[0][j] - Tmp[1][j];
    B[2] = Tmp[3][j] + Tmp[2][j];
    B[3] = Tmp[2][j] - Tmp[3][j];
    B[4] = Tmp[5][j] + Tmp[4][j]; 
    B[5] = Tmp[4][j] - Tmp[5][j];
    B[6] = Tmp[7][j] + Tmp[6][j];
    B[7] = Tmp[6][j] - Tmp[7][j];

    C[0] = B[2] + B[0];
    C[1] = B[3] + B[1];
    C[2] = B[0] - B[2];
    C[3] = B[1] - B[3];
    C[4] = B[6] + B[4]; 
    C[5] = B[7] + B[5];
    C[6] = B[4] - B[6];
    C[7] = B[5] - B[7];

    Dst[0] = (C[4] + C[0] + 64)>>7;
    Dst[1] = (C[5] + C[1] + 64)>>7;
    Dst[2] = (C[6] + C[2] + 64)>>7;
    Dst[3] = (C[7] + C[3] + 64)>>7;
    Dst[4] = (C[0] - C[4] + 64)>>7;
    Dst[5] = (C[1] - C[5] + 64)>>7;
    Dst[6] = (C[2] - C[6] + 64)>>7;
    Dst[7] = (C[3] - C[7] + 64)>>7;
    Dst += DstStride;
  }
}
void xTransform::InvTransformHadamard_16x16_STD(int16* Src, int16* Dst, uint32 SrcStride, uint32 DstStride)
{
  int32 B[16],C[16];
  int32 Tmp[16][16]; 

  for(int32 j=0; j<16; j++) //vertical transform
  {
    for(int32 i=0; i<16; i+=2)
    {
      B[i  ] = Src[(i+1)*SrcStride] + Src[    i*SrcStride]; //read with transposition
      B[i+1] = Src[    i*SrcStride] - Src[(i+1)*SrcStride];
    }
    Src++;

    for(int32 i=0; i<16; i+=4)
    {
      C[i  ] = B[i+2] + B[i  ];
      C[i+1] = B[i+3] + B[i+1];
      C[i+2] = B[i  ] - B[i+2];
      C[i+3] = B[i+1] - B[i+3];
    }

    for(int32 i=0; i<16; i+=8)
    {
      B[i  ] = C[i+4] + C[i  ];
      B[i+1] = C[i+5] + C[i+1];
      B[i+2] = C[i+6] + C[i+2];
      B[i+3] = C[i+7] + C[i+3];
      B[i+4] = C[i  ] - C[i+4];
      B[i+5] = C[i+1] - C[i+5];
      B[i+6] = C[i+2] - C[i+6];
      B[i+7] = C[i+3] - C[i+7];
    } 

    for(int32 i=0; i<8; i++)
    {
      Tmp[j][i  ] = B[i+8] + B[i  ]; 
      Tmp[j][i+8] = B[i  ] - B[i+8];
    } 
  }

  for(int32 j=0; j<16; j++) //horizontal transform
  {
    for(int32 i=0; i<16; i+=2)
    {
      B[i  ] = Tmp[i+1][j] + Tmp[i  ][j];  //read with transposition
      B[i+1] = Tmp[i  ][j] - Tmp[i+1][j];
    }

    for(int32 i=0; i<16; i+=4)
    {
      C[i  ] = B[i+2] + B[i  ];
      C[i+1] = B[i+3] + B[i+1];
      C[i+2] = B[i  ] - B[i+2];
      C[i+3] = B[i+1] - B[i+3];
    }

    for(int32 i=0; i<16; i+=8)
    {
      B[i  ] = C[i+4] + C[i  ];
      B[i+1] = C[i+5] + C[i+1];
      B[i+2] = C[i+6] + C[i+2];
      B[i+3] = C[i+7] + C[i+3];
      B[i+4] = C[i  ] - C[i+4];
      B[i+5] = C[i+1] - C[i+5];
      B[i+6] = C[i+2] - C[i+6];
      B[i+7] = C[i+3] - C[i+7];
    } 

    for(int32 i=0; i<8; i++)
    {
      Dst[i  ] = (B[i+8] + B[i  ] + 64)>>7; 
      Dst[i+8] = (B[i  ] - B[i+8] + 64)>>7;
    } 
    Dst += DstStride;
  }
}
void xTransform::InvTransformHadamard_32x32_STD(int16* Src, int16* Dst, uint32 SrcStride, uint32 DstStride)
{
  int32 B[32],C[32];
  int32 Tmp[32][32];

  for(int32 j=0; j<32; j++) //vertical transform
  {
    for(int32 i=0; i<32; i+=2)
    {
      B[i  ] = Src[(i+1)*SrcStride] + Src[    i*SrcStride]; //read with transposition
      B[i+1] = Src[    i*SrcStride] - Src[(i+1)*SrcStride];
    }
    Src++;

    for(int32 i=0; i<32; i+=4)
    {
      C[i  ] = B[i+2] + B[i  ];
      C[i+1] = B[i+3] + B[i+1];
      C[i+2] = B[i  ] - B[i+2];
      C[i+3] = B[i+1] - B[i+3];
    }

    for(int32 i=0; i<4; i++)
    {
      B[i   ] = C[i+4 ] + C[i   ];
      B[i+4 ] = C[i   ] - C[i+4 ];
      B[i+8 ] = C[i+12] + C[i+8 ];
      B[i+12] = C[i+8 ] - C[i+12];
      B[i+16] = C[i+20] + C[i+16];
      B[i+20] = C[i+16] - C[i+20];
      B[i+24] = C[i+28] + C[i+24];
      B[i+28] = C[i+24] - C[i+28];
    } 

    for(int32 i=0; i<8; i++)
    {
      C[i   ] = B[i+8 ] + B[i   ];
      C[i+8 ] = B[i   ] - B[i+8 ];
      C[i+16] = B[i+24] + B[i+16];
      C[i+24] = B[i+16] - B[i+24];
    }

    for(int32 i=0; i<16; i++)
    {
      Tmp[j][i   ] = C[i+16] + C[i   ];
      Tmp[j][i+16] = C[i   ] - C[i+16];
    } 
  }

  for(int32 j=0; j<32; j++) //horizontal transform
  {
    for(int32 i=0; i<32; i+=2)
    {
      B[i  ] = Tmp[i+1][j] + Tmp[i  ][j]; //read with transposition 
      B[i+1] = Tmp[i  ][j] - Tmp[i+1][j];
    }

    for(int32 i=0; i<32; i+=4)
    {
      C[i  ] = B[i+2] + B[i  ];
      C[i+1] = B[i+3] + B[i+1];
      C[i+2] = B[i  ] - B[i+2];
      C[i+3] = B[i+1] - B[i+3];
    }

    for(int32 i=0; i<4; i++)
    {
      B[i   ] = C[i+4 ] + C[i   ];
      B[i+4 ] = C[i   ] - C[i+4 ];
      B[i+8 ] = C[i+12] + C[i+8 ];
      B[i+12] = C[i+8 ] - C[i+12];
      B[i+16] = C[i+20] + C[i+16];
      B[i+20] = C[i+16] - C[i+20];
      B[i+24] = C[i+28] + C[i+24];
      B[i+28] = C[i+24] - C[i+28];
    } 

    for(int32 i=0; i<8; i++)
    {
      C[i   ] = B[i+8 ] + B[i   ];
      C[i+8 ] = B[i   ] - B[i+8 ];
      C[i+16] = B[i+24] + B[i+16];
      C[i+24] = B[i+16] - B[i+24];
    }

    for(int32 i=0; i<16; i++)
    {
      Dst[i   ] = (C[i+16] + C[i   ] + 64)>>7; 
      Dst[i+16] = (C[i   ] - C[i+16] + 64)>>7;
    } 
    Dst += DstStride;
  }
}

void xTransform::TransformDCT_4x4_STD_C(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[2],O[2];
  int32 Tmp[4][4];
  int32 Shift1st = BitDepth - 7;
  int32 Add1st   = 1<<(Shift1st-1); 

  for(int32 j=0; j<4; j++) //horizontal transform
  {    
    E[0] = Src[0] + Src[3];
    O[0] = Src[0] - Src[3];
    E[1] = Src[1] + Src[2];
    O[1] = Src[1] - Src[2];
    Src += SrcStride;

    Tmp[0][j] = xClipS16((E[0] + E[1])<<(6-Shift1st)); //store with transposition
    Tmp[2][j] = xClipS16((E[0] - E[1])<<(6-Shift1st));
    Tmp[1][j] = xClipS16(( 83*O[0] + 36*O[1] + Add1st)>>Shift1st);
    Tmp[3][j] = xClipS16(( 36*O[0] - 83*O[1] + Add1st)>>Shift1st);     
  }

  for(int32 j=0; j<4; j++) //vertical transform
  {
    E[0] = Tmp[j][0] + Tmp[j][3];
    O[0] = Tmp[j][0] - Tmp[j][3];
    E[1] = Tmp[j][1] + Tmp[j][2];
    O[1] = Tmp[j][1] - Tmp[j][2];

    Dst[ 0]  = xClipS16((((E[0] + E[1])<<6) + 128)>>8); //store with transposition
    Dst[ 4]  = xClipS16(( 83*O[0] + 36*O[1] + 128)>>8);
    Dst[ 8]  = xClipS16((((E[0] - E[1])<<6) + 128)>>8);    
    Dst[12]  = xClipS16(( 36*O[0] - 83*O[1] + 128)>>8);
    Dst++;
  }
}
void xTransform::TransformDCT_4x4_STD_M(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[2],O[2];
  int32 Tmp[4][4];
  int32 Shift1st = BitDepth - 7;
  int32 Add1st   = 1<<(Shift1st-1); 

  for(int32 j=0;j<4;j++) //horizontal transform
  {    
    E[0] = Src[0] + Src[3];
    O[0] = Src[0] - Src[3];
    E[1] = Src[1] + Src[2];
    O[1] = Src[1] - Src[2];
    Src += SrcStride;

    Tmp[0][j] = xClipS16((m_TrM4x4[0][0]*E[0] + m_TrM4x4[0][1]*E[1] + Add1st)>>Shift1st);  //store with transposition
    Tmp[2][j] = xClipS16((m_TrM4x4[2][0]*E[0] + m_TrM4x4[2][1]*E[1] + Add1st)>>Shift1st);
    Tmp[1][j] = xClipS16((m_TrM4x4[1][0]*O[0] + m_TrM4x4[1][1]*O[1] + Add1st)>>Shift1st);
    Tmp[3][j] = xClipS16((m_TrM4x4[3][0]*O[0] + m_TrM4x4[3][1]*O[1] + Add1st)>>Shift1st);    
  }

  for(int32 j=0;j<4;j++) //vertical transform
  {
    E[0] = Tmp[j][0] + Tmp[j][3];
    O[0] = Tmp[j][0] - Tmp[j][3];
    E[1] = Tmp[j][1] + Tmp[j][2];
    O[1] = Tmp[j][1] - Tmp[j][2];

    Dst[ 0] = xClipS16((m_TrM4x4[0][0]*E[0] + m_TrM4x4[0][1]*E[1] + 128)>>8);  //store with transposition
    Dst[ 4] = xClipS16((m_TrM4x4[2][0]*E[0] + m_TrM4x4[2][1]*E[1] + 128)>>8);
    Dst[ 8] = xClipS16((m_TrM4x4[1][0]*O[0] + m_TrM4x4[1][1]*O[1] + 128)>>8);
    Dst[12] = xClipS16((m_TrM4x4[3][0]*O[0] + m_TrM4x4[3][1]*O[1] + 128)>>8);    
    Dst++;
  }
}
void xTransform::TransformDCT_4x4_SSE(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth) 
{  
  __m128i Tmp_0, Tmp_1;
  const int32 Shift1st = BitDepth - 7;
  const __m128i xT4_0_1 = _mm_setr_epi16(64, 64, 64, 64, 83, 36,-36,-83);
  const __m128i xT4_2_3 = _mm_setr_epi16(64,-64,-64, 64, 36,-83, 83,-36);
  const __m128i Add1st = _mm_set1_epi32(1<<(Shift1st-1));
  const __m128i add_2nd = _mm_set1_epi32(128);
  
  //load and horizontal transform
  {
    __m128i line_0 = _mm_loadl_epi64((__m128i*)Src); Src += SrcStride;
    __m128i line_1 = _mm_loadl_epi64((__m128i*)Src); Src += SrcStride;
    __m128i line_2 = _mm_loadl_epi64((__m128i*)Src); Src += SrcStride;
    __m128i line_3 = _mm_loadl_epi64((__m128i*)Src);

    line_0 = _mm_unpacklo_epi64(line_0, line_0);
    line_1 = _mm_unpacklo_epi64(line_1, line_1);
    line_2 = _mm_unpacklo_epi64(line_2, line_2);
    line_3 = _mm_unpacklo_epi64(line_3, line_3);

    __m128i t_0 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_0, xT4_0_1), _mm_madd_epi16(line_0, xT4_2_3)), Add1st), Shift1st);
    __m128i t_1 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_1, xT4_0_1), _mm_madd_epi16(line_1, xT4_2_3)), Add1st), Shift1st);
    __m128i t_2 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_2, xT4_0_1), _mm_madd_epi16(line_2, xT4_2_3)), Add1st), Shift1st);
    __m128i t_3 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_3, xT4_0_1), _mm_madd_epi16(line_3, xT4_2_3)), Add1st), Shift1st);
  
    Tmp_0 = _mm_packs_epi32(t_0, t_1);
    Tmp_1 = _mm_packs_epi32(t_2, t_3);
  }
  
  //transpose
  {
    __m128i tr_0 = _mm_unpacklo_epi16(Tmp_0, Tmp_1);
    __m128i tr_1 = _mm_unpackhi_epi16(Tmp_0, Tmp_1);  
    Tmp_0 = _mm_unpacklo_epi16(tr_0, tr_1);
    Tmp_1 = _mm_unpackhi_epi16(tr_0, tr_1);  
  }

  //vertical transforms
  {
    __m128i line_0 = _mm_unpacklo_epi64(Tmp_0, Tmp_0);
    __m128i line_1 = _mm_unpackhi_epi64(Tmp_0, Tmp_0);
    __m128i line_2 = _mm_unpacklo_epi64(Tmp_1, Tmp_1);
    __m128i line_3 = _mm_unpackhi_epi64(Tmp_1, Tmp_1);

    __m128i t_0 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_0, xT4_0_1), _mm_madd_epi16(line_0, xT4_2_3)), add_2nd), 8);
    __m128i t_1 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_1, xT4_0_1), _mm_madd_epi16(line_1, xT4_2_3)), add_2nd), 8);
    __m128i t_2 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_2, xT4_0_1), _mm_madd_epi16(line_2, xT4_2_3)), add_2nd), 8);
    __m128i t_3 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_3, xT4_0_1), _mm_madd_epi16(line_3, xT4_2_3)), add_2nd), 8);

    Tmp_0 = _mm_packs_epi32(t_0, t_1);
    Tmp_1 = _mm_packs_epi32(t_2, t_3);
  }

  //transpose
  {
    __m128i tr_0 = _mm_unpacklo_epi16(Tmp_0, Tmp_1);
    __m128i tr_1 = _mm_unpackhi_epi16(Tmp_0, Tmp_1);  
    Tmp_0 = _mm_unpacklo_epi16(tr_0, tr_1);
    Tmp_1 = _mm_unpackhi_epi16(tr_0, tr_1);
  }

  //store
  _mm_store_si128((__m128i*)(Dst  ), Tmp_0);  
  _mm_store_si128((__m128i*)(Dst+8), Tmp_1); 
}
void xTransform::TransformDCT_4x4_SSE(int16* restrict Src, int16* restrict Dst, uint32 BitDepth)  //no SrcStride version - continous Src buffer
{  
  __m128i Tmp_0, Tmp_1;
  const int32 Shift1st = BitDepth - 7;
  const __m128i xT4_0_1 = _mm_setr_epi16(64, 64, 64, 64, 83, 36,-36,-83);
  const __m128i xT4_2_3 = _mm_setr_epi16(64,-64,-64, 64, 36,-83, 83,-36);
  const __m128i Add1st = _mm_set1_epi32(1<<(Shift1st-1));
  const __m128i add_2nd = _mm_set1_epi32(128);
  
  //load and horizontal transform
  {
    Tmp_0 = _mm_load_si128((__m128i*)(Src  ));
    Tmp_1 = _mm_load_si128((__m128i*)(Src+8));

    __m128i line_0 = _mm_unpacklo_epi64(Tmp_0, Tmp_0);
    __m128i line_1 = _mm_unpackhi_epi64(Tmp_0, Tmp_0);
    __m128i line_2 = _mm_unpacklo_epi64(Tmp_1, Tmp_1);
    __m128i line_3 = _mm_unpackhi_epi64(Tmp_1, Tmp_1);

    __m128i t_0 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_0, xT4_0_1), _mm_madd_epi16(line_0, xT4_2_3)), Add1st), Shift1st);
    __m128i t_1 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_1, xT4_0_1), _mm_madd_epi16(line_1, xT4_2_3)), Add1st), Shift1st);
    __m128i t_2 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_2, xT4_0_1), _mm_madd_epi16(line_2, xT4_2_3)), Add1st), Shift1st);
    __m128i t_3 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_3, xT4_0_1), _mm_madd_epi16(line_3, xT4_2_3)), Add1st), Shift1st);
  
    Tmp_0 = _mm_packs_epi32(t_0, t_1);
    Tmp_1 = _mm_packs_epi32(t_2, t_3);
  }
  
  //transpose
  {
    __m128i tr_0 = _mm_unpacklo_epi16(Tmp_0, Tmp_1);
    __m128i tr_1 = _mm_unpackhi_epi16(Tmp_0, Tmp_1);  
    Tmp_0 = _mm_unpacklo_epi16(tr_0, tr_1);
    Tmp_1 = _mm_unpackhi_epi16(tr_0, tr_1);  
  }

  //vertical transforms
  {
    __m128i line_0 = _mm_unpacklo_epi64(Tmp_0, Tmp_0);
    __m128i line_1 = _mm_unpackhi_epi64(Tmp_0, Tmp_0);
    __m128i line_2 = _mm_unpacklo_epi64(Tmp_1, Tmp_1);
    __m128i line_3 = _mm_unpackhi_epi64(Tmp_1, Tmp_1);

    __m128i t_0 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_0, xT4_0_1), _mm_madd_epi16(line_0, xT4_2_3)), add_2nd), 8);
    __m128i t_1 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_1, xT4_0_1), _mm_madd_epi16(line_1, xT4_2_3)), add_2nd), 8);
    __m128i t_2 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_2, xT4_0_1), _mm_madd_epi16(line_2, xT4_2_3)), add_2nd), 8);
    __m128i t_3 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_3, xT4_0_1), _mm_madd_epi16(line_3, xT4_2_3)), add_2nd), 8);

    Tmp_0 = _mm_packs_epi32(t_0, t_1);
    Tmp_1 = _mm_packs_epi32(t_2, t_3);
  }

  //transpose
  {
    __m128i tr_0 = _mm_unpacklo_epi16(Tmp_0, Tmp_1);
    __m128i tr_1 = _mm_unpackhi_epi16(Tmp_0, Tmp_1);  
    Tmp_0 = _mm_unpacklo_epi16(tr_0, tr_1);  
    Tmp_1 = _mm_unpackhi_epi16(tr_0, tr_1); 
  }

  //store
  _mm_store_si128((__m128i*)(Dst  ), Tmp_0);  
  _mm_store_si128((__m128i*)(Dst+8), Tmp_1); 
}
void xTransform::TransformDCT_4x4_AVX(int16* restrict Src, int16* restrict Dst, uint32 BitDepth)  //no SrcStride version - continous Src buffer //non tested
{  
  const static __m256i xT4V = _mm256_setr_epi16(64, 64, 64, 64, 83, 36,-36,-83, 64,-64,-64, 64, 36,-83, 83,-36);

  const static __m256i xT4Va = _mm256_setr_epi16(64, 64, 64, 64, 83, 36,-36,-83, 64, 64, 64, 64, 83, 36,-36,-83);
  const static __m256i xT4Vb = _mm256_setr_epi16(64,-64,-64, 64, 36,-83, 83,-36, 64,-64,-64, 64, 36,-83, 83,-36);

  const int32 Shift1st = BitDepth - 7;
  const __m256i Add1stV = _mm256_set1_epi32(1<<(Shift1st-1));
  const __m256i Add2ndV = _mm256_set1_epi32(128);

  //load
  __m256i SrcV = _mm256_loadu_si256((__m256i*)(Src));

  SrcV = _mm256_setr_epi16(1,1,1,1,2,2,2,2,3,3,3,3,4,4,4,4);

  //horizontal transform
  __m256i PrmSrcV0 = _mm256_permute4x64_epi64(SrcV, 0x50);
  __m256i PrmSrcV1 = _mm256_permute4x64_epi64(SrcV, 0xFA);
  __m256i Tr01V = _mm256_srai_epi32(_mm256_add_epi32(_mm256_hadd_epi32(_mm256_madd_epi16(PrmSrcV0, xT4Va), _mm256_madd_epi16(PrmSrcV0, xT4Vb)), Add1stV), Shift1st);
  __m256i Tr23V = _mm256_srai_epi32(_mm256_add_epi32(_mm256_hadd_epi32(_mm256_madd_epi16(PrmSrcV1, xT4Va), _mm256_madd_epi16(PrmSrcV1, xT4Vb)), Add1stV), Shift1st);
  __m256i TrV = _mm256_packs_epi32(Tr01V, Tr23V);

  //transpose
  __m256i Tmp = _mm256_permute4x64_epi64(TrV, 0xB1);
  TrV = _mm256_unpacklo_epi16(TrV, Tmp);
  Tmp = _mm256_permute4x64_epi64(TrV, 0x4E);
  TrV = _mm256_unpacklo_epi16(TrV, Tmp);
  //sprawdzi permute int32 + permute int8

  //vertical transform
  //PrmSrcV0 = _mm256_permute4x64_epi64(TrV, 0xA0);
  //PrmSrcV1 = _mm256_permute4x64_epi64(TrV, 0xF5);
  //Tr02V = _mm256_srai_epi32(_mm256_add_epi32(_mm256_hadd_epi32(_mm256_madd_epi16(PrmSrcV0, xT4Va), _mm256_madd_epi16(PrmSrcV0, xT4Vb)), Add1stV), Shift1st);
  //Tr13V = _mm256_srai_epi32(_mm256_add_epi32(_mm256_hadd_epi32(_mm256_madd_epi16(PrmSrcV1, xT4Va), _mm256_madd_epi16(PrmSrcV1, xT4Vb)), Add1stV), Shift1st);
  //TrV = _mm256_packs_epi32(Tr02V, Tr13V);

  ////transpose
  //Tmp = _mm256_permute4x64_epi64(TrV, 0xA0);
  //TrV = _mm256_unpacklo_epi16(TrV, Tmp);
  //Tmp = _mm256_permute4x64_epi64(TrV, 0xA0);
  //TrV = _mm256_unpacklo_epi16(TrV, Tmp);

  //store
  _mm256_storeu_si256((__m256i*)(Dst), TrV);
}
void xTransform::TransformDCT_8x8_STD_C(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[4],O[4];
  int32 EE[2],EO[2];
  int32 Tmp[8][8];
  int32 Shift1st = BitDepth - 6;
  int32 Add1st   = 1<<(Shift1st-1); 

  for (int32 j=0; j<8; j++) //horizontal transform
  {    
    /* E and O*/
    for (int32 k=0;k<4;k++)
    {
      E[k] = Src[k] + Src[7-k];
      O[k] = Src[k] - Src[7-k];
    }
    Src += SrcStride;

    /* EE and EO */
    EE[0] = E[0] + E[3];    
    EO[0] = E[0] - E[3];
    EE[1] = E[1] + E[2];
    EO[1] = E[1] - E[2];

    Tmp[0][j] = xClipS16((EE[0] + EE[1])<<(6-Shift1st));
    Tmp[4][j] = xClipS16((EE[0] - EE[1])<<(6-Shift1st)); 
    Tmp[2][j] = xClipS16((83*EO[0] + 36*EO[1] + Add1st)>>Shift1st);
    Tmp[6][j] = xClipS16((36*EO[0] - 83*EO[1] + Add1st)>>Shift1st); 

    Tmp[1][j] = xClipS16((89*O[0] + 75*O[1] + 50*O[2] + 18*O[3] + Add1st)>>Shift1st); //store with transposition
    Tmp[3][j] = xClipS16((75*O[0] - 18*O[1] - 89*O[2] - 50*O[3] + Add1st)>>Shift1st);
    Tmp[5][j] = xClipS16((50*O[0] - 89*O[1] + 18*O[2] + 75*O[3] + Add1st)>>Shift1st);
    Tmp[7][j] = xClipS16((18*O[0] - 50*O[1] + 75*O[2] - 89*O[3] + Add1st)>>Shift1st);
  }

  for (int32 j=0; j<8; j++) //vertical transform
  {    
    /* E and O*/
    for (int32 k=0;k<4;k++)
    {
      E[k] = Tmp[j][k] + Tmp[j][7-k];
      O[k] = Tmp[j][k] - Tmp[j][7-k];
    }    

    /* EE and EO */
    EE[0] = E[0] + E[3];    
    EO[0] = E[0] - E[3];
    EE[1] = E[1] + E[2];
    EO[1] = E[1] - E[2];

    Dst[0*8] = xClipS16((((EE[0] + EE[1])<<6) + 256)>>9); //store with transposition
    Dst[4*8] = xClipS16((((EE[0] - EE[1])<<6) + 256)>>9); 
    Dst[2*8] = xClipS16(( 83*EO[0] + 36*EO[1] + 256)>>9);
    Dst[6*8] = xClipS16(( 36*EO[0] - 83*EO[1] + 256)>>9); 

    Dst[1*8] = xClipS16((89*O[0] + 75*O[1] + 50*O[2] + 18*O[3] + 256)>>9);
    Dst[3*8] = xClipS16((75*O[0] - 18*O[1] - 89*O[2] - 50*O[3] + 256)>>9);
    Dst[5*8] = xClipS16((50*O[0] - 89*O[1] + 18*O[2] + 75*O[3] + 256)>>9);
    Dst[7*8] = xClipS16((18*O[0] - 50*O[1] + 75*O[2] - 89*O[3] + 256)>>9);
    Dst++;
  } 
}
void xTransform::TransformDCT_8x8_STD_M(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[4],O[4];
  int32 EE[2],EO[2];
  int32 Tmp[8][8];
  int32 Shift1st = BitDepth - 6;
  int32 Add1st   = 1<<(Shift1st-1); 

  for (int32 j=0; j<8; j++) //horizontal transform
  {    
    /* E and O*/
    for (int32 k=0;k<4;k++)
    {
      E[k] = Src[k] + Src[7-k];
      O[k] = Src[k] - Src[7-k];
    }
    Src += SrcStride;

    /* EE and EO */
    EE[0] = E[0] + E[3];    
    EO[0] = E[0] - E[3];
    EE[1] = E[1] + E[2];
    EO[1] = E[1] - E[2];

    Tmp[0][j] = xClipS16((m_TrM8x8[0][0]*EE[0] + m_TrM8x8[0][1]*EE[1] + Add1st)>>Shift1st);
    Tmp[4][j] = xClipS16((m_TrM8x8[4][0]*EE[0] + m_TrM8x8[4][1]*EE[1] + Add1st)>>Shift1st); 
    Tmp[2][j] = xClipS16((m_TrM8x8[2][0]*EO[0] + m_TrM8x8[2][1]*EO[1] + Add1st)>>Shift1st);
    Tmp[6][j] = xClipS16((m_TrM8x8[6][0]*EO[0] + m_TrM8x8[6][1]*EO[1] + Add1st)>>Shift1st); 

    Tmp[1][j] = xClipS16((m_TrM8x8[1][0]*O[0] + m_TrM8x8[1][1]*O[1] + m_TrM8x8[1][2]*O[2] + m_TrM8x8[1][3]*O[3] + Add1st)>>Shift1st);
    Tmp[3][j] = xClipS16((m_TrM8x8[3][0]*O[0] + m_TrM8x8[3][1]*O[1] + m_TrM8x8[3][2]*O[2] + m_TrM8x8[3][3]*O[3] + Add1st)>>Shift1st);
    Tmp[5][j] = xClipS16((m_TrM8x8[5][0]*O[0] + m_TrM8x8[5][1]*O[1] + m_TrM8x8[5][2]*O[2] + m_TrM8x8[5][3]*O[3] + Add1st)>>Shift1st);
    Tmp[7][j] = xClipS16((m_TrM8x8[7][0]*O[0] + m_TrM8x8[7][1]*O[1] + m_TrM8x8[7][2]*O[2] + m_TrM8x8[7][3]*O[3] + Add1st)>>Shift1st);
  }

  for (int32 j=0; j<8; j++) //vertical transform
  {    
    /* E and O*/
    for (int32 k=0;k<4;k++)
    {
      E[k] = Tmp[j][k] + Tmp[j][7-k];
      O[k] = Tmp[j][k] - Tmp[j][7-k];
    }    

    /* EE and EO */
    EE[0] = E[0] + E[3];    
    EO[0] = E[0] - E[3];
    EE[1] = E[1] + E[2];
    EO[1] = E[1] - E[2];

    Dst[0*8] = xClipS16((m_TrM8x8[0][0]*EE[0] + m_TrM8x8[0][1]*EE[1] + 256)>>9);
    Dst[4*8] = xClipS16((m_TrM8x8[4][0]*EE[0] + m_TrM8x8[4][1]*EE[1] + 256)>>9); 
    Dst[2*8] = xClipS16((m_TrM8x8[2][0]*EO[0] + m_TrM8x8[2][1]*EO[1] + 256)>>9);
    Dst[6*8] = xClipS16((m_TrM8x8[6][0]*EO[0] + m_TrM8x8[6][1]*EO[1] + 256)>>9); 

    Dst[1*8] = xClipS16((m_TrM8x8[1][0]*O[0] + m_TrM8x8[1][1]*O[1] + m_TrM8x8[1][2]*O[2] + m_TrM8x8[1][3]*O[3] + 256)>>9);
    Dst[3*8] = xClipS16((m_TrM8x8[3][0]*O[0] + m_TrM8x8[3][1]*O[1] + m_TrM8x8[3][2]*O[2] + m_TrM8x8[3][3]*O[3] + 256)>>9);
    Dst[5*8] = xClipS16((m_TrM8x8[5][0]*O[0] + m_TrM8x8[5][1]*O[1] + m_TrM8x8[5][2]*O[2] + m_TrM8x8[5][3]*O[3] + 256)>>9);
    Dst[7*8] = xClipS16((m_TrM8x8[7][0]*O[0] + m_TrM8x8[7][1]*O[1] + m_TrM8x8[7][2]*O[2] + m_TrM8x8[7][3]*O[3] + 256)>>9);
    Dst++;
  } 
}
void xTransform::TransformDCT_8x8_SSE(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth)
{
  __m128i Tmp[8];
  const __m128i xT8[8] = {_mm_setr_epi16(64, 64, 64, 64, 64, 64, 64, 64),
                          _mm_setr_epi16(89, 75, 50, 18,-18,-50,-75,-89),
                          _mm_setr_epi16(83, 36,-36,-83,-83,-36, 36, 83),
                          _mm_setr_epi16(75,-18,-89,-50, 50, 89, 18,-75),
                          _mm_setr_epi16(64,-64,-64, 64, 64,-64,-64, 64),
                          _mm_setr_epi16(50,-89, 18, 75,-75,-18, 89,-50),
                          _mm_setr_epi16(36,-83, 83,-36,-36, 83,-83, 36),
                          _mm_setr_epi16(18,-50, 75,-89, 89,-75, 50,-18)};

  const int32 Shift1st = BitDepth - 6;
  const __m128i Add1st = _mm_set1_epi32(1<<(Shift1st-1));
  const __m128i Add2nd = _mm_set1_epi32(256);
  
  //load and horizontal transform
  for(int32 j=0; j<8; j++)
  {
    __m128i line_j = _mm_loadu_si128((__m128i*)Src);
    Src += SrcStride;
    
    __m128i tr_0 = _mm_madd_epi16(line_j, xT8[0]);
    __m128i tr_1 = _mm_madd_epi16(line_j, xT8[1]);
    __m128i tr_2 = _mm_madd_epi16(line_j, xT8[2]);
    __m128i tr_3 = _mm_madd_epi16(line_j, xT8[3]);
    __m128i tr_4 = _mm_madd_epi16(line_j, xT8[4]);
    __m128i tr_5 = _mm_madd_epi16(line_j, xT8[5]);
    __m128i tr_6 = _mm_madd_epi16(line_j, xT8[6]);
    __m128i tr_7 = _mm_madd_epi16(line_j, xT8[7]);

    __m128i tr_01 = _mm_hadd_epi32(tr_0, tr_1);
    __m128i tr_23 = _mm_hadd_epi32(tr_2, tr_3);
    __m128i tr_45 = _mm_hadd_epi32(tr_4, tr_5);
    __m128i tr_67 = _mm_hadd_epi32(tr_6, tr_7);

    __m128i tr_0123 = _mm_hadd_epi32(tr_01, tr_23);
    __m128i tr_4567 = _mm_hadd_epi32(tr_45, tr_67);
    
    tr_0123 = _mm_add_epi32(tr_0123, Add1st);
    tr_0123 = _mm_srai_epi32(tr_0123, Shift1st);

    tr_4567 = _mm_add_epi32(tr_4567, Add1st);
    tr_4567 = _mm_srai_epi32(tr_4567, Shift1st);
  
    Tmp[j] = _mm_packs_epi32(tr_0123, tr_4567); 
  } 
  
  //transpose  
  {
    __m128i tr_A0 = _mm_unpacklo_epi16(Tmp[0], Tmp[4]);
    __m128i tr_A1 = _mm_unpackhi_epi16(Tmp[0], Tmp[4]);
    __m128i tr_A2 = _mm_unpacklo_epi16(Tmp[1], Tmp[5]);
    __m128i tr_A3 = _mm_unpackhi_epi16(Tmp[1], Tmp[5]);
    __m128i tr_A4 = _mm_unpacklo_epi16(Tmp[2], Tmp[6]);
    __m128i tr_A5 = _mm_unpackhi_epi16(Tmp[2], Tmp[6]);
    __m128i tr_A6 = _mm_unpacklo_epi16(Tmp[3], Tmp[7]);
    __m128i tr_A7 = _mm_unpackhi_epi16(Tmp[3], Tmp[7]);

    __m128i tr_B0 = _mm_unpacklo_epi16(tr_A0, tr_A4);
    __m128i tr_B1 = _mm_unpackhi_epi16(tr_A0, tr_A4);
    __m128i tr_B2 = _mm_unpacklo_epi16(tr_A1, tr_A5);
    __m128i tr_B3 = _mm_unpackhi_epi16(tr_A1, tr_A5);
    __m128i tr_B4 = _mm_unpacklo_epi16(tr_A2, tr_A6);
    __m128i tr_B5 = _mm_unpackhi_epi16(tr_A2, tr_A6);
    __m128i tr_B6 = _mm_unpacklo_epi16(tr_A3, tr_A7);
    __m128i tr_B7 = _mm_unpackhi_epi16(tr_A3, tr_A7);

    Tmp[0] = _mm_unpacklo_epi16(tr_B0, tr_B4);
    Tmp[1] = _mm_unpackhi_epi16(tr_B0, tr_B4);
    Tmp[2] = _mm_unpacklo_epi16(tr_B1, tr_B5);
    Tmp[3] = _mm_unpackhi_epi16(tr_B1, tr_B5);
    Tmp[4] = _mm_unpacklo_epi16(tr_B2, tr_B6);
    Tmp[5] = _mm_unpackhi_epi16(tr_B2, tr_B6);
    Tmp[6] = _mm_unpacklo_epi16(tr_B3, tr_B7);
    Tmp[7] = _mm_unpackhi_epi16(tr_B3, tr_B7);
  }  

  //vertical transform
  for(int32 j=0; j<8; j++)
  {
    __m128i line_j = Tmp[j];    
    
    __m128i tr_0 = _mm_madd_epi16(line_j, xT8[0]);
    __m128i tr_1 = _mm_madd_epi16(line_j, xT8[1]);
    __m128i tr_2 = _mm_madd_epi16(line_j, xT8[2]);
    __m128i tr_3 = _mm_madd_epi16(line_j, xT8[3]);
    __m128i tr_4 = _mm_madd_epi16(line_j, xT8[4]);
    __m128i tr_5 = _mm_madd_epi16(line_j, xT8[5]);
    __m128i tr_6 = _mm_madd_epi16(line_j, xT8[6]);
    __m128i tr_7 = _mm_madd_epi16(line_j, xT8[7]);

    __m128i tr_01 = _mm_hadd_epi32(tr_0, tr_1);
    __m128i tr_23 = _mm_hadd_epi32(tr_2, tr_3);
    __m128i tr_45 = _mm_hadd_epi32(tr_4, tr_5);
    __m128i tr_67 = _mm_hadd_epi32(tr_6, tr_7);

    __m128i tr_0123 = _mm_hadd_epi32(tr_01, tr_23);
    __m128i tr_4567 = _mm_hadd_epi32(tr_45, tr_67);
    
    tr_0123 = _mm_add_epi32(tr_0123, Add2nd);
    tr_0123 = _mm_srai_epi32(tr_0123, 9);

    tr_4567 = _mm_add_epi32(tr_4567, Add2nd);
    tr_4567 = _mm_srai_epi32(tr_4567, 9);
  
    Tmp[j] = _mm_packs_epi32(tr_0123, tr_4567); 
  }  

  //transpose 
  {
    __m128i tr_A0 = _mm_unpacklo_epi16(Tmp[0], Tmp[4]);
    __m128i tr_A1 = _mm_unpackhi_epi16(Tmp[0], Tmp[4]);
    __m128i tr_A2 = _mm_unpacklo_epi16(Tmp[1], Tmp[5]);
    __m128i tr_A3 = _mm_unpackhi_epi16(Tmp[1], Tmp[5]);
    __m128i tr_A4 = _mm_unpacklo_epi16(Tmp[2], Tmp[6]);
    __m128i tr_A5 = _mm_unpackhi_epi16(Tmp[2], Tmp[6]);
    __m128i tr_A6 = _mm_unpacklo_epi16(Tmp[3], Tmp[7]);
    __m128i tr_A7 = _mm_unpackhi_epi16(Tmp[3], Tmp[7]);

    __m128i tr_B0 = _mm_unpacklo_epi16(tr_A0, tr_A4);
    __m128i tr_B1 = _mm_unpackhi_epi16(tr_A0, tr_A4);
    __m128i tr_B2 = _mm_unpacklo_epi16(tr_A1, tr_A5);
    __m128i tr_B3 = _mm_unpackhi_epi16(tr_A1, tr_A5);
    __m128i tr_B4 = _mm_unpacklo_epi16(tr_A2, tr_A6);
    __m128i tr_B5 = _mm_unpackhi_epi16(tr_A2, tr_A6);
    __m128i tr_B6 = _mm_unpacklo_epi16(tr_A3, tr_A7);
    __m128i tr_B7 = _mm_unpackhi_epi16(tr_A3, tr_A7);

    Tmp[0] = _mm_unpacklo_epi16(tr_B0, tr_B4);
    Tmp[1] = _mm_unpackhi_epi16(tr_B0, tr_B4);
    Tmp[2] = _mm_unpacklo_epi16(tr_B1, tr_B5);
    Tmp[3] = _mm_unpackhi_epi16(tr_B1, tr_B5);
    Tmp[4] = _mm_unpacklo_epi16(tr_B2, tr_B6);
    Tmp[5] = _mm_unpackhi_epi16(tr_B2, tr_B6);
    Tmp[6] = _mm_unpacklo_epi16(tr_B3, tr_B7);
    Tmp[7] = _mm_unpackhi_epi16(tr_B3, tr_B7);
  }  

  //store
  for(int32 j=0; j<8; j++)
  {
    _mm_storeu_si128((__m128i*)Dst, Tmp[j]);
    Dst += 8;   
  } 
}
void xTransform::TransformDCT_8x8_SSEv2(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth)
{
  __m128i Tmp[8];
  const __m128i xT8[8] = {_mm_setr_epi16(64, 64, 64, 64, 64, 64, 64, 64),
                          _mm_setr_epi16(89, 75, 50, 18,-18,-50,-75,-89),
                          _mm_setr_epi16(83, 36,-36,-83,-83,-36, 36, 83),
                          _mm_setr_epi16(75,-18,-89,-50, 50, 89, 18,-75),
                          _mm_setr_epi16(64,-64,-64, 64, 64,-64,-64, 64),
                          _mm_setr_epi16(50,-89, 18, 75,-75,-18, 89,-50),
                          _mm_setr_epi16(36,-83, 83,-36,-36, 83,-83, 36),
                          _mm_setr_epi16(18,-50, 75,-89, 89,-75, 50,-18)};

  const int32 Shift1st = BitDepth - 6;
  const __m128i Add1st = _mm_set1_epi32(1<<(Shift1st-1));
  const __m128i Add2nd = _mm_set1_epi32(256);
  
  //load and horizontal transform
  for(int32 j=0; j<8; j++)
  {
    __m128i line_j = _mm_loadu_si128((__m128i*)Src);
    Src += SrcStride;
    
    __m128i tr_0 = _mm_madd_epi16(line_j, xT8[0]);
    __m128i tr_1 = _mm_madd_epi16(line_j, xT8[1]);
    __m128i tr_2 = _mm_madd_epi16(line_j, xT8[2]);
    __m128i tr_3 = _mm_madd_epi16(line_j, xT8[3]);
    __m128i tr_4 = _mm_madd_epi16(line_j, xT8[4]);
    __m128i tr_5 = _mm_madd_epi16(line_j, xT8[5]);
    __m128i tr_6 = _mm_madd_epi16(line_j, xT8[6]);
    __m128i tr_7 = _mm_madd_epi16(line_j, xT8[7]);

    __m128i tr_01 = _mm_hadd_epi32(tr_0, tr_1);
    __m128i tr_23 = _mm_hadd_epi32(tr_2, tr_3);
    __m128i tr_45 = _mm_hadd_epi32(tr_4, tr_5);
    __m128i tr_67 = _mm_hadd_epi32(tr_6, tr_7);

    __m128i tr_0123 = _mm_hadd_epi32(tr_01, tr_23);
    __m128i tr_4567 = _mm_hadd_epi32(tr_45, tr_67);
    
    tr_0123 = _mm_add_epi32(tr_0123, Add1st);
    tr_0123 = _mm_srai_epi32(tr_0123, Shift1st);

    tr_4567 = _mm_add_epi32(tr_4567, Add1st);
    tr_4567 = _mm_srai_epi32(tr_4567, Shift1st);
  
    Tmp[j] = _mm_packs_epi32(tr_0123, tr_4567); 
  } 

  //vertical transform
  for(int32 i=0; i<8; i+=2)
  {
    //even
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();

      for(int32 j=0; j<8; j+=2)
      {
        __m128i Line1 = Tmp[j  ];
        __m128i Line2 = Tmp[7-j];
        __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));
        __m128i LineA = _mm_add_epi32(Line1A, Line2A);
        __m128i LineB = _mm_add_epi32(Line1B, Line2B); 
        __m128i TX = _mm_set1_epi32(m_TrM8x8[i][j]);
        LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 9);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 9);

      _mm_storeu_si128((__m128i*)Dst, _mm_packs_epi32(SumA, SumB));
      Dst += 8;
    }

    //odd
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();

      for(int32 j=0; j<8; j+=2)
      {
        __m128i Line1 = Tmp[j  ];
        __m128i Line2 = Tmp[7-j];
        __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));
        __m128i LineA = _mm_sub_epi32(Line1A, Line2A);
        __m128i LineB = _mm_sub_epi32(Line1B, Line2B); 
        __m128i TX = _mm_set1_epi32(m_TrM8x8[i+1][j]);
        LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 9);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 9);

      _mm_storeu_si128((__m128i*)Dst, _mm_packs_epi32(SumA, SumB));
      Dst += 8;
    }
  }   
}
void xTransform::TransformDCT_16x16_STD_C(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[8],O[8];
  int32 EE[4],EO[4];
  int32 EEE[2],EEO[2];
  int32 Tmp[16][16];
  int32 Shift1st = BitDepth - 5; //+1 per size
  int32 Add1st   = 1<<(Shift1st-1); 

  for (int32 j=0; j<16; j++)  //horizontal transform
  {    
    for (int32 k=0;k<8;k++)
    {
      E[k] = Src[k] + Src[15-k];
      O[k] = Src[k] - Src[15-k];
    } 
    Src += SrcStride;

    for(int32 k=0;k<4;k++)
    {
      EE[k] = E[k] + E[7-k];
      EO[k] = E[k] - E[7-k];
    }

    EEE[0] = EE[0] + EE[3];    
    EEO[0] = EE[0] - EE[3];
    EEE[1] = EE[1] + EE[2];
    EEO[1] = EE[1] - EE[2];

    Tmp[ 0][j] = xClipS16((((EEE[0] + EEE[1])<<6) + Add1st)>>Shift1st);        //store with transposition  
    Tmp[ 8][j] = xClipS16((((EEE[0] - EEE[1])<<6) + Add1st)>>Shift1st); 
    Tmp[ 4][j] = xClipS16((83*EEO[0] + 36*EEO[1] + Add1st)>>Shift1st);        
    Tmp[12][j] = xClipS16((36*EEO[0] - 83*EEO[1] + Add1st)>>Shift1st);

    Tmp[ 2][j] = xClipS16((89*EO[0] + 75*EO[1] + 50*EO[2] + 18*EO[3] + Add1st)>>Shift1st);
    Tmp[ 6][j] = xClipS16((75*EO[0] - 18*EO[1] - 89*EO[2] - 50*EO[3] + Add1st)>>Shift1st);
    Tmp[10][j] = xClipS16((50*EO[0] - 89*EO[1] + 18*EO[2] + 75*EO[3] + Add1st)>>Shift1st);
    Tmp[14][j] = xClipS16((18*EO[0] - 50*EO[1] + 75*EO[2] - 89*EO[3] + Add1st)>>Shift1st);

    Tmp[ 1][j] = xClipS16((90*O[0] + 87*O[1] + 80*O[2] + 70*O[3] + 57*O[4] + 43*O[5] + 25*O[6] + 9*O[7] + Add1st)>>Shift1st);
    Tmp[ 3][j] = xClipS16((87*O[0] + 57*O[1] + 9*O[2] - 43*O[3] - 80*O[4] - 90*O[5] - 70*O[6] - 25*O[7] + Add1st)>>Shift1st);
    Tmp[ 5][j] = xClipS16((80*O[0] + 9*O[1] - 70*O[2] - 87*O[3] - 25*O[4] + 57*O[5] + 90*O[6] + 43*O[7] + Add1st)>>Shift1st);
    Tmp[ 7][j] = xClipS16((70*O[0] - 43*O[1] - 87*O[2] + 9*O[3] + 90*O[4] + 25*O[5] - 80*O[6] - 57*O[7] + Add1st)>>Shift1st);
    Tmp[ 9][j] = xClipS16((57*O[0] - 80*O[1] - 25*O[2] + 90*O[3] - 9*O[4] - 87*O[5] + 43*O[6] + 70*O[7] + Add1st)>>Shift1st);
    Tmp[11][j] = xClipS16((43*O[0] - 90*O[1] + 57*O[2] + 25*O[3] - 87*O[4] + 70*O[5] + 9*O[6] - 80*O[7] + Add1st)>>Shift1st);
    Tmp[13][j] = xClipS16((25*O[0] - 70*O[1] + 90*O[2] - 80*O[3] + 43*O[4] + 9*O[5] - 57*O[6] + 87*O[7] + Add1st)>>Shift1st);
    Tmp[15][j] = xClipS16((9*O[0] - 25*O[1] + 43*O[2] - 57*O[3] + 70*O[4] - 80*O[5] + 87*O[6] - 90*O[7] + Add1st)>>Shift1st);
  }

  for(int32 j=0; j<16; j++)  //vertical transform
  {    
    for (int32 k=0;k<8;k++)
    {
      E[k] = Tmp[j][k] + Tmp[j][15-k];
      O[k] = Tmp[j][k] - Tmp[j][15-k];
    } 

    for (int32 k=0;k<4;k++)
    {
      EE[k] = E[k] + E[7-k];
      EO[k] = E[k] - E[7-k];
    }

    EEE[0] = EE[0] + EE[3];    
    EEO[0] = EE[0] - EE[3];
    EEE[1] = EE[1] + EE[2];
    EEO[1] = EE[1] - EE[2];

    Dst[ 0     ] = xClipS16((((EEE[0] + EEE[1])<<6) + 512)>>10);        //store with transposition
    Dst[ 8*16  ] = xClipS16((((EEE[0] - EEE[1])<<6) + 512)>>10);    
    Dst[ 4*16  ] = xClipS16((83*EEO[0] + 36*EEO[1] + 512)>>10);        
    Dst[ 12*16 ] = xClipS16((36*EEO[0] - 83*EEO[1] + 512)>>10);

    Dst[ 2*16  ] = xClipS16((89*EO[0] + 75*EO[1] + 50*EO[2] + 18*EO[3] + 512)>>10);
    Dst[ 6*16  ] = xClipS16((75*EO[0] - 18*EO[1] - 89*EO[2] - 50*EO[3] + 512)>>10);
    Dst[ 10*16 ] = xClipS16((50*EO[0] - 89*EO[1] + 18*EO[2] + 75*EO[3] + 512)>>10);
    Dst[ 14*16 ] = xClipS16((18*EO[0] - 50*EO[1] + 75*EO[2] - 89*EO[3] + 512)>>10);

    Dst[ 1*16  ] = xClipS16((90*O[0] + 87*O[1] + 80*O[2] + 70*O[3] + 57*O[4] + 43*O[5] + 25*O[6] + 9*O[7] + 512)>>10);
    Dst[ 3*16  ] = xClipS16((87*O[0] + 57*O[1] + 9*O[2] - 43*O[3] - 80*O[4] - 90*O[5] - 70*O[6] - 25*O[7] + 512)>>10);
    Dst[ 5*16  ] = xClipS16((80*O[0] + 9*O[1] - 70*O[2] - 87*O[3] - 25*O[4] + 57*O[5] + 90*O[6] + 43*O[7] + 512)>>10);
    Dst[ 7*16  ] = xClipS16((70*O[0] - 43*O[1] - 87*O[2] + 9*O[3] + 90*O[4] + 25*O[5] - 80*O[6] - 57*O[7] + 512)>>10);
    Dst[ 9*16  ] = xClipS16((57*O[0] - 80*O[1] - 25*O[2] + 90*O[3] - 9*O[4] - 87*O[5] + 43*O[6] + 70*O[7] + 512)>>10);
    Dst[ 11*16 ] = xClipS16((43*O[0] - 90*O[1] + 57*O[2] + 25*O[3] - 87*O[4] + 70*O[5] + 9*O[6] - 80*O[7] + 512)>>10);
    Dst[ 13*16 ] = xClipS16((25*O[0] - 70*O[1] + 90*O[2] - 80*O[3] + 43*O[4] + 9*O[5] - 57*O[6] + 87*O[7] + 512)>>10);
    Dst[ 15*16 ] = xClipS16((9*O[0] - 25*O[1] + 43*O[2] - 57*O[3] + 70*O[4] - 80*O[5] + 87*O[6] - 90*O[7] + 512)>>10);
    Dst++;
  } 
}
void xTransform::TransformDCT_16x16_STD_M(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[8],O[8];
  int32 EE[4],EO[4];
  int32 EEE[2],EEO[2];
  int32 Tmp[16][16];
  int32 Shift1st = BitDepth - 5; //+1 per size
  int32 Add1st   = 1<<(Shift1st-1); 

  for (int32 j=0; j<16; j++)  //horizontal transform
  {    
    for (int32 k=0;k<8;k++)
    {
      E[k] = Src[k] + Src[15-k];
      O[k] = Src[k] - Src[15-k];
    } 
    Src += SrcStride;

    for(int32 k=0;k<4;k++)
    {
      EE[k] = E[k] + E[7-k];
      EO[k] = E[k] - E[7-k];
    }

    EEE[0] = EE[0] + EE[3];    
    EEO[0] = EE[0] - EE[3];
    EEE[1] = EE[1] + EE[2];
    EEO[1] = EE[1] - EE[2];

    Tmp[ 0][j] = xClipS16((((EEE[0] + EEE[1])<<6) + Add1st)>>Shift1st);        //store with transposition  
    Tmp[ 8][j] = xClipS16((((EEE[0] - EEE[1])<<6) + Add1st)>>Shift1st); 
    Tmp[ 4][j] = xClipS16((83*EEO[0] + 36*EEO[1] + Add1st)>>Shift1st);        
    Tmp[12][j] = xClipS16((36*EEO[0] - 83*EEO[1] + Add1st)>>Shift1st);

    for (int32 k=2;k<16;k+=4)
    {
      Tmp[k][j] = (m_TrM16x16[k][0]*EO[0] + m_TrM16x16[k][1]*EO[1] + m_TrM16x16[k][2]*EO[2] + m_TrM16x16[k][3]*EO[3] + Add1st)>>Shift1st;      
    }

    for (int32 k=1;k<16;k+=2)
    {
      Tmp[k][j] = (m_TrM16x16[k][0]*O[0] + m_TrM16x16[k][1]*O[1] + m_TrM16x16[k][2]*O[2] + m_TrM16x16[k][3]*O[3] + 
                   m_TrM16x16[k][4]*O[4] + m_TrM16x16[k][5]*O[5] + m_TrM16x16[k][6]*O[6] + m_TrM16x16[k][7]*O[7] + Add1st)>>Shift1st;
    }
  }

  for(int32 j=0; j<16; j++)  //vertical transform
  {    
    for(int32 k=0; k<8; k++)
    {
      E[k] = Tmp[j][k] + Tmp[j][15-k];
      O[k] = Tmp[j][k] - Tmp[j][15-k];
    } 

    for(int32 k=0; k<4; k++)
    {
      EE[k] = E[k] + E[7-k];
      EO[k] = E[k] - E[7-k];
    }

    EEE[0] = EE[0] + EE[3];    
    EEO[0] = EE[0] - EE[3];
    EEE[1] = EE[1] + EE[2];
    EEO[1] = EE[1] - EE[2];

    Dst[ 0     ] = xClipS16((64*EEE[0] + 64*EEE[1] + 512)>>10);        //store with transposition
    Dst[ 8*16  ] = xClipS16((64*EEE[0] - 64*EEE[1] + 512)>>10);    
    Dst[ 4*16  ] = xClipS16((83*EEO[0] + 36*EEO[1] + 512)>>10);        
    Dst[ 12*16 ] = xClipS16((36*EEO[0] - 83*EEO[1] + 512)>>10);

    for(int32 k=2; k<16; k+=4)
    {
      Dst[k*16] = xClipS16((m_TrM16x16[k][0]*EO[0] + m_TrM16x16[k][1]*EO[1] + m_TrM16x16[k][2]*EO[2] + m_TrM16x16[k][3]*EO[3] + 512)>>10);      
    }

    for(int32 k=1; k<16; k+=2)
    {
      Dst[k*16] = xClipS16((m_TrM16x16[k][0]*O[0] + m_TrM16x16[k][1]*O[1] + m_TrM16x16[k][2]*O[2] + m_TrM16x16[k][3]*O[3] + 
                              m_TrM16x16[k][4]*O[4] + m_TrM16x16[k][5]*O[5] + m_TrM16x16[k][6]*O[6] + m_TrM16x16[k][7]*O[7] + 512)>>10);
    } 
    Dst++;
  } 
}
void xTransform::TransformDCT_16x16_SSE(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth) //horizontal transform->transpose->vertical transform->transpose
{
  static const __m128i xT16_DCT[32] = {
  _mm_setr_epi16( 64, 64, 64, 64, 64, 64, 64, 64), _mm_setr_epi16( 64, 64, 64, 64, 64, 64, 64, 64),
  _mm_setr_epi16( 90, 87, 80, 70, 57, 43, 25,  9), _mm_setr_epi16( -9,-25,-43,-57,-70,-80,-87,-90),
  _mm_setr_epi16( 89, 75, 50, 18,-18,-50,-75,-89), _mm_setr_epi16(-89,-75,-50,-18, 18, 50, 75, 89),
  _mm_setr_epi16( 87, 57,  9,-43,-80,-90,-70,-25), _mm_setr_epi16( 25, 70, 90, 80, 43, -9,-57,-87),
  _mm_setr_epi16( 83, 36,-36,-83,-83,-36, 36, 83), _mm_setr_epi16( 83, 36,-36,-83,-83,-36, 36, 83),
  _mm_setr_epi16( 80,  9,-70,-87,-25, 57, 90, 43), _mm_setr_epi16(-43,-90,-57, 25, 87, 70, -9,-80),
  _mm_setr_epi16( 75,-18,-89,-50, 50, 89, 18,-75), _mm_setr_epi16(-75, 18, 89, 50,-50,-89,-18, 75),
  _mm_setr_epi16( 70,-43,-87,  9, 90, 25,-80,-57), _mm_setr_epi16( 57, 80,-25,-90, -9, 87, 43,-70),
  _mm_setr_epi16( 64,-64,-64, 64, 64,-64,-64, 64), _mm_setr_epi16( 64,-64,-64, 64, 64,-64,-64, 64),
  _mm_setr_epi16( 57,-80,-25, 90, -9,-87, 43, 70), _mm_setr_epi16(-70,-43, 87,  9,-90, 25, 80,-57),
  _mm_setr_epi16( 50,-89, 18, 75,-75,-18, 89,-50), _mm_setr_epi16(-50, 89,-18,-75, 75, 18,-89, 50),
  _mm_setr_epi16( 43,-90, 57, 25,-87, 70,  9,-80), _mm_setr_epi16( 80, -9,-70, 87,-25,-57, 90,-43),
  _mm_setr_epi16( 36,-83, 83,-36,-36, 83,-83, 36), _mm_setr_epi16( 36,-83, 83,-36,-36, 83,-83, 36),
  _mm_setr_epi16( 25,-70, 90,-80, 43,  9,-57, 87), _mm_setr_epi16(-87, 57, -9,-43, 80,-90, 70,-25),
  _mm_setr_epi16( 18,-50, 75,-89, 89,-75, 50,-18), _mm_setr_epi16(-18, 50,-75, 89,-89, 75,-50, 18),
  _mm_setr_epi16(  9,-25, 43,-57, 70,-80, 87,-90), _mm_setr_epi16( 90,-87, 80,-70, 57,-43, 25, -9)};

  const int32 Shift1st = BitDepth - 5;
  const __m128i Add1st = _mm_set1_epi32(1<<(Shift1st-1));
  const __m128i Add2nd = _mm_set1_epi32(512);
  __m128i tr[16];
  __m128i Tmp_A[32];
  __m128i Tmp_B[32];

  //load and horizontal transform
  for(int32 j=0; j<16; j++)
  {
    __m128i LineA = _mm_loadu_si128((__m128i*)(Src  ));
    __m128i LineB = _mm_loadu_si128((__m128i*)(Src+8));
    Src += SrcStride;    

    for(int32 i=0, k=0; i<32; i+=2, k++)
    {
      tr[k] = _mm_add_epi32(_mm_madd_epi16(LineA, xT16_DCT[i  ]), _mm_madd_epi16(LineB, xT16_DCT[i+1]));
    }

    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add1st), Shift1st);

    Tmp_B[2*j  ] = _mm_packs_epi32(tr[0], tr[1]);
    Tmp_B[2*j+1] = _mm_packs_epi32(tr[2], tr[3]);
  }

  //transpose
  for(int32 i=0; i<16; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[16+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[16+i]);
  }

  for(int32 i=0; i<16; i++)
  {
    Tmp_B[2*i  ] = _mm_unpacklo_epi16(Tmp_A[0+i], Tmp_A[16+i]);
    Tmp_B[2*i+1] = _mm_unpackhi_epi16(Tmp_A[0+i], Tmp_A[16+i]);
  }

  for(int32 i=0; i<16; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[16+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[16+i]);
  }

  for(int32 i=0; i<16; i++)
  {
    Tmp_B[2*i  ] = _mm_unpacklo_epi16(Tmp_A[0+i], Tmp_A[16+i]);
    Tmp_B[2*i+1] = _mm_unpackhi_epi16(Tmp_A[0+i], Tmp_A[16+i]);
  }

  //vertical transform
  for(int32 j=0; j<16; j++)
  {
    __m128i LineA = Tmp_B[2*j  ];
    __m128i LineB = Tmp_B[2*j+1];

    for(int32 i=0, k=0; i<32; i+=2, k++)
    {
      tr[k] = _mm_add_epi32(_mm_madd_epi16(LineA, xT16_DCT[i  ]), _mm_madd_epi16(LineB, xT16_DCT[i+1]));
    }

    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add2nd), 10);

    Tmp_B[2*j  ] = _mm_packs_epi32(tr[0], tr[1]);
    Tmp_B[2*j+1] = _mm_packs_epi32(tr[2], tr[3]);
  }

  //transpose
  for(int32 i=0; i<16; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[16+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[16+i]);
  }

  for(int32 i=0; i<16; i++)
  {
    Tmp_B[2*i  ] = _mm_unpacklo_epi16(Tmp_A[0+i], Tmp_A[16+i]);
    Tmp_B[2*i+1] = _mm_unpackhi_epi16(Tmp_A[0+i], Tmp_A[16+i]);
  }

  for(int32 i=0; i<16; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[16+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[16+i]);
  }

  for(int32 i=0; i<16; i++)
  {
    Tmp_B[2*i  ] = _mm_unpacklo_epi16(Tmp_A[0+i], Tmp_A[16+i]);
    Tmp_B[2*i+1] = _mm_unpackhi_epi16(Tmp_A[0+i], Tmp_A[16+i]);
  }

  //storage
  for(int32 j=0; j<16; j++)
  {
    _mm_storeu_si128((__m128i*)(Dst  ), Tmp_B[2*j  ]);
    _mm_storeu_si128((__m128i*)(Dst+8), Tmp_B[2*j+1]);
    Dst += 16;
  }
}
void xTransform::TransformDCT_16x16_SSEv2(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth) //horizonal transform uses one symmetry, vertical transform uses three symmetries
{
  static const __m128i xT16e_DCT[8] ={_mm_setr_epi16(64, 64, 64, 64, 64, 64, 64, 64),
                                      _mm_setr_epi16(89, 75, 50, 18,-18,-50,-75,-89),
                                      _mm_setr_epi16(83, 36,-36,-83,-83,-36, 36, 83),
                                      _mm_setr_epi16(75,-18,-89,-50, 50, 89, 18,-75),
                                      _mm_setr_epi16(64,-64,-64, 64, 64,-64,-64, 64),
                                      _mm_setr_epi16(50,-89, 18, 75,-75,-18, 89,-50),
                                      _mm_setr_epi16(36,-83, 83,-36,-36, 83,-83, 36),
                                      _mm_setr_epi16(18,-50, 75,-89, 89,-75, 50,-18)};

  static const __m128i xT16o_DCT[8] ={_mm_setr_epi16(90, 87, 80, 70, 57, 43, 25,  9),
                                      _mm_setr_epi16(87, 57,  9,-43,-80,-90,-70,-25),
                                      _mm_setr_epi16(80,  9,-70,-87,-25, 57, 90, 43),
                                      _mm_setr_epi16(70,-43,-87,  9, 90, 25,-80,-57),
                                      _mm_setr_epi16(57,-80,-25, 90, -9,-87, 43, 70),
                                      _mm_setr_epi16(43,-90, 57, 25,-87, 70,  9,-80),
                                      _mm_setr_epi16(25,-70, 90,-80, 43,  9,-57, 87),
                                      _mm_setr_epi16( 9,-25, 43,-57, 70,-80, 87,-90)};

  const int32 Shift1st = BitDepth - 5;
  const __m128i Add1st = _mm_set1_epi32(1<<(Shift1st-1));
  const __m128i Add2nd = _mm_set1_epi32(512);
  __m128i Tmp[32];

  //load and horizontal transform
  for(int32 j=0; j<16; j++)
  {
    __m128i tr[16];
    __m128i LineA = _mm_loadu_si128((__m128i*)(Src  ));
    __m128i LineB = _mm_loadu_si128((__m128i*)(Src+8));
    Src += SrcStride;    

    //reverse order
    LineB = _mm_shufflehi_epi16(LineB, (0<<6)|(1<<4)|(2<<2)|(3<<0));
    LineB = _mm_shufflelo_epi16(LineB, (0<<6)|(1<<4)|(2<<2)|(3<<0));
    LineB = _mm_shuffle_epi32  (LineB, (1<<6)|(0<<4)|(3<<2)|(2<<0));

    __m128i LineE = _mm_add_epi16(LineA, LineB);
    __m128i LineO = _mm_sub_epi16(LineA, LineB);

    for(int32 i=0, k=0; i<16; i+=2, k++)
    {
      tr[i  ] = _mm_madd_epi16(LineE, xT16e_DCT[k]);
      tr[i+1] = _mm_madd_epi16(LineO, xT16o_DCT[k]);
    }

    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add1st), Shift1st);

    Tmp[2*j  ] = _mm_packs_epi32(tr[0], tr[1]);
    Tmp[2*j+1] = _mm_packs_epi32(tr[2], tr[3]);
  }

  //vertical transform
  for(int32 i=0; i<16; i+=4)
  {  
    //even even
    if(i==0) // first line - multiplication by 64
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

      for(int32 j=0, k=0; j<4; j++, k+=2)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
        __m128i Line_2A = Tmp[14-k  ];
        __m128i Line_2B = Tmp[14-k+1];
        __m128i Line_3A = Tmp[16+k  ];
        __m128i Line_3B = Tmp[16+k+1];
        __m128i Line_4A = Tmp[30-k  ];
        __m128i Line_4B = Tmp[30-k+1];

        __m128i LineA = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1A), _mm_cvtepi16_epi32(Line_2A)),
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3A), _mm_cvtepi16_epi32(Line_4A)));
        __m128i LineB = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2A,8))),
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4A,8))));
        __m128i LineC = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1B), _mm_cvtepi16_epi32(Line_2B)), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3B), _mm_cvtepi16_epi32(Line_4B)));
        __m128i LineD = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2B,8))), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4B,8))));

        LineA = _mm_slli_epi32(LineA, 6);
        LineB = _mm_slli_epi32(LineB, 6);
        LineC = _mm_slli_epi32(LineC, 6);
        LineD = _mm_slli_epi32(LineD, 6);
        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 10);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 10);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 10);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 10);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));
      Dst += 16;
    }
    else
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

      for(int32 j=0, k=0; j<4; j++, k+=2)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
        __m128i Line_2A = Tmp[14-k  ];
        __m128i Line_2B = Tmp[14-k+1];
        __m128i Line_3A = Tmp[16+k  ];
        __m128i Line_3B = Tmp[16+k+1];
        __m128i Line_4A = Tmp[30-k  ];
        __m128i Line_4B = Tmp[30-k+1];

        __m128i LineA = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1A), _mm_cvtepi16_epi32(Line_2A)),
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3A), _mm_cvtepi16_epi32(Line_4A)));
        __m128i LineB = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2A,8))),
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4A,8))));
        __m128i LineC = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1B), _mm_cvtepi16_epi32(Line_2B)), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3B), _mm_cvtepi16_epi32(Line_4B)));
        __m128i LineD = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2B,8))), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4B,8))));

        __m128i TX = _mm_set1_epi32(m_TrM16x16[i][j]);
        LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        LineC = _mm_mullo_epi32(LineC, TX);
        LineD = _mm_mullo_epi32(LineD, TX);
        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 10);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 10);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 10);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 10);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));
      Dst += 16;
    }
    //odd
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

      for(int32 j=0, k=0; j<8; j++, k+=2)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
        __m128i Line_2A = Tmp[30-k  ];
        __m128i Line_2B = Tmp[30-k+1];

        __m128i Line1A = _mm_cvtepi16_epi32(Line_1A);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1A,8));
        __m128i Line1C = _mm_cvtepi16_epi32(Line_1B);
        __m128i Line1D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1B,8));
        __m128i Line2A = _mm_cvtepi16_epi32(Line_2A);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2A,8));
        __m128i Line2C = _mm_cvtepi16_epi32(Line_2B);
        __m128i Line2D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2B,8));

        __m128i LineA = _mm_sub_epi32(Line1A, Line2A);
        __m128i LineB = _mm_sub_epi32(Line1B, Line2B);
        __m128i LineC = _mm_sub_epi32(Line1C, Line2C);
        __m128i LineD = _mm_sub_epi32(Line1D, Line2D);

        __m128i TX = _mm_set1_epi32(m_TrM16x16[i+1][j]);
        LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        LineC = _mm_mullo_epi32(LineC, TX);
        LineD = _mm_mullo_epi32(LineD, TX);
        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 10);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 10);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 10);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 10);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));
      Dst += 16;
    }
    //even odd
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

      for(int32 j=0, k=0; j<4; j++, k+=2)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
        __m128i Line_2A = Tmp[14-k  ];
        __m128i Line_2B = Tmp[14-k+1];
        __m128i Line_3A = Tmp[16+k  ];
        __m128i Line_3B = Tmp[16+k+1];
        __m128i Line_4A = Tmp[30-k  ];
        __m128i Line_4B = Tmp[30-k+1];

        __m128i Line1A = _mm_cvtepi16_epi32(Line_1A);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1A,8));
        __m128i Line1C = _mm_cvtepi16_epi32(Line_1B);
        __m128i Line1D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1B,8));
        __m128i Line2A = _mm_cvtepi16_epi32(Line_2A);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2A,8));
        __m128i Line2C = _mm_cvtepi16_epi32(Line_2B);
        __m128i Line2D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2B,8));
        __m128i Line3A = _mm_cvtepi16_epi32(Line_3A);
        __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_3A,8));
        __m128i Line3C = _mm_cvtepi16_epi32(Line_3B);
        __m128i Line3D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_3B,8));
        __m128i Line4A = _mm_cvtepi16_epi32(Line_4A);
        __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_4A,8));
        __m128i Line4C = _mm_cvtepi16_epi32(Line_4B);
        __m128i Line4D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_4B,8));

        __m128i LineA = _mm_sub_epi32(_mm_add_epi32(Line1A, Line4A), _mm_add_epi32(Line3A, Line2A));
        __m128i LineB = _mm_sub_epi32(_mm_add_epi32(Line1B, Line4B), _mm_add_epi32(Line3B, Line2B));
        __m128i LineC = _mm_sub_epi32(_mm_add_epi32(Line1C, Line4C), _mm_add_epi32(Line3C, Line2C));
        __m128i LineD = _mm_sub_epi32(_mm_add_epi32(Line1D, Line4D), _mm_add_epi32(Line3D, Line2D));

        __m128i TX = _mm_set1_epi32(m_TrM16x16[i+2][j]);
        LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        LineC = _mm_mullo_epi32(LineC, TX);
        LineD = _mm_mullo_epi32(LineD, TX);
        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 10);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 10);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 10);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 10);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));
      Dst += 16;
    }
    //odd
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

      for(int32 j=0, k=0; j<8; j++, k+=2)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
        __m128i Line_2A = Tmp[30-k  ];
        __m128i Line_2B = Tmp[30-k+1];

        __m128i Line1A = _mm_cvtepi16_epi32(Line_1A);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1A,8));
        __m128i Line1C = _mm_cvtepi16_epi32(Line_1B);
        __m128i Line1D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1B,8));
        __m128i Line2A = _mm_cvtepi16_epi32(Line_2A);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2A,8));
        __m128i Line2C = _mm_cvtepi16_epi32(Line_2B);
        __m128i Line2D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2B,8));

        __m128i LineA = _mm_sub_epi32(Line1A, Line2A);
        __m128i LineB = _mm_sub_epi32(Line1B, Line2B);
        __m128i LineC = _mm_sub_epi32(Line1C, Line2C);
        __m128i LineD = _mm_sub_epi32(Line1D, Line2D);

        __m128i TX = _mm_set1_epi32(m_TrM16x16[i+3][j]);
        LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        LineC = _mm_mullo_epi32(LineC, TX);
        LineD = _mm_mullo_epi32(LineD, TX);
        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 10);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 10);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 10);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 10);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));
      Dst += 16;
    }
  }    
}
void xTransform::TransformDCT_16x16_SSEv3(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth) //slower than SSEv2, vertical transf. uses 4 symmetries
{
  static const __m128i xT16e_DCT[8] ={_mm_setr_epi16(64, 64, 64, 64, 64, 64, 64, 64),
                                      _mm_setr_epi16(89, 75, 50, 18,-18,-50,-75,-89),
                                      _mm_setr_epi16(83, 36,-36,-83,-83,-36, 36, 83),
                                      _mm_setr_epi16(75,-18,-89,-50, 50, 89, 18,-75),
                                      _mm_setr_epi16(64,-64,-64, 64, 64,-64,-64, 64),
                                      _mm_setr_epi16(50,-89, 18, 75,-75,-18, 89,-50),
                                      _mm_setr_epi16(36,-83, 83,-36,-36, 83,-83, 36),
                                      _mm_setr_epi16(18,-50, 75,-89, 89,-75, 50,-18)};

  static const __m128i xT16o_DCT[8] ={_mm_setr_epi16(90, 87, 80, 70, 57, 43, 25,  9),
                                      _mm_setr_epi16(87, 57,  9,-43,-80,-90,-70,-25),
                                      _mm_setr_epi16(80,  9,-70,-87,-25, 57, 90, 43),
                                      _mm_setr_epi16(70,-43,-87,  9, 90, 25,-80,-57),
                                      _mm_setr_epi16(57,-80,-25, 90, -9,-87, 43, 70),
                                      _mm_setr_epi16(43,-90, 57, 25,-87, 70,  9,-80),
                                      _mm_setr_epi16(25,-70, 90,-80, 43,  9,-57, 87),
                                      _mm_setr_epi16( 9,-25, 43,-57, 70,-80, 87,-90)};

  const int32 Shift1st = BitDepth - 5;
  const __m128i Add1st = _mm_set1_epi32(1<<(Shift1st-1));
  const __m128i Add2nd = _mm_set1_epi32(512);
  const __m128i Add2ndv2 = _mm_set1_epi32(1024);
  __m128i Tmp[32];

  //load and horizontal transform
  for(int32 j=0; j<16; j++)
  {
    __m128i tr[16];
    __m128i LineA = _mm_loadu_si128((__m128i*)(Src  ));
    __m128i LineB = _mm_loadu_si128((__m128i*)(Src+8));
    Src += SrcStride;    

    //reverse order
    LineB = _mm_shufflehi_epi16(LineB, (0<<6)|(1<<4)|(2<<2)|(3<<0));
    LineB = _mm_shufflelo_epi16(LineB, (0<<6)|(1<<4)|(2<<2)|(3<<0));
    LineB = _mm_shuffle_epi32  (LineB, (1<<6)|(0<<4)|(3<<2)|(2<<0));

    __m128i LineE = _mm_add_epi16(LineA, LineB);
    __m128i LineO = _mm_sub_epi16(LineA, LineB);

    for(int32 i=0, k=0; i<16; i+=2, k++)
    {
      tr[i  ] = _mm_madd_epi16(LineE, xT16e_DCT[k]);
      tr[i+1] = _mm_madd_epi16(LineO, xT16o_DCT[k]);
    }

    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add1st), Shift1st);

    Tmp[2*j  ] = _mm_packs_epi32(tr[0], tr[1]);
    Tmp[2*j+1] = _mm_packs_epi32(tr[2], tr[3]);
  }

  //vertical transform
  for(int32 i=0; i<16; i+=4)
  {  
    //even even
    if(i==0)
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

      for(int32 j=0, k=0; j<2; j++, k+=2)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];

    __m128i Line_11A= Tmp[6-k];
        __m128i Line_11B= Tmp[6-k+1];

    __m128i Line_12A= Tmp[8+k];
        __m128i Line_12B= Tmp[8+k+1];

        __m128i Line_2A = Tmp[14-k  ];
        __m128i Line_2B = Tmp[14-k+1];

        __m128i Line_3A = Tmp[16+k  ];
        __m128i Line_3B = Tmp[16+k+1];

    __m128i Line_31A = Tmp[22-k  ];
        __m128i Line_31B = Tmp[22-k+1];

    __m128i Line_32A = Tmp[24+k  ];
        __m128i Line_32B = Tmp[24+k+1];

        __m128i Line_4A = Tmp[30-k  ];
        __m128i Line_4B = Tmp[30-k+1];

    __m128i LineATmp=_mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_11A), _mm_cvtepi16_epi32(Line_12A)),
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_31A), _mm_cvtepi16_epi32(Line_32A)));
        __m128i LineA = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1A), _mm_cvtepi16_epi32(Line_2A)),
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3A), _mm_cvtepi16_epi32(Line_4A)));
    LineA = _mm_add_epi32(LineA,LineATmp);

    __m128i LineBTmp=_mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_11A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_12A,8))),
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_31A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_32A,8))));
        __m128i LineB = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2A,8))),
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4A,8))));
        LineB = _mm_add_epi32(LineB,LineBTmp);

    __m128i LineCTmp=_mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_11B), _mm_cvtepi16_epi32(Line_12B)),
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_31B), _mm_cvtepi16_epi32(Line_32B)));
    __m128i LineC = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1B), _mm_cvtepi16_epi32(Line_2B)), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3B), _mm_cvtepi16_epi32(Line_4B)));
    LineC = _mm_add_epi32(LineC,LineCTmp);

    __m128i LineDTmp=_mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_11B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_12B,8))),
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_31B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_32B,8))));
        __m128i LineD = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2B,8))), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4B,8))));
    LineD = _mm_add_epi32(LineD,LineDTmp);

        LineA = _mm_slli_epi32(LineA, 6);
        LineB = _mm_slli_epi32(LineB, 6);
        LineC = _mm_slli_epi32(LineC, 6);
        LineD = _mm_slli_epi32(LineD, 6);
        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 10);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 10);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 10);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 10);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));
      Dst += 16;
    }
    else
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

      for(int32 j=0, k=0; j<2; j++, k+=2)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];

    __m128i Line_11A= Tmp[6-k];
        __m128i Line_11B= Tmp[6-k+1];

    __m128i Line_12A= Tmp[8+k];
        __m128i Line_12B= Tmp[8+k+1];

        __m128i Line_2A = Tmp[14-k  ];
        __m128i Line_2B = Tmp[14-k+1];
        
    __m128i Line_3A = Tmp[16+k  ];
        __m128i Line_3B = Tmp[16+k+1];

    __m128i Line_31A = Tmp[22-k  ];
        __m128i Line_31B = Tmp[22-k+1];

    __m128i Line_32A = Tmp[24+k  ];
        __m128i Line_32B = Tmp[24+k+1];
        
    __m128i Line_4A = Tmp[30-k  ];
        __m128i Line_4B = Tmp[30-k+1];
    __m128i LineA;
    __m128i LineB;
    __m128i LineC;
    __m128i LineD;
    
      __m128i LineATmp=_mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_11A), _mm_cvtepi16_epi32(Line_12A)),
              _mm_add_epi32(_mm_cvtepi16_epi32(Line_31A), _mm_cvtepi16_epi32(Line_32A)));
      LineA = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1A), _mm_cvtepi16_epi32(Line_2A)),
              _mm_add_epi32(_mm_cvtepi16_epi32(Line_3A), _mm_cvtepi16_epi32(Line_4A)));
      __m128i LineBTmp=_mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_11A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_12A,8))),
              _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_31A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_32A,8))));
      LineB = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2A,8))),
              _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4A,8))));
      __m128i LineCTmp=_mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_11B), _mm_cvtepi16_epi32(Line_12B)),
              _mm_add_epi32(_mm_cvtepi16_epi32(Line_31B), _mm_cvtepi16_epi32(Line_32B)));
      LineC = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1B), _mm_cvtepi16_epi32(Line_2B)), 
              _mm_add_epi32(_mm_cvtepi16_epi32(Line_3B), _mm_cvtepi16_epi32(Line_4B)));
      __m128i LineDTmp=_mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_11B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_12B,8))),
              _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_31B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_32B,8))));
      LineD = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2B,8))), 
              _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4B,8))));
      if(i==4||i==12)
      {
        LineA = _mm_sub_epi32(LineA,LineATmp);
        LineB = _mm_sub_epi32(LineB,LineBTmp);
        LineC = _mm_sub_epi32(LineC,LineCTmp);
        LineD = _mm_sub_epi32(LineD,LineDTmp);
      }
      else
      {
        LineA = _mm_add_epi32(LineA,LineATmp);
        LineB = _mm_add_epi32(LineB,LineBTmp);
        LineC = _mm_add_epi32(LineC,LineCTmp);
        LineD = _mm_add_epi32(LineD,LineDTmp);
      }
    
        __m128i TX = _mm_set1_epi32(m_TrM16x16[i][j]);
        LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        LineC = _mm_mullo_epi32(LineC, TX);
        LineD = _mm_mullo_epi32(LineD, TX);
        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 10);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 10);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 10);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 10);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));
      Dst += 16;
    }
    //odd
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

      for(int32 j=0, k=0; j<8; j++, k+=2)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
        __m128i Line_2A = Tmp[30-k  ];
        __m128i Line_2B = Tmp[30-k+1];

        __m128i Line1A = _mm_cvtepi16_epi32(Line_1A);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1A,8));
        __m128i Line1C = _mm_cvtepi16_epi32(Line_1B);
        __m128i Line1D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1B,8));
        __m128i Line2A = _mm_cvtepi16_epi32(Line_2A);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2A,8));
        __m128i Line2C = _mm_cvtepi16_epi32(Line_2B);
        __m128i Line2D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2B,8));

        __m128i LineA = _mm_sub_epi32(Line1A, Line2A);
        __m128i LineB = _mm_sub_epi32(Line1B, Line2B);
        __m128i LineC = _mm_sub_epi32(Line1C, Line2C);
        __m128i LineD = _mm_sub_epi32(Line1D, Line2D);

        __m128i TX = _mm_set1_epi32(m_TrM16x16[i+1][j]);
        LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        LineC = _mm_mullo_epi32(LineC, TX);
        LineD = _mm_mullo_epi32(LineD, TX);
        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 10);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 10);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 10);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 10);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));
      Dst += 16;
    }
    //even odd
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

      for(int32 j=0, k=0; j<4; j++, k+=2)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
        __m128i Line_2A = Tmp[14-k  ];
        __m128i Line_2B = Tmp[14-k+1];
        __m128i Line_3A = Tmp[16+k  ];
        __m128i Line_3B = Tmp[16+k+1];
        __m128i Line_4A = Tmp[30-k  ];
        __m128i Line_4B = Tmp[30-k+1];

        __m128i Line1A = _mm_cvtepi16_epi32(Line_1A);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1A,8));
        __m128i Line1C = _mm_cvtepi16_epi32(Line_1B);
        __m128i Line1D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1B,8));
        __m128i Line2A = _mm_cvtepi16_epi32(Line_2A);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2A,8));
        __m128i Line2C = _mm_cvtepi16_epi32(Line_2B);
        __m128i Line2D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2B,8));
        __m128i Line3A = _mm_cvtepi16_epi32(Line_3A);
        __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_3A,8));
        __m128i Line3C = _mm_cvtepi16_epi32(Line_3B);
        __m128i Line3D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_3B,8));
        __m128i Line4A = _mm_cvtepi16_epi32(Line_4A);
        __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_4A,8));
        __m128i Line4C = _mm_cvtepi16_epi32(Line_4B);
        __m128i Line4D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_4B,8));

        __m128i LineA = _mm_sub_epi32(_mm_add_epi32(Line1A, Line4A), _mm_add_epi32(Line3A, Line2A));
        __m128i LineB = _mm_sub_epi32(_mm_add_epi32(Line1B, Line4B), _mm_add_epi32(Line3B, Line2B));
        __m128i LineC = _mm_sub_epi32(_mm_add_epi32(Line1C, Line4C), _mm_add_epi32(Line3C, Line2C));
        __m128i LineD = _mm_sub_epi32(_mm_add_epi32(Line1D, Line4D), _mm_add_epi32(Line3D, Line2D));

        __m128i TX = _mm_set1_epi32(m_TrM16x16[i+2][j]);
        LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        LineC = _mm_mullo_epi32(LineC, TX);
        LineD = _mm_mullo_epi32(LineD, TX);
        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 10);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 10);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 10);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 10);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));
      Dst += 16;
    }
    //odd
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

      for(int32 j=0, k=0; j<8; j++, k+=2)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
        __m128i Line_2A = Tmp[30-k  ];
        __m128i Line_2B = Tmp[30-k+1];

        __m128i Line1A = _mm_cvtepi16_epi32(Line_1A);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1A,8));
        __m128i Line1C = _mm_cvtepi16_epi32(Line_1B);
        __m128i Line1D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1B,8));
        __m128i Line2A = _mm_cvtepi16_epi32(Line_2A);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2A,8));
        __m128i Line2C = _mm_cvtepi16_epi32(Line_2B);
        __m128i Line2D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2B,8));

        __m128i LineA = _mm_sub_epi32(Line1A, Line2A);
        __m128i LineB = _mm_sub_epi32(Line1B, Line2B);
        __m128i LineC = _mm_sub_epi32(Line1C, Line2C);
        __m128i LineD = _mm_sub_epi32(Line1D, Line2D);

        __m128i TX = _mm_set1_epi32(m_TrM16x16[i+3][j]);
        LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        LineC = _mm_mullo_epi32(LineC, TX);
        LineD = _mm_mullo_epi32(LineD, TX);
        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 10);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 10);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 10);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 10);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));
      Dst += 16;
    }
  }    
}
void xTransform::TransformDCT_32x32_STD_C(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[16],O[16];
  int32 EE[8],EO[8];
  int32 EEE[4],EEO[4];
  int32 EEEE[2],EEEO[2];
  int32 Tmp[32][32];
  int32 Shift1st = BitDepth - 4;
  int32 Add1st   = 1<<(Shift1st-1);

  for (int32 j=0; j<32; j++)
  {    
    for (int32 k=0;k<16;k++)
    {
      E[k] = Src[k] + Src[31-k];
      O[k] = Src[k] - Src[31-k];
    }
    Src += SrcStride;

    for (int32 k=0;k<8;k++)
    {
      EE[k] = E[k] + E[15-k];
      EO[k] = E[k] - E[15-k];
    }

    for (int32 k=0;k<4;k++)
    {
      EEE[k] = EE[k] + EE[7-k];
      EEO[k] = EE[k] - EE[7-k];
    }

    EEEE[0] = EEE[0] + EEE[3];    
    EEEO[0] = EEE[0] - EEE[3];
    EEEE[1] = EEE[1] + EEE[2];
    EEEO[1] = EEE[1] - EEE[2]; 

    Tmp[ 0][j] = xClipS16((((EEEE[0] + EEEE[1])<<6) + Add1st)>>Shift1st);
    Tmp[16][j] = xClipS16((((EEEE[0] - EEEE[1])<<6) + Add1st)>>Shift1st);
    Tmp[ 8][j] = xClipS16((83*EEEO[0] + 36*EEEO[1] + Add1st)>>Shift1st); 
    Tmp[24][j] = xClipS16((36*EEEO[0] - 83*EEEO[1] + Add1st)>>Shift1st);

    Tmp[ 4][j] = xClipS16((89*EEO[0] + 75*EEO[1] + 50*EEO[2] + 18*EEO[3] + Add1st)>>Shift1st);
    Tmp[12][j] = xClipS16((75*EEO[0] - 18*EEO[1] - 89*EEO[2] - 50*EEO[3] + Add1st)>>Shift1st);
    Tmp[20][j] = xClipS16((50*EEO[0] - 89*EEO[1] + 18*EEO[2] + 75*EEO[3] + Add1st)>>Shift1st);
    Tmp[28][j] = xClipS16((18*EEO[0] - 50*EEO[1] + 75*EEO[2] - 89*EEO[3] + Add1st)>>Shift1st);

    Tmp[ 2][j] = xClipS16((90*EO[0] + 87*EO[1] + 80*EO[2] + 70*EO[3] + 57*EO[4] + 43*EO[5] + 25*EO[6] + 9*EO[7] + Add1st)>>Shift1st);
    Tmp[ 6][j] = xClipS16((87*EO[0] + 57*EO[1] + 9*EO[2] - 43*EO[3] - 80*EO[4] - 90*EO[5] - 70*EO[6] - 25*EO[7] + Add1st)>>Shift1st);
    Tmp[10][j] = xClipS16((80*EO[0] + 9*EO[1] - 70*EO[2] - 87*EO[3] - 25*EO[4] + 57*EO[5] + 90*EO[6] + 43*EO[7] + Add1st)>>Shift1st);
    Tmp[14][j] = xClipS16((70*EO[0] - 43*EO[1] - 87*EO[2] + 9*EO[3] + 90*EO[4] + 25*EO[5] - 80*EO[6] - 57*EO[7] + Add1st)>>Shift1st);
    Tmp[18][j] = xClipS16((57*EO[0] - 80*EO[1] - 25*EO[2] + 90*EO[3] - 9*EO[4] - 87*EO[5] + 43*EO[6] + 70*EO[7] + Add1st)>>Shift1st);
    Tmp[22][j] = xClipS16((43*EO[0] - 90*EO[1] + 57*EO[2] + 25*EO[3] - 87*EO[4] + 70*EO[5] + 9*EO[6] - 80*EO[7] + Add1st)>>Shift1st);
    Tmp[26][j] = xClipS16((25*EO[0] - 70*EO[1] + 90*EO[2] - 80*EO[3] + 43*EO[4] + 9*EO[5] - 57*EO[6] + 87*EO[7] + Add1st)>>Shift1st);
    Tmp[30][j] = xClipS16((9*EO[0] - 25*EO[1] + 43*EO[2] - 57*EO[3] + 70*EO[4] - 80*EO[5] + 87*EO[6] - 90*EO[7] + Add1st)>>Shift1st);

    Tmp[ 1][j] = xClipS16((90*O[ 0] + 90*O[ 1] + 88*O[ 2] + 85*O[ 3] + 82*O[ 4] + 78*O[ 5] + 73*O[ 6] + 67*O[ 7] + 61*O[ 8] + 54*O[ 9] + 46*O[10] + 38*O[11] + 31*O[12] + 22*O[13] + 13*O[14] +  4*O[15] + Add1st)>>Shift1st);
    Tmp[ 3][j] = xClipS16((90*O[ 0] + 82*O[ 1] + 67*O[ 2] + 46*O[ 3] + 22*O[ 4] -  4*O[ 5] - 31*O[ 6] - 54*O[ 7] - 73*O[ 8] - 85*O[ 9] - 90*O[10] - 88*O[11] - 78*O[12] - 61*O[13] - 38*O[14] - 13*O[15] + Add1st)>>Shift1st);
    Tmp[ 5][j] = xClipS16((88*O[ 0] + 67*O[ 1] + 31*O[ 2] - 13*O[ 3] - 54*O[ 4] - 82*O[ 5] - 90*O[ 6] - 78*O[ 7] - 46*O[ 8] -  4*O[ 9] + 38*O[10] + 73*O[11] + 90*O[12] + 85*O[13] + 61*O[14] + 22*O[15] + Add1st)>>Shift1st);
    Tmp[ 7][j] = xClipS16((85*O[ 0] + 46*O[ 1] - 13*O[ 2] - 67*O[ 3] - 90*O[ 4] - 73*O[ 5] - 22*O[ 6] + 38*O[ 7] + 82*O[ 8] + 88*O[ 9] + 54*O[10] -  4*O[11] - 61*O[12] - 90*O[13] - 78*O[14] - 31*O[15] + Add1st)>>Shift1st);
    Tmp[ 9][j] = xClipS16((82*O[ 0] + 22*O[ 1] - 54*O[ 2] - 90*O[ 3] - 61*O[ 4] + 13*O[ 5] + 78*O[ 6] + 85*O[ 7] + 31*O[ 8] - 46*O[ 9] - 90*O[10] - 67*O[11] +  4*O[12] + 73*O[13] + 88*O[14] + 38*O[15] + Add1st)>>Shift1st);
    Tmp[11][j] = xClipS16((78*O[ 0] -  4*O[ 1] - 82*O[ 2] - 73*O[ 3] + 13*O[ 4] + 85*O[ 5] + 67*O[ 6] - 22*O[ 7] - 88*O[ 8] - 61*O[ 9] + 31*O[10] + 90*O[11] + 54*O[12] - 38*O[13] - 90*O[14] - 46*O[15] + Add1st)>>Shift1st);
    Tmp[13][j] = xClipS16((73*O[ 0] - 31*O[ 1] - 90*O[ 2] - 22*O[ 3] + 78*O[ 4] + 67*O[ 5] - 38*O[ 6] - 90*O[ 7] - 13*O[ 8] + 82*O[ 9] + 61*O[10] - 46*O[11] - 88*O[12] -  4*O[13] + 85*O[14] + 54*O[15] + Add1st)>>Shift1st);
    Tmp[15][j] = xClipS16((67*O[ 0] - 54*O[ 1] - 78*O[ 2] + 38*O[ 3] + 85*O[ 4] - 22*O[ 5] - 90*O[ 6] +  4*O[ 7] + 90*O[ 8] + 13*O[ 9] - 88*O[10] - 31*O[11] + 82*O[12] + 46*O[13] - 73*O[14] - 61*O[15] + Add1st)>>Shift1st);
    Tmp[17][j] = xClipS16((61*O[ 0] - 73*O[ 1] - 46*O[ 2] + 82*O[ 3] + 31*O[ 4] - 88*O[ 5] - 13*O[ 6] + 90*O[ 7] -  4*O[ 8] - 90*O[ 9] + 22*O[10] + 85*O[11] - 38*O[12] - 78*O[13] + 54*O[14] + 67*O[15] + Add1st)>>Shift1st);
    Tmp[19][j] = xClipS16((54*O[ 0] - 85*O[ 1] -  4*O[ 2] + 88*O[ 3] - 46*O[ 4] - 61*O[ 5] + 82*O[ 6] + 13*O[ 7] - 90*O[ 8] + 38*O[ 9] + 67*O[10] - 78*O[11] - 22*O[12] + 90*O[13] - 31*O[14] - 73*O[15] + Add1st)>>Shift1st);
    Tmp[21][j] = xClipS16((46*O[ 0] - 90*O[ 1] + 38*O[ 2] + 54*O[ 3] - 90*O[ 4] + 31*O[ 5] + 61*O[ 6] - 88*O[ 7] + 22*O[ 8] + 67*O[ 9] - 85*O[10] + 13*O[11] + 73*O[12] - 82*O[13] +  4*O[14] + 78*O[15] + Add1st)>>Shift1st);
    Tmp[23][j] = xClipS16((38*O[ 0] - 88*O[ 1] + 73*O[ 2] -  4*O[ 3] - 67*O[ 4] + 90*O[ 5] - 46*O[ 6] - 31*O[ 7] + 85*O[ 8] - 78*O[ 9] + 13*O[10] + 61*O[11] - 90*O[12] + 54*O[13] + 22*O[14] - 82*O[15] + Add1st)>>Shift1st);
    Tmp[25][j] = xClipS16((31*O[ 0] - 78*O[ 1] + 90*O[ 2] - 61*O[ 3] +  4*O[ 4] + 54*O[ 5] - 88*O[ 6] + 82*O[ 7] - 38*O[ 8] - 22*O[ 9] + 73*O[10] - 90*O[11] + 67*O[12] - 13*O[13] - 46*O[14] + 85*O[15] + Add1st)>>Shift1st);
    Tmp[27][j] = xClipS16((22*O[ 0] - 61*O[ 1] + 85*O[ 2] - 90*O[ 3] + 73*O[ 4] - 38*O[ 5] -  4*O[ 6] + 46*O[ 7] - 78*O[ 8] + 90*O[ 9] - 82*O[10] + 54*O[11] - 13*O[12] - 31*O[13] + 67*O[14] - 88*O[15] + Add1st)>>Shift1st);
    Tmp[29][j] = xClipS16((13*O[ 0] - 38*O[ 1] + 61*O[ 2] - 78*O[ 3] + 88*O[ 4] - 90*O[ 5] + 85*O[ 6] - 73*O[ 7] + 54*O[ 8] - 31*O[ 9] +  4*O[10] + 22*O[11] - 46*O[12] + 67*O[13] - 82*O[14] + 90*O[15] + Add1st)>>Shift1st);
    Tmp[31][j] = xClipS16(( 4*O[ 0] - 13*O[ 1] + 22*O[ 2] - 31*O[ 3] + 38*O[ 4] - 46*O[ 5] + 54*O[ 6] - 61*O[ 7] + 67*O[ 8] - 73*O[ 9] + 78*O[10] - 82*O[11] + 85*O[12] - 88*O[13] + 90*O[14] - 90*O[15] + Add1st)>>Shift1st);
  }

  for (int32 j=0; j<32; j++)
  {    
    for (int32 k=0;k<16;k++)
    {
      E[k] = Tmp[j][k] + Tmp[j][31-k];
      O[k] = Tmp[j][k] - Tmp[j][31-k];
    } 

    for (int32 k=0;k<8;k++)
    {
      EE[k] = E[k] + E[15-k];
      EO[k] = E[k] - E[15-k];
    }

    for (int32 k=0;k<4;k++)
    {
      EEE[k] = EE[k] + EE[7-k];
      EEO[k] = EE[k] - EE[7-k];
    }

    EEEE[0] = EEE[0] + EEE[3];    
    EEEO[0] = EEE[0] - EEE[3];
    EEEE[1] = EEE[1] + EEE[2];
    EEEO[1] = EEE[1] - EEE[2];

    Dst[ 0   ] = xClipS16((64*EEEE[0] + 64*EEEE[1] + 1024)>>11);
    Dst[16*32] = xClipS16((64*EEEE[0] - 64*EEEE[1] + 1024)>>11);
    Dst[ 8*32] = xClipS16((83*EEEO[0] + 36*EEEO[1] + 1024)>>11);
    Dst[24*32] = xClipS16((36*EEEO[0] - 83*EEEO[1] + 1024)>>11);

    Dst[ 4*32] = xClipS16((89*EEO[0] + 75*EEO[1] + 50*EEO[2] + 18*EEO[3] +1024)>>11);
    Dst[12*32] = xClipS16((75*EEO[0] - 18*EEO[1] - 89*EEO[2] - 50*EEO[3] +1024)>>11);
    Dst[20*32] = xClipS16((50*EEO[0] - 89*EEO[1] + 18*EEO[2] + 75*EEO[3] +1024)>>11);
    Dst[28*32] = xClipS16((18*EEO[0] - 50*EEO[1] + 75*EEO[2] - 89*EEO[3] +1024)>>11);    

    Dst[ 2*32] = xClipS16((90*EO[0] + 87*EO[1] + 80*EO[2] + 70*EO[3] + 57*EO[4] + 43*EO[5] + 25*EO[6] +  9*EO[7] + 1024)>>11);
    Dst[ 6*32] = xClipS16((87*EO[0] + 57*EO[1] +  9*EO[2] - 43*EO[3] - 80*EO[4] - 90*EO[5] - 70*EO[6] - 25*EO[7] + 1024)>>11);
    Dst[10*32] = xClipS16((80*EO[0] +  9*EO[1] - 70*EO[2] - 87*EO[3] - 25*EO[4] + 57*EO[5] + 90*EO[6] + 43*EO[7] + 1024)>>11);
    Dst[14*32] = xClipS16((70*EO[0] - 43*EO[1] - 87*EO[2] +  9*EO[3] + 90*EO[4] + 25*EO[5] - 80*EO[6] - 57*EO[7] + 1024)>>11);
    Dst[18*32] = xClipS16((57*EO[0] - 80*EO[1] - 25*EO[2] + 90*EO[3] -  9*EO[4] - 87*EO[5] + 43*EO[6] + 70*EO[7] + 1024)>>11);
    Dst[22*32] = xClipS16((43*EO[0] - 90*EO[1] + 57*EO[2] + 25*EO[3] - 87*EO[4] + 70*EO[5] +  9*EO[6] - 80*EO[7] + 1024)>>11);
    Dst[26*32] = xClipS16((25*EO[0] - 70*EO[1] + 90*EO[2] - 80*EO[3] + 43*EO[4] +  9*EO[5] - 57*EO[6] + 87*EO[7] + 1024)>>11);
    Dst[30*32] = xClipS16(( 9*EO[0] - 25*EO[1] + 43*EO[2] - 57*EO[3] + 70*EO[4] - 80*EO[5] + 87*EO[6] - 90*EO[7] + 1024)>>11);

    Dst[ 1*32] = xClipS16((90*O[ 0] + 90*O[ 1] + 88*O[ 2] + 85*O[ 3] + 82*O[ 4] + 78*O[ 5] + 73*O[ 6] + 67*O[ 7] + 61*O[ 8] + 54*O[ 9] + 46*O[10] + 38*O[11] + 31*O[12] + 22*O[13] + 13*O[14] +  4*O[15] + 1024)>>11);
    Dst[ 3*32] = xClipS16((90*O[ 0] + 82*O[ 1] + 67*O[ 2] + 46*O[ 3] + 22*O[ 4] -  4*O[ 5] - 31*O[ 6] - 54*O[ 7] - 73*O[ 8] - 85*O[ 9] - 90*O[10] - 88*O[11] - 78*O[12] - 61*O[13] - 38*O[14] - 13*O[15] + 1024)>>11);
    Dst[ 5*32] = xClipS16((88*O[ 0] + 67*O[ 1] + 31*O[ 2] - 13*O[ 3] - 54*O[ 4] - 82*O[ 5] - 90*O[ 6] - 78*O[ 7] - 46*O[ 8] -  4*O[ 9] + 38*O[10] + 73*O[11] + 90*O[12] + 85*O[13] + 61*O[14] + 22*O[15] + 1024)>>11);
    Dst[ 7*32] = xClipS16((85*O[ 0] + 46*O[ 1] - 13*O[ 2] - 67*O[ 3] - 90*O[ 4] - 73*O[ 5] - 22*O[ 6] + 38*O[ 7] + 82*O[ 8] + 88*O[ 9] + 54*O[10] -  4*O[11] - 61*O[12] - 90*O[13] - 78*O[14] - 31*O[15] + 1024)>>11);
    Dst[ 9*32] = xClipS16((82*O[ 0] + 22*O[ 1] - 54*O[ 2] - 90*O[ 3] - 61*O[ 4] + 13*O[ 5] + 78*O[ 6] + 85*O[ 7] + 31*O[ 8] - 46*O[ 9] - 90*O[10] - 67*O[11] +  4*O[12] + 73*O[13] + 88*O[14] + 38*O[15] + 1024)>>11);
    Dst[11*32] = xClipS16((78*O[ 0] -  4*O[ 1] - 82*O[ 2] - 73*O[ 3] + 13*O[ 4] + 85*O[ 5] + 67*O[ 6] - 22*O[ 7] - 88*O[ 8] - 61*O[ 9] + 31*O[10] + 90*O[11] + 54*O[12] - 38*O[13] - 90*O[14] - 46*O[15] + 1024)>>11);
    Dst[13*32] = xClipS16((73*O[ 0] - 31*O[ 1] - 90*O[ 2] - 22*O[ 3] + 78*O[ 4] + 67*O[ 5] - 38*O[ 6] - 90*O[ 7] - 13*O[ 8] + 82*O[ 9] + 61*O[10] - 46*O[11] - 88*O[12] -  4*O[13] + 85*O[14] + 54*O[15] + 1024)>>11);
    Dst[15*32] = xClipS16((67*O[ 0] - 54*O[ 1] - 78*O[ 2] + 38*O[ 3] + 85*O[ 4] - 22*O[ 5] - 90*O[ 6] +  4*O[ 7] + 90*O[ 8] + 13*O[ 9] - 88*O[10] - 31*O[11] + 82*O[12] + 46*O[13] - 73*O[14] - 61*O[15] + 1024)>>11);
    Dst[17*32] = xClipS16((61*O[ 0] - 73*O[ 1] - 46*O[ 2] + 82*O[ 3] + 31*O[ 4] - 88*O[ 5] - 13*O[ 6] + 90*O[ 7] -  4*O[ 8] - 90*O[ 9] + 22*O[10] + 85*O[11] - 38*O[12] - 78*O[13] + 54*O[14] + 67*O[15] + 1024)>>11);
    Dst[19*32] = xClipS16((54*O[ 0] - 85*O[ 1] -  4*O[ 2] + 88*O[ 3] - 46*O[ 4] - 61*O[ 5] + 82*O[ 6] + 13*O[ 7] - 90*O[ 8] + 38*O[ 9] + 67*O[10] - 78*O[11] - 22*O[12] + 90*O[13] - 31*O[14] - 73*O[15] + 1024)>>11);
    Dst[21*32] = xClipS16((46*O[ 0] - 90*O[ 1] + 38*O[ 2] + 54*O[ 3] - 90*O[ 4] + 31*O[ 5] + 61*O[ 6] - 88*O[ 7] + 22*O[ 8] + 67*O[ 9] - 85*O[10] + 13*O[11] + 73*O[12] - 82*O[13] +  4*O[14] + 78*O[15] + 1024)>>11);
    Dst[23*32] = xClipS16((38*O[ 0] - 88*O[ 1] + 73*O[ 2] -  4*O[ 3] - 67*O[ 4] + 90*O[ 5] - 46*O[ 6] - 31*O[ 7] + 85*O[ 8] - 78*O[ 9] + 13*O[10] + 61*O[11] - 90*O[12] + 54*O[13] + 22*O[14] - 82*O[15] + 1024)>>11);
    Dst[25*32] = xClipS16((31*O[ 0] - 78*O[ 1] + 90*O[ 2] - 61*O[ 3] +  4*O[ 4] + 54*O[ 5] - 88*O[ 6] + 82*O[ 7] - 38*O[ 8] - 22*O[ 9] + 73*O[10] - 90*O[11] + 67*O[12] - 13*O[13] - 46*O[14] + 85*O[15] + 1024)>>11);
    Dst[27*32] = xClipS16((22*O[ 0] - 61*O[ 1] + 85*O[ 2] - 90*O[ 3] + 73*O[ 4] - 38*O[ 5] -  4*O[ 6] + 46*O[ 7] - 78*O[ 8] + 90*O[ 9] - 82*O[10] + 54*O[11] - 13*O[12] - 31*O[13] + 67*O[14] - 88*O[15] + 1024)>>11);
    Dst[29*32] = xClipS16((13*O[ 0] - 38*O[ 1] + 61*O[ 2] - 78*O[ 3] + 88*O[ 4] - 90*O[ 5] + 85*O[ 6] - 73*O[ 7] + 54*O[ 8] - 31*O[ 9] +  4*O[10] + 22*O[11] - 46*O[12] + 67*O[13] - 82*O[14] + 90*O[15] + 1024)>>11);
    Dst[31*32] = xClipS16(( 4*O[ 0] - 13*O[ 1] + 22*O[ 2] - 31*O[ 3] + 38*O[ 4] - 46*O[ 5] + 54*O[ 6] - 61*O[ 7] + 67*O[ 8] - 73*O[ 9] + 78*O[10] - 82*O[11] + 85*O[12] - 88*O[13] + 90*O[14] - 90*O[15] + 1024)>>11);
    Dst++;
  }
}
void xTransform::TransformDCT_32x32_STD_M(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[16],O[16];
  int32 EE[8],EO[8];
  int32 EEE[4],EEO[4];
  int32 EEEE[2],EEEO[2];
  int32 Tmp[32][32];
  int32 Shift1st = BitDepth - 4;
  int32 Add1st   = 1<<(Shift1st-1);

  for (int32 j=0; j<32; j++)
  {    
    for (int32 k=0;k<16;k++)
    {
      E[k] = Src[k] + Src[31-k];
      O[k] = Src[k] - Src[31-k];
    }
    Src += SrcStride;

    for (int32 k=0;k<8;k++)
    {
      EE[k] = E[k] + E[15-k];
      EO[k] = E[k] - E[15-k];
    }

    for (int32 k=0;k<4;k++)
    {
      EEE[k] = EE[k] + EE[7-k];
      EEO[k] = EE[k] - EE[7-k];
    }

    EEEE[0] = EEE[0] + EEE[3];    
    EEEO[0] = EEE[0] - EEE[3];
    EEEE[1] = EEE[1] + EEE[2];
    EEEO[1] = EEE[1] - EEE[2]; 

    Tmp[ 0][j] = xClipS16((m_TrM32x32[ 0][0]*EEEE[0] + m_TrM32x32[ 0][1]*EEEE[1] + Add1st)>>Shift1st);
    Tmp[16][j] = xClipS16((m_TrM32x32[16][0]*EEEE[0] + m_TrM32x32[16][1]*EEEE[1] + Add1st)>>Shift1st);
    Tmp[ 8][j] = xClipS16((m_TrM32x32[ 8][0]*EEEO[0] + m_TrM32x32[ 8][1]*EEEO[1] + Add1st)>>Shift1st); 
    Tmp[24][j] = xClipS16((m_TrM32x32[24][0]*EEEO[0] + m_TrM32x32[24][1]*EEEO[1] + Add1st)>>Shift1st);
    for (int32 k=4;k<32;k+=8)
    {
      Tmp[ k][j] = xClipS16((m_TrM32x32[k][0]*EEO[0] + m_TrM32x32[k][1]*EEO[1] + m_TrM32x32[k][2]*EEO[2] + m_TrM32x32[k][3]*EEO[3] + Add1st)>>Shift1st);
    }       
    for (int32 k=2;k<32;k+=4)
    {
      Tmp[ k][j] = xClipS16((m_TrM32x32[k][0]*EO[0] + m_TrM32x32[k][1]*EO[1] + m_TrM32x32[k][2]*EO[2] + m_TrM32x32[k][3]*EO[3] + 
                               m_TrM32x32[k][4]*EO[4] + m_TrM32x32[k][5]*EO[5] + m_TrM32x32[k][6]*EO[6] + m_TrM32x32[k][7]*EO[7] + Add1st)>>Shift1st);
    }       
    for (int32 k=1;k<32;k+=2)
    {
      Tmp[ k][j] = xClipS16((m_TrM32x32[k][ 0]*O[ 0] + m_TrM32x32[k][ 1]*O[ 1] + m_TrM32x32[k][ 2]*O[ 2] + m_TrM32x32[k][ 3]*O[ 3] + 
                               m_TrM32x32[k][ 4]*O[ 4] + m_TrM32x32[k][ 5]*O[ 5] + m_TrM32x32[k][ 6]*O[ 6] + m_TrM32x32[k][ 7]*O[ 7] +
                               m_TrM32x32[k][ 8]*O[ 8] + m_TrM32x32[k][ 9]*O[ 9] + m_TrM32x32[k][10]*O[10] + m_TrM32x32[k][11]*O[11] + 
                               m_TrM32x32[k][12]*O[12] + m_TrM32x32[k][13]*O[13] + m_TrM32x32[k][14]*O[14] + m_TrM32x32[k][15]*O[15] + Add1st)>>Shift1st);
    }
  }

  for (int32 j=0; j<32; j++)
  {    
    for (int32 k=0;k<16;k++)
    {
      E[k] = Tmp[j][k] + Tmp[j][31-k];
      O[k] = Tmp[j][k] - Tmp[j][31-k];
    } 

    for (int32 k=0;k<8;k++)
    {
      EE[k] = E[k] + E[15-k];
      EO[k] = E[k] - E[15-k];
    }

    for (int32 k=0;k<4;k++)
    {
      EEE[k] = EE[k] + EE[7-k];
      EEO[k] = EE[k] - EE[7-k];
    }

    EEEE[0] = EEE[0] + EEE[3];    
    EEEO[0] = EEE[0] - EEE[3];
    EEEE[1] = EEE[1] + EEE[2];
    EEEO[1] = EEE[1] - EEE[2];

    Dst[ 0*32] = xClipS16((m_TrM32x32[ 0][0]*EEEE[0] + m_TrM32x32[ 0][1]*EEEE[1] + 1024)>>11);
    Dst[16*32] = xClipS16((m_TrM32x32[16][0]*EEEE[0] + m_TrM32x32[16][1]*EEEE[1] + 1024)>>11);
    Dst[ 8*32] = xClipS16((m_TrM32x32[ 8][0]*EEEO[0] + m_TrM32x32[ 8][1]*EEEO[1] + 1024)>>11); 
    Dst[24*32] = xClipS16((m_TrM32x32[24][0]*EEEO[0] + m_TrM32x32[24][1]*EEEO[1] + 1024)>>11);
    for (int32 k=4;k<32;k+=8)
    {
      Dst[ k*32] = xClipS16((m_TrM32x32[k][0]*EEO[0] + m_TrM32x32[k][1]*EEO[1] + m_TrM32x32[k][2]*EEO[2] + m_TrM32x32[k][3]*EEO[3] + 1024)>>11);
    }       
    for (int32 k=2;k<32;k+=4)
    {
      Dst[ k*32] = xClipS16((m_TrM32x32[k][0]*EO[0] + m_TrM32x32[k][1]*EO[1] + m_TrM32x32[k][2]*EO[2] + m_TrM32x32[k][3]*EO[3] + 
                               m_TrM32x32[k][4]*EO[4] + m_TrM32x32[k][5]*EO[5] + m_TrM32x32[k][6]*EO[6] + m_TrM32x32[k][7]*EO[7] + 1024)>>11);
    }       
    for (int32 k=1;k<32;k+=2)
    {
      Dst[ k*32] = xClipS16((m_TrM32x32[k][ 0]*O[ 0] + m_TrM32x32[k][ 1]*O[ 1] + m_TrM32x32[k][ 2]*O[ 2] + m_TrM32x32[k][ 3]*O[ 3] + 
                               m_TrM32x32[k][ 4]*O[ 4] + m_TrM32x32[k][ 5]*O[ 5] + m_TrM32x32[k][ 6]*O[ 6] + m_TrM32x32[k][ 7]*O[ 7] +
                               m_TrM32x32[k][ 8]*O[ 8] + m_TrM32x32[k][ 9]*O[ 9] + m_TrM32x32[k][10]*O[10] + m_TrM32x32[k][11]*O[11] + 
                               m_TrM32x32[k][12]*O[12] + m_TrM32x32[k][13]*O[13] + m_TrM32x32[k][14]*O[14] + m_TrM32x32[k][15]*O[15] + 1024)>>11);
    }
    Dst++;
  }
}
void xTransform::TransformDCT_32x32_SSE(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth) //horizontal transform->transpose->vertical transform->transpose
{
  static const __m128i xT32_DCT[128] = {
  _mm_setr_epi16(64, 64, 64, 64, 64, 64, 64, 64),  _mm_setr_epi16(64, 64, 64, 64, 64, 64, 64, 64),  _mm_setr_epi16(64, 64, 64, 64, 64, 64, 64, 64),  _mm_setr_epi16(64, 64, 64, 64, 64, 64, 64, 64),
  _mm_setr_epi16(90, 90, 88, 85, 82, 78, 73, 67),  _mm_setr_epi16(61, 54, 46, 38, 31, 22, 13, 4),  _mm_setr_epi16(-4, -13, -22, -31, -38, -46, -54, -61),  _mm_setr_epi16(-67, -73, -78, -82, -85, -88, -90, -90),
  _mm_setr_epi16(90, 87, 80, 70, 57, 43, 25, 9),  _mm_setr_epi16(-9, -25, -43, -57, -70, -80, -87, -90),  _mm_setr_epi16(-90, -87, -80, -70, -57, -43, -25, -9),  _mm_setr_epi16(9, 25, 43, 57, 70, 80, 87, 90),
  _mm_setr_epi16(90, 82, 67, 46, 22, -4, -31, -54),  _mm_setr_epi16(-73, -85, -90, -88, -78, -61, -38, -13),  _mm_setr_epi16(13, 38, 61, 78, 88, 90, 85, 73),  _mm_setr_epi16(54, 31, 4, -22, -46, -67, -82, -90),
  _mm_setr_epi16(89, 75, 50, 18, -18, -50, -75, -89),  _mm_setr_epi16(-89, -75, -50, -18, 18, 50, 75, 89),  _mm_setr_epi16(89, 75, 50, 18, -18, -50, -75, -89),  _mm_setr_epi16(-89, -75, -50, -18, 18, 50, 75, 89),
  _mm_setr_epi16(88, 67, 31, -13, -54, -82, -90, -78),  _mm_setr_epi16(-46, -4, 38, 73, 90, 85, 61, 22),  _mm_setr_epi16(-22, -61, -85, -90, -73, -38, 4, 46),  _mm_setr_epi16(78, 90, 82, 54, 13, -31, -67, -88),
  _mm_setr_epi16(87, 57, 9, -43, -80, -90, -70, -25),  _mm_setr_epi16(25, 70, 90, 80, 43, -9, -57, -87),  _mm_setr_epi16(-87, -57, -9, 43, 80, 90, 70, 25),  _mm_setr_epi16(-25, -70, -90, -80, -43, 9, 57, 87),
  _mm_setr_epi16(85, 46, -13, -67, -90, -73, -22, 38),  _mm_setr_epi16(82, 88, 54, -4, -61, -90, -78, -31),  _mm_setr_epi16(31, 78, 90, 61, 4, -54, -88, -82),  _mm_setr_epi16(-38, 22, 73, 90, 67, 13, -46, -85),
  _mm_setr_epi16(83, 36, -36, -83, -83, -36, 36, 83),  _mm_setr_epi16(83, 36, -36, -83, -83, -36, 36, 83),  _mm_setr_epi16(83, 36, -36, -83, -83, -36, 36, 83),  _mm_setr_epi16(83, 36, -36, -83, -83, -36, 36, 83),
  _mm_setr_epi16(82, 22, -54, -90, -61, 13, 78, 85),  _mm_setr_epi16(31, -46, -90, -67, 4, 73, 88, 38),  _mm_setr_epi16(-38, -88, -73, -4, 67, 90, 46, -31),  _mm_setr_epi16(-85, -78, -13, 61, 90, 54, -22, -82),
  _mm_setr_epi16(80, 9, -70, -87, -25, 57, 90, 43),  _mm_setr_epi16(-43, -90, -57, 25, 87, 70, -9, -80),  _mm_setr_epi16(-80, -9, 70, 87, 25, -57, -90, -43),  _mm_setr_epi16(43, 90, 57, -25, -87, -70, 9, 80),
  _mm_setr_epi16(78, -4, -82, -73, 13, 85, 67, -22),  _mm_setr_epi16(-88, -61, 31, 90, 54, -38, -90, -46),  _mm_setr_epi16(46, 90, 38, -54, -90, -31, 61, 88),  _mm_setr_epi16(22, -67, -85, -13, 73, 82, 4, -78),
  _mm_setr_epi16(75, -18, -89, -50, 50, 89, 18, -75),  _mm_setr_epi16(-75, 18, 89, 50, -50, -89, -18, 75),  _mm_setr_epi16(75, -18, -89, -50, 50, 89, 18, -75),  _mm_setr_epi16(-75, 18, 89, 50, -50, -89, -18, 75),
  _mm_setr_epi16(73, -31, -90, -22, 78, 67, -38, -90),  _mm_setr_epi16(-13, 82, 61, -46, -88, -4, 85, 54),  _mm_setr_epi16(-54, -85, 4, 88, 46, -61, -82, 13),  _mm_setr_epi16(90, 38, -67, -78, 22, 90, 31, -73),
  _mm_setr_epi16(70, -43, -87, 9, 90, 25, -80, -57),  _mm_setr_epi16(57, 80, -25, -90, -9, 87, 43, -70),  _mm_setr_epi16(-70, 43, 87, -9, -90, -25, 80, 57),  _mm_setr_epi16(-57, -80, 25, 90, 9, -87, -43, 70),
  _mm_setr_epi16(67, -54, -78, 38, 85, -22, -90, 4),  _mm_setr_epi16(90, 13, -88, -31, 82, 46, -73, -61),  _mm_setr_epi16(61, 73, -46, -82, 31, 88, -13, -90),  _mm_setr_epi16(-4, 90, 22, -85, -38, 78, 54, -67),
  _mm_setr_epi16(64, -64, -64, 64, 64, -64, -64, 64),  _mm_setr_epi16(64, -64, -64, 64, 64, -64, -64, 64),  _mm_setr_epi16(64, -64, -64, 64, 64, -64, -64, 64),  _mm_setr_epi16(64, -64, -64, 64, 64, -64, -64, 64),
  _mm_setr_epi16(61, -73, -46, 82, 31, -88, -13, 90),  _mm_setr_epi16(-4, -90, 22, 85, -38, -78, 54, 67),  _mm_setr_epi16(-67, -54, 78, 38, -85, -22, 90, 4),  _mm_setr_epi16(-90, 13, 88, -31, -82, 46, 73, -61),
  _mm_setr_epi16(57, -80, -25, 90, -9, -87, 43, 70),  _mm_setr_epi16(-70, -43, 87, 9, -90, 25, 80, -57),  _mm_setr_epi16(-57, 80, 25, -90, 9, 87, -43, -70),  _mm_setr_epi16(70, 43, -87, -9, 90, -25, -80, 57),
  _mm_setr_epi16(54, -85, -4, 88, -46, -61, 82, 13),  _mm_setr_epi16(-90, 38, 67, -78, -22, 90, -31, -73),  _mm_setr_epi16(73, 31, -90, 22, 78, -67, -38, 90),  _mm_setr_epi16(-13, -82, 61, 46, -88, 4, 85, -54),
  _mm_setr_epi16(50, -89, 18, 75, -75, -18, 89, -50),  _mm_setr_epi16(-50, 89, -18, -75, 75, 18, -89, 50),  _mm_setr_epi16(50, -89, 18, 75, -75, -18, 89, -50),  _mm_setr_epi16(-50, 89, -18, -75, 75, 18, -89, 50),
  _mm_setr_epi16(46, -90, 38, 54, -90, 31, 61, -88),  _mm_setr_epi16(22, 67, -85, 13, 73, -82, 4, 78),  _mm_setr_epi16(-78, -4, 82, -73, -13, 85, -67, -22),  _mm_setr_epi16(88, -61, -31, 90, -54, -38, 90, -46),
  _mm_setr_epi16(43, -90, 57, 25, -87, 70, 9, -80),  _mm_setr_epi16(80, -9, -70, 87, -25, -57, 90, -43),  _mm_setr_epi16(-43, 90, -57, -25, 87, -70, -9, 80),  _mm_setr_epi16(-80, 9, 70, -87, 25, 57, -90, 43),
  _mm_setr_epi16(38, -88, 73, -4, -67, 90, -46, -31),  _mm_setr_epi16(85, -78, 13, 61, -90, 54, 22, -82),  _mm_setr_epi16(82, -22, -54, 90, -61, -13, 78, -85),  _mm_setr_epi16(31, 46, -90, 67, 4, -73, 88, -38),
  _mm_setr_epi16(36, -83, 83, -36, -36, 83, -83, 36),  _mm_setr_epi16(36, -83, 83, -36, -36, 83, -83, 36),  _mm_setr_epi16(36, -83, 83, -36, -36, 83, -83, 36),  _mm_setr_epi16(36, -83, 83, -36, -36, 83, -83, 36),
  _mm_setr_epi16(31, -78, 90, -61, 4, 54, -88, 82),  _mm_setr_epi16(-38, -22, 73, -90, 67, -13, -46, 85),  _mm_setr_epi16(-85, 46, 13, -67, 90, -73, 22, 38),  _mm_setr_epi16(-82, 88, -54, -4, 61, -90, 78, -31),
  _mm_setr_epi16(25, -70, 90, -80, 43, 9, -57, 87),  _mm_setr_epi16(-87, 57, -9, -43, 80, -90, 70, -25),  _mm_setr_epi16(-25, 70, -90, 80, -43, -9, 57, -87),  _mm_setr_epi16(87, -57, 9, 43, -80, 90, -70, 25),
  _mm_setr_epi16(22, -61, 85, -90, 73, -38, -4, 46),  _mm_setr_epi16(-78, 90, -82, 54, -13, -31, 67, -88),  _mm_setr_epi16(88, -67, 31, 13, -54, 82, -90, 78),  _mm_setr_epi16(-46, 4, 38, -73, 90, -85, 61, -22),
  _mm_setr_epi16(18, -50, 75, -89, 89, -75, 50, -18),  _mm_setr_epi16(-18, 50, -75, 89, -89, 75, -50, 18),  _mm_setr_epi16(18, -50, 75, -89, 89, -75, 50, -18),  _mm_setr_epi16(-18, 50, -75, 89, -89, 75, -50, 18),
  _mm_setr_epi16(13, -38, 61, -78, 88, -90, 85, -73),  _mm_setr_epi16(54, -31, 4, 22, -46, 67, -82, 90),  _mm_setr_epi16(-90, 82, -67, 46, -22, -4, 31, -54),  _mm_setr_epi16(73, -85, 90, -88, 78, -61, 38, -13),
  _mm_setr_epi16(9, -25, 43, -57, 70, -80, 87, -90),  _mm_setr_epi16(90, -87, 80, -70, 57, -43, 25, -9),  _mm_setr_epi16(-9, 25, -43, 57, -70, 80, -87, 90),  _mm_setr_epi16(-90, 87, -80, 70, -57, 43, -25, 9),
  _mm_setr_epi16(4, -13, 22, -31, 38, -46, 54, -61),  _mm_setr_epi16(67, -73, 78, -82, 85, -88, 90, -90),  _mm_setr_epi16(90, -90, 88, -85, 82, -78, 73, -67),  _mm_setr_epi16(61, -54, 46, -38, 31, -22, 13, -4),

  };

  int32 Shift1st = BitDepth - 4;
  //int32 Add1st   = 1<<(Shift1st-1);
  const int32 shift_2nd = 11;
  const __m128i Add2nd = _mm_set1_epi32(1024);
  const __m128i Add1st = _mm_set1_epi32(1<<(Shift1st-1));

  __m128i tr[32];
  __m128i Tmp_A[128];
  __m128i Tmp_B[128];

   //load and horizontal transform
  for(int32 j=0; j<32; j++)
  {
    __m128i LineA = _mm_loadu_si128((__m128i*)(Src  ));
    __m128i LineB = _mm_loadu_si128((__m128i*)(Src+8));
  __m128i LineC = _mm_loadu_si128((__m128i*)(Src+16));
  __m128i LineD = _mm_loadu_si128((__m128i*)(Src+24));
    Src += SrcStride;    

    for(int32 i=0, k=0; i<128; i+=4, k++)
    {
      __m128i Sum1= _mm_add_epi32(_mm_madd_epi16(LineA, xT32_DCT[i  ]), _mm_madd_epi16(LineB, xT32_DCT[i+1]));
    __m128i Sum2= _mm_add_epi32(_mm_madd_epi16(LineC, xT32_DCT[i+2]), _mm_madd_epi16(LineD, xT32_DCT[i+3]));
    tr[k] = _mm_add_epi32(Sum1,Sum2);
    }

    for(int32 i=0; i<16; i++)tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add1st), Shift1st);

    Tmp_B[4*j  ] = _mm_packs_epi32(tr[0], tr[1]);
    Tmp_B[4*j+1] = _mm_packs_epi32(tr[2], tr[3]);
  Tmp_B[4*j+2] = _mm_packs_epi32(tr[4], tr[5]);
  Tmp_B[4*j+3] = _mm_packs_epi32(tr[6], tr[7]);
  }

  //transpose
  for(int32 i=0; i<64; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[64+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[64+i]);
  }

  for(int32 i=0; i<64; i++)
  {
    Tmp_B[2*i  ] = _mm_unpacklo_epi16(Tmp_A[0+i], Tmp_A[64+i]);
    Tmp_B[2*i+1] = _mm_unpackhi_epi16(Tmp_A[0+i], Tmp_A[64+i]);
  }

  for(int32 i=0; i<64; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[64+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[64+i]);
  }

  for(int32 i=0; i<64; i++)
  {
    Tmp_B[2*i  ] = _mm_unpacklo_epi16(Tmp_A[0+i], Tmp_A[64+i]);
    Tmp_B[2*i+1] = _mm_unpackhi_epi16(Tmp_A[0+i], Tmp_A[64+i]);
  }

  for(int32 i=0; i<64; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[64+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[64+i]);
  }

  //vertical transform
  for(int32 j=0; j<32; j++)
  {
    __m128i LineA = Tmp_A[4*j  ];
    __m128i LineB = Tmp_A[4*j+1];
  __m128i LineC = Tmp_A[4*j+2];
  __m128i LineD = Tmp_A[4*j+3];

    for(int32 i=0, k=0; i<128; i+=4, k++)
    {
    __m128i Sum1= _mm_add_epi32(_mm_madd_epi16(LineA, xT32_DCT[i  ]), _mm_madd_epi16(LineB, xT32_DCT[i+1]));
    __m128i Sum2= _mm_add_epi32(_mm_madd_epi16(LineC, xT32_DCT[i+2]), _mm_madd_epi16(LineD, xT32_DCT[i+3]));
    tr[k] = _mm_add_epi32(Sum1,Sum2);    
  }

    for(int32 i=0; i<16; i++)tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add2nd), shift_2nd);

    Tmp_B[4*j  ] = _mm_packs_epi32(tr[0], tr[1]);
    Tmp_B[4*j+1] = _mm_packs_epi32(tr[2], tr[3]);
  Tmp_B[4*j+2] = _mm_packs_epi32(tr[4], tr[5]);
  Tmp_B[4*j+3] = _mm_packs_epi32(tr[6], tr[7]);
  }

  //transpose
  for(int32 i=0; i<64; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[64+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[64+i]);
  }

  for(int32 i=0; i<64; i++)
  {
    Tmp_B[2*i  ] = _mm_unpacklo_epi16(Tmp_A[0+i], Tmp_A[64+i]);
    Tmp_B[2*i+1] = _mm_unpackhi_epi16(Tmp_A[0+i], Tmp_A[64+i]);
  }

  for(int32 i=0; i<64; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[64+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[64+i]);
  }

  for(int32 i=0; i<64; i++)
  {
    Tmp_B[2*i  ] = _mm_unpacklo_epi16(Tmp_A[0+i], Tmp_A[64+i]);
    Tmp_B[2*i+1] = _mm_unpackhi_epi16(Tmp_A[0+i], Tmp_A[64+i]);
  }

  for(int32 i=0; i<64; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[64+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[64+i]);
  }

  //storage
  for(int32 j=0; j<32; j++)
  {
    _mm_storeu_si128((__m128i*)(Dst   ), Tmp_A[4*j  ]);
    _mm_storeu_si128((__m128i*)(Dst+8 ), Tmp_A[4*j+1]);
  _mm_storeu_si128((__m128i*)(Dst+16), Tmp_A[4*j+2]);
    _mm_storeu_si128((__m128i*)(Dst+24), Tmp_A[4*j+3]);
    Dst += 32;
  }
}
void xTransform::TransformDCT_32x32_SSEv2(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth) //horizontal transform 1 symmetry, vertical transform 2 symmetries
{
   static const __m128i xT32e_DCT[32] = {
  _mm_setr_epi16(64, 64, 64, 64, 64, 64, 64, 64),  _mm_setr_epi16(64, 64, 64, 64, 64, 64, 64, 64),  
  _mm_setr_epi16(90, 87, 80, 70, 57, 43, 25, 9),  _mm_setr_epi16(-9, -25, -43, -57, -70, -80, -87, -90),  
  _mm_setr_epi16(89, 75, 50, 18, -18, -50, -75, -89),  _mm_setr_epi16(-89, -75, -50, -18, 18, 50, 75, 89),
  _mm_setr_epi16(87, 57, 9, -43, -80, -90, -70, -25),  _mm_setr_epi16(25, 70, 90, 80, 43, -9, -57, -87),  
  _mm_setr_epi16(83, 36, -36, -83, -83, -36, 36, 83),  _mm_setr_epi16(83, 36, -36, -83, -83, -36, 36, 83),
  _mm_setr_epi16(80, 9, -70, -87, -25, 57, 90, 43),  _mm_setr_epi16(-43, -90, -57, 25, 87, 70, -9, -80),
  _mm_setr_epi16(75, -18, -89, -50, 50, 89, 18, -75),  _mm_setr_epi16(-75, 18, 89, 50, -50, -89, -18, 75),
  _mm_setr_epi16(70, -43, -87, 9, 90, 25, -80, -57),  _mm_setr_epi16(57, 80, -25, -90, -9, 87, 43, -70),  
  _mm_setr_epi16(64, -64, -64, 64, 64, -64, -64, 64),  _mm_setr_epi16(64, -64, -64, 64, 64, -64, -64, 64),
  _mm_setr_epi16(57, -80, -25, 90, -9, -87, 43, 70),  _mm_setr_epi16(-70, -43, 87, 9, -90, 25, 80, -57),  
  _mm_setr_epi16(50, -89, 18, 75, -75, -18, 89, -50),  _mm_setr_epi16(-50, 89, -18, -75, 75, 18, -89, 50),  
  _mm_setr_epi16(43, -90, 57, 25, -87, 70, 9, -80),  _mm_setr_epi16(80, -9, -70, 87, -25, -57, 90, -43),
  _mm_setr_epi16(36, -83, 83, -36, -36, 83, -83, 36),  _mm_setr_epi16(36, -83, 83, -36, -36, 83, -83, 36),  
  _mm_setr_epi16(25, -70, 90, -80, 43, 9, -57, 87),  _mm_setr_epi16(-87, 57, -9, -43, 80, -90, 70, -25),  
  _mm_setr_epi16(18, -50, 75, -89, 89, -75, 50, -18),  _mm_setr_epi16(-18, 50, -75, 89, -89, 75, -50, 18),
  _mm_setr_epi16(9, -25, 43, -57, 70, -80, 87, -90),  _mm_setr_epi16(90, -87, 80, -70, 57, -43, 25, -9),  
  };
    static const __m128i xT32o_DCT[32] = {
  _mm_setr_epi16(90, 90, 88, 85, 82, 78, 73, 67),    _mm_setr_epi16(61, 54, 46, 38, 31, 22, 13, 4),  
  _mm_setr_epi16(90, 82, 67, 46, 22, -4, -31, -54),  _mm_setr_epi16(-73, -85, -90, -88, -78, -61, -38, -13),  
  _mm_setr_epi16(88, 67, 31, -13, -54, -82, -90, -78),_mm_setr_epi16(-46, -4, 38, 73, 90, 85, 61, 22),  
  _mm_setr_epi16(85, 46, -13, -67, -90, -73, -22, 38),_mm_setr_epi16(82, 88, 54, -4, -61, -90, -78, -31),  
  _mm_setr_epi16(82, 22, -54, -90, -61, 13, 78, 85),  _mm_setr_epi16(31, -46, -90, -67, 4, 73, 88, 38),  
  _mm_setr_epi16(78, -4, -82, -73, 13, 85, 67, -22),  _mm_setr_epi16(-88, -61, 31, 90, 54, -38, -90, -46),  
  _mm_setr_epi16(73, -31, -90, -22, 78, 67, -38, -90),_mm_setr_epi16(-13, 82, 61, -46, -88, -4, 85, 54),  
  _mm_setr_epi16(67, -54, -78, 38, 85, -22, -90, 4),  _mm_setr_epi16(90, 13, -88, -31, 82, 46, -73, -61),
  _mm_setr_epi16(61, -73, -46, 82, 31, -88, -13, 90),  _mm_setr_epi16(-4, -90, 22, 85, -38, -78, 54, 67),  
  _mm_setr_epi16(54, -85, -4, 88, -46, -61, 82, 13),  _mm_setr_epi16(-90, 38, 67, -78, -22, 90, -31, -73),
  _mm_setr_epi16(46, -90, 38, 54, -90, 31, 61, -88),  _mm_setr_epi16(22, 67, -85, 13, 73, -82, 4, 78),
  _mm_setr_epi16(38, -88, 73, -4, -67, 90, -46, -31),  _mm_setr_epi16(85, -78, 13, 61, -90, 54, 22, -82),
  _mm_setr_epi16(31, -78, 90, -61, 4, 54, -88, 82),  _mm_setr_epi16(-38, -22, 73, -90, 67, -13, -46, 85),
  _mm_setr_epi16(22, -61, 85, -90, 73, -38, -4, 46),  _mm_setr_epi16(-78, 90, -82, 54, -13, -31, 67, -88),
  _mm_setr_epi16(13, -38, 61, -78, 88, -90, 85, -73),  _mm_setr_epi16(54, -31, 4, 22, -46, 67, -82, 90),
  _mm_setr_epi16(4, -13, 22, -31, 38, -46, 54, -61),  _mm_setr_epi16(67, -73, 78, -82, 85, -88, 90, -90),
  };

  const int32 Shift1st = BitDepth - 4;
  const __m128i Add1st = _mm_set1_epi32(1<<(Shift1st-1));
  const __m128i Add2nd = _mm_set1_epi32(1024);
  const __m128i add_3nd = _mm_set1_epi32(16);

  __m128i Tmp[128];

  //load and horizontal transform
  for(int32 j=0; j<32; j++)
  {
    __m128i tr[32];
    __m128i LineA = _mm_loadu_si128((__m128i*)(Src  ));
    __m128i LineB = _mm_loadu_si128((__m128i*)(Src+8));
  __m128i LineC = _mm_loadu_si128((__m128i*)(Src+16));
  __m128i LineD = _mm_loadu_si128((__m128i*)(Src+24));
  Src += SrcStride;    

    //reverse order CD
    LineD = _mm_shufflehi_epi16(LineD, (0<<6)|(1<<4)|(2<<2)|(3<<0));
    LineD = _mm_shufflelo_epi16(LineD, (0<<6)|(1<<4)|(2<<2)|(3<<0));
    LineD = _mm_shuffle_epi32  (LineD, (1<<6)|(0<<4)|(3<<2)|(2<<0));

  LineC = _mm_shufflehi_epi16(LineC, (0<<6)|(1<<4)|(2<<2)|(3<<0));
    LineC = _mm_shufflelo_epi16(LineC, (0<<6)|(1<<4)|(2<<2)|(3<<0));
    LineC = _mm_shuffle_epi32  (LineC, (1<<6)|(0<<4)|(3<<2)|(2<<0));
  //__m128i LineTmp=LineC;
  //LineC=LineD;
  //LineD=LineTmp;
    __m128i LineE1 = _mm_add_epi16(LineA, LineD);
    __m128i LineE2 = _mm_add_epi16(LineB, LineC);
  __m128i LineO1 = _mm_sub_epi16(LineA, LineD);
  __m128i LineO2 = _mm_sub_epi16(LineB, LineC);

    for(int32 i=0, k=0; i<32; i+=2, k+=2)
    {
    tr[k ] = _mm_add_epi32(_mm_madd_epi16(LineE1, xT32e_DCT[i]), _mm_madd_epi16(LineE2, xT32e_DCT[i+1]));
    tr[k+1] = _mm_add_epi32(_mm_madd_epi16(LineO1, xT32o_DCT[i]),_mm_madd_epi16(LineO2, xT32o_DCT[i+1]));
    }

    for(int32 i=0; i<16; i++)tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add1st), Shift1st);

  Tmp[4*j  ] = _mm_packs_epi32(tr[0], tr[1]);
    Tmp[4*j+1] = _mm_packs_epi32(tr[2], tr[3]);
  Tmp[4*j+2] = _mm_packs_epi32(tr[4], tr[5]);
  Tmp[4*j+3] = _mm_packs_epi32(tr[6], tr[7]);
  }

  //vertical transform
  for(int32 i=0; i<32; i+=4)
  {  
    //even even
    if(i==0)//first line - multiplication by 64
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

    __m128i SumE = _mm_setzero_si128();
      __m128i SumF = _mm_setzero_si128();
      __m128i SumG = _mm_setzero_si128();
      __m128i SumH = _mm_setzero_si128();

      for(int32 j=0, k=0; j<8; j++, k+=4)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
    __m128i Line_1C = Tmp[k+2];
    __m128i Line_1D = Tmp[k+3];

        __m128i Line_2A = Tmp[60-k  ];
        __m128i Line_2B = Tmp[60-k+1];
    __m128i Line_2C = Tmp[60-k+2];
        __m128i Line_2D = Tmp[60-k+3];

        __m128i Line_3A = Tmp[64+k  ];
        __m128i Line_3B = Tmp[64+k+1];
    __m128i Line_3C = Tmp[64+k+2];
        __m128i Line_3D = Tmp[64+k+3];

        __m128i Line_4A = Tmp[124-k  ];
        __m128i Line_4B = Tmp[124-k+1];
    __m128i Line_4C = Tmp[124-k+2];
        __m128i Line_4D = Tmp[124-k+3];

        __m128i LineA = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1A), _mm_cvtepi16_epi32(Line_2A)),
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3A), _mm_cvtepi16_epi32(Line_4A)));
        __m128i LineB = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2A,8))),
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4A,8))));
        __m128i LineC = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1B), _mm_cvtepi16_epi32(Line_2B)), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3B), _mm_cvtepi16_epi32(Line_4B)));
        __m128i LineD = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2B,8))), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4B,8))));
    
    __m128i LineE = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1C), _mm_cvtepi16_epi32(Line_2C)),
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3C), _mm_cvtepi16_epi32(Line_4C)));
        __m128i LineF = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1C,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2C,8))),
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3C,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4C,8))));
        __m128i LineG = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1D), _mm_cvtepi16_epi32(Line_2D)), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3D), _mm_cvtepi16_epi32(Line_4D)));
        __m128i LineH = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1D,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2D,8))), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3D,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4D,8))));
    /*
        LineA = _mm_slli_epi32(LineA, 6);
        LineB = _mm_slli_epi32(LineB, 6);
        LineC = _mm_slli_epi32(LineC, 6);
        LineD = _mm_slli_epi32(LineD, 6);
    
    LineE = _mm_slli_epi32(LineE, 6);
        LineF = _mm_slli_epi32(LineF, 6);
        LineG = _mm_slli_epi32(LineG, 6);
        LineH = _mm_slli_epi32(LineH, 6);
    */
        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
    
    SumE = _mm_add_epi32(SumE, LineE);
        SumF = _mm_add_epi32(SumF, LineF);
        SumG = _mm_add_epi32(SumG, LineG);
        SumH = _mm_add_epi32(SumH, LineH);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, add_3nd), 5);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, add_3nd), 5);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, add_3nd), 5);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, add_3nd), 5);

    SumE = _mm_srai_epi32(_mm_add_epi32(SumE, add_3nd), 5);
      SumF = _mm_srai_epi32(_mm_add_epi32(SumF, add_3nd), 5);
      SumG = _mm_srai_epi32(_mm_add_epi32(SumG, add_3nd), 5);
      SumH = _mm_srai_epi32(_mm_add_epi32(SumH, add_3nd), 5);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));

    _mm_storeu_si128((__m128i*)(Dst+16), _mm_packs_epi32(SumE, SumF));
      _mm_storeu_si128((__m128i*)(Dst+24), _mm_packs_epi32(SumG, SumH));
      Dst += 32;
    }
    else
    {//+|-|-|+||+|-|-|+
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

    __m128i SumE = _mm_setzero_si128();
      __m128i SumF = _mm_setzero_si128();
      __m128i SumG = _mm_setzero_si128();
      __m128i SumH = _mm_setzero_si128();

      for(int32 j=0, k=0; j<8; j++, k+=4)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
    __m128i Line_1C = Tmp[k+2];
    __m128i Line_1D = Tmp[k+3];

        __m128i Line_2A = Tmp[60-k  ];
        __m128i Line_2B = Tmp[60-k+1];
    __m128i Line_2C = Tmp[60-k+2];
        __m128i Line_2D = Tmp[60-k+3];

        __m128i Line_3A = Tmp[64+k  ];
        __m128i Line_3B = Tmp[64+k+1];
    __m128i Line_3C = Tmp[64+k+2];
        __m128i Line_3D = Tmp[64+k+3];

        __m128i Line_4A = Tmp[124-k  ];
        __m128i Line_4B = Tmp[124-k+1];
    __m128i Line_4C = Tmp[124-k+2];
        __m128i Line_4D = Tmp[124-k+3];

        __m128i LineA = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1A), _mm_cvtepi16_epi32(Line_2A)),
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3A), _mm_cvtepi16_epi32(Line_4A)));
        __m128i LineB = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2A,8))),
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4A,8))));
        __m128i LineC = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1B), _mm_cvtepi16_epi32(Line_2B)), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3B), _mm_cvtepi16_epi32(Line_4B)));
        __m128i LineD = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2B,8))), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4B,8))));
    
    __m128i LineE = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1C), _mm_cvtepi16_epi32(Line_2C)),
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3C), _mm_cvtepi16_epi32(Line_4C)));
        __m128i LineF = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1C,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2C,8))),
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3C,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4C,8))));
        __m128i LineG = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1D), _mm_cvtepi16_epi32(Line_2D)), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3D), _mm_cvtepi16_epi32(Line_4D)));
        __m128i LineH = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1D,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2D,8))), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3D,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4D,8))));
    
    __m128i TX = _mm_set1_epi32(m_TrM32x32[i][j]);

    LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        LineC = _mm_mullo_epi32(LineC, TX);
        LineD = _mm_mullo_epi32(LineD, TX);

    LineE = _mm_mullo_epi32(LineE, TX);
        LineF = _mm_mullo_epi32(LineF, TX);
        LineG = _mm_mullo_epi32(LineG, TX);
        LineH = _mm_mullo_epi32(LineH, TX);

        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);

    SumE = _mm_add_epi32(SumE, LineE);
        SumF = _mm_add_epi32(SumF, LineF);
        SumG = _mm_add_epi32(SumG, LineG);
        SumH = _mm_add_epi32(SumH, LineH);

      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 11);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 11);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 11);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 11);

    SumE = _mm_srai_epi32(_mm_add_epi32(SumE, Add2nd), 11);
      SumF = _mm_srai_epi32(_mm_add_epi32(SumF, Add2nd), 11);
      SumG = _mm_srai_epi32(_mm_add_epi32(SumG, Add2nd), 11);
      SumH = _mm_srai_epi32(_mm_add_epi32(SumH, Add2nd), 11);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));

    _mm_storeu_si128((__m128i*)(Dst+16), _mm_packs_epi32(SumE, SumF));
      _mm_storeu_si128((__m128i*)(Dst+24), _mm_packs_epi32(SumG, SumH));
      Dst += 32;
    }
    //odd //  +||-
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

    __m128i SumE = _mm_setzero_si128();
      __m128i SumF = _mm_setzero_si128();
      __m128i SumG = _mm_setzero_si128();
      __m128i SumH = _mm_setzero_si128();

      for(int32 j=0, k=0; j<16; j++, k+=4)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
    __m128i Line_1C = Tmp[k+2];
        __m128i Line_1D = Tmp[k+3];

        __m128i Line_2A = Tmp[124-k  ];
        __m128i Line_2B = Tmp[124-k+1];
    __m128i Line_2C = Tmp[124-k+2];
        __m128i Line_2D = Tmp[124-k+3];

        __m128i Line1A = _mm_cvtepi16_epi32(Line_1A);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1A,8));
        __m128i Line1C = _mm_cvtepi16_epi32(Line_1B);
        __m128i Line1D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1B,8));

    __m128i Line1E = _mm_cvtepi16_epi32(Line_1C);
        __m128i Line1F = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1C,8));
        __m128i Line1G = _mm_cvtepi16_epi32(Line_1D);
        __m128i Line1H = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1D,8));

        __m128i Line2A = _mm_cvtepi16_epi32(Line_2A);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2A,8));
        __m128i Line2C = _mm_cvtepi16_epi32(Line_2B);
        __m128i Line2D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2B,8));

    __m128i Line2E = _mm_cvtepi16_epi32(Line_2C);
        __m128i Line2F = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2C,8));
        __m128i Line2G = _mm_cvtepi16_epi32(Line_2D);
        __m128i Line2H = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2D,8));

        __m128i LineA = _mm_sub_epi32(Line1A, Line2A);
        __m128i LineB = _mm_sub_epi32(Line1B, Line2B);
        __m128i LineC = _mm_sub_epi32(Line1C, Line2C);
        __m128i LineD = _mm_sub_epi32(Line1D, Line2D);
    
    __m128i LineE = _mm_sub_epi32(Line1E, Line2E);
        __m128i LineF = _mm_sub_epi32(Line1F, Line2F);
        __m128i LineG = _mm_sub_epi32(Line1G, Line2G);
        __m128i LineH = _mm_sub_epi32(Line1H, Line2H);

        __m128i TX = _mm_set1_epi32(m_TrM32x32[i+1][j]);
        LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        LineC = _mm_mullo_epi32(LineC, TX);
        LineD = _mm_mullo_epi32(LineD, TX);
    LineE = _mm_mullo_epi32(LineE, TX);
        LineF = _mm_mullo_epi32(LineF, TX);
        LineG = _mm_mullo_epi32(LineG, TX);
        LineH = _mm_mullo_epi32(LineH, TX);

        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
    SumE = _mm_add_epi32(SumE, LineE);
        SumF = _mm_add_epi32(SumF, LineF);
        SumG = _mm_add_epi32(SumG, LineG);
        SumH = _mm_add_epi32(SumH, LineH);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 11);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 11);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 11);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 11);

    SumE = _mm_srai_epi32(_mm_add_epi32(SumE, Add2nd), 11);
      SumF = _mm_srai_epi32(_mm_add_epi32(SumF, Add2nd), 11);
      SumG = _mm_srai_epi32(_mm_add_epi32(SumG, Add2nd), 11);
      SumH = _mm_srai_epi32(_mm_add_epi32(SumH, Add2nd), 11);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));
    _mm_storeu_si128((__m128i*)(Dst+16), _mm_packs_epi32(SumE, SumF));
      _mm_storeu_si128((__m128i*)(Dst+24), _mm_packs_epi32(SumG, SumH));
      Dst += 32;
    }
    //even odd +--+
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();
    __m128i SumE = _mm_setzero_si128();
      __m128i SumF = _mm_setzero_si128();
      __m128i SumG = _mm_setzero_si128();
      __m128i SumH = _mm_setzero_si128();

      for(int32 j=0, k=0; j<8; j++, k+=4)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
    __m128i Line_1C = Tmp[k+2];
    __m128i Line_1D = Tmp[k+3];

        __m128i Line_2A = Tmp[60-k  ];
        __m128i Line_2B = Tmp[60-k+1];
    __m128i Line_2C = Tmp[60-k+2];
        __m128i Line_2D = Tmp[60-k+3];

        __m128i Line_3A = Tmp[64+k  ];
        __m128i Line_3B = Tmp[64+k+1];
    __m128i Line_3C = Tmp[64+k+2];
        __m128i Line_3D = Tmp[64+k+3];

        __m128i Line_4A = Tmp[124-k  ];
        __m128i Line_4B = Tmp[124-k+1];
    __m128i Line_4C = Tmp[124-k+2];
        __m128i Line_4D = Tmp[124-k+3];

        __m128i Line1A = _mm_cvtepi16_epi32(Line_1A);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1A,8));
        __m128i Line1C = _mm_cvtepi16_epi32(Line_1B);
        __m128i Line1D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1B,8));

    __m128i Line1E = _mm_cvtepi16_epi32(Line_1C);
        __m128i Line1F = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1C,8));
        __m128i Line1G = _mm_cvtepi16_epi32(Line_1D);
        __m128i Line1H = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1D,8));

        __m128i Line2A = _mm_cvtepi16_epi32(Line_2A);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2A,8));
        __m128i Line2C = _mm_cvtepi16_epi32(Line_2B);
        __m128i Line2D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2B,8));

    __m128i Line2E = _mm_cvtepi16_epi32(Line_2C);
        __m128i Line2F = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2C,8));
        __m128i Line2G = _mm_cvtepi16_epi32(Line_2D);
        __m128i Line2H = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2D,8));

        __m128i Line3A = _mm_cvtepi16_epi32(Line_3A);
        __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_3A,8));
        __m128i Line3C = _mm_cvtepi16_epi32(Line_3B);
        __m128i Line3D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_3B,8));

    __m128i Line3E = _mm_cvtepi16_epi32(Line_3C);
        __m128i Line3F = _mm_cvtepi16_epi32(_mm_srli_si128( Line_3C,8));
        __m128i Line3G = _mm_cvtepi16_epi32(Line_3D);
        __m128i Line3H = _mm_cvtepi16_epi32(_mm_srli_si128( Line_3D,8));

        __m128i Line4A = _mm_cvtepi16_epi32(Line_4A);
        __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_4A,8));
        __m128i Line4C = _mm_cvtepi16_epi32(Line_4B);
        __m128i Line4D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_4B,8));

    __m128i Line4E = _mm_cvtepi16_epi32(Line_4C);
        __m128i Line4F = _mm_cvtepi16_epi32(_mm_srli_si128( Line_4C,8));
        __m128i Line4G = _mm_cvtepi16_epi32(Line_4D);
        __m128i Line4H = _mm_cvtepi16_epi32(_mm_srli_si128( Line_4D,8));

        __m128i LineA = _mm_sub_epi32(_mm_add_epi32(Line1A, Line4A), _mm_add_epi32(Line3A, Line2A));
        __m128i LineB = _mm_sub_epi32(_mm_add_epi32(Line1B, Line4B), _mm_add_epi32(Line3B, Line2B));
        __m128i LineC = _mm_sub_epi32(_mm_add_epi32(Line1C, Line4C), _mm_add_epi32(Line3C, Line2C));
        __m128i LineD = _mm_sub_epi32(_mm_add_epi32(Line1D, Line4D), _mm_add_epi32(Line3D, Line2D));

    __m128i LineE = _mm_sub_epi32(_mm_add_epi32(Line1E, Line4E), _mm_add_epi32(Line3E, Line2E));
        __m128i LineF = _mm_sub_epi32(_mm_add_epi32(Line1F, Line4F), _mm_add_epi32(Line3F, Line2F));
        __m128i LineG = _mm_sub_epi32(_mm_add_epi32(Line1G, Line4G), _mm_add_epi32(Line3G, Line2G));
        __m128i LineH = _mm_sub_epi32(_mm_add_epi32(Line1H, Line4H), _mm_add_epi32(Line3H, Line2H));

        __m128i TX = _mm_set1_epi32(m_TrM32x32[i+2][j]);
        LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        LineC = _mm_mullo_epi32(LineC, TX);
        LineD = _mm_mullo_epi32(LineD, TX);
    
    LineE = _mm_mullo_epi32(LineE, TX);
        LineF = _mm_mullo_epi32(LineF, TX);
        LineG = _mm_mullo_epi32(LineG, TX);
        LineH = _mm_mullo_epi32(LineH, TX);

        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
    SumE = _mm_add_epi32(SumE, LineE);
        SumF = _mm_add_epi32(SumF, LineF);
        SumG = _mm_add_epi32(SumG, LineG);
        SumH = _mm_add_epi32(SumH, LineH);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 11);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 11);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 11);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 11);

    SumE = _mm_srai_epi32(_mm_add_epi32(SumE, Add2nd), 11);
      SumF = _mm_srai_epi32(_mm_add_epi32(SumF, Add2nd), 11);
      SumG = _mm_srai_epi32(_mm_add_epi32(SumG, Add2nd), 11);
      SumH = _mm_srai_epi32(_mm_add_epi32(SumH, Add2nd), 11);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));

    _mm_storeu_si128((__m128i*)(Dst+16), _mm_packs_epi32(SumE, SumF));
      _mm_storeu_si128((__m128i*)(Dst+24), _mm_packs_epi32(SumG, SumH));
      Dst += 32;
    }
    //odd
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

    __m128i SumE = _mm_setzero_si128();
      __m128i SumF = _mm_setzero_si128();
      __m128i SumG = _mm_setzero_si128();
      __m128i SumH = _mm_setzero_si128();

      for(int32 j=0, k=0; j<16; j++, k+=4)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
    __m128i Line_1C = Tmp[k+2];
        __m128i Line_1D = Tmp[k+3];

        __m128i Line_2A = Tmp[124-k  ];
        __m128i Line_2B = Tmp[124-k+1];
    __m128i Line_2C = Tmp[124-k+2];
        __m128i Line_2D = Tmp[124-k+3];

        __m128i Line1A = _mm_cvtepi16_epi32(Line_1A);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1A,8));
        __m128i Line1C = _mm_cvtepi16_epi32(Line_1B);
        __m128i Line1D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1B,8));

    __m128i Line1E = _mm_cvtepi16_epi32(Line_1C);
        __m128i Line1F = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1C,8));
        __m128i Line1G = _mm_cvtepi16_epi32(Line_1D);
        __m128i Line1H = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1D,8));

        __m128i Line2A = _mm_cvtepi16_epi32(Line_2A);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2A,8));
        __m128i Line2C = _mm_cvtepi16_epi32(Line_2B);
        __m128i Line2D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2B,8));

    __m128i Line2E = _mm_cvtepi16_epi32(Line_2C);
        __m128i Line2F = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2C,8));
        __m128i Line2G = _mm_cvtepi16_epi32(Line_2D);
        __m128i Line2H = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2D,8));

        __m128i LineA = _mm_sub_epi32(Line1A, Line2A);
        __m128i LineB = _mm_sub_epi32(Line1B, Line2B);
        __m128i LineC = _mm_sub_epi32(Line1C, Line2C);
        __m128i LineD = _mm_sub_epi32(Line1D, Line2D);

    __m128i LineE = _mm_sub_epi32(Line1E, Line2E);
        __m128i LineF = _mm_sub_epi32(Line1F, Line2F);
        __m128i LineG = _mm_sub_epi32(Line1G, Line2G);
        __m128i LineH = _mm_sub_epi32(Line1H, Line2H);

        __m128i TX = _mm_set1_epi32(m_TrM32x32[i+3][j]);
        LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        LineC = _mm_mullo_epi32(LineC, TX);
        LineD = _mm_mullo_epi32(LineD, TX);

    LineE = _mm_mullo_epi32(LineE, TX);
        LineF = _mm_mullo_epi32(LineF, TX);
        LineG = _mm_mullo_epi32(LineG, TX);
        LineH = _mm_mullo_epi32(LineH, TX);

        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
     
    SumE = _mm_add_epi32(SumE, LineE);
        SumF = _mm_add_epi32(SumF, LineF);
        SumG = _mm_add_epi32(SumG, LineG);
        SumH = _mm_add_epi32(SumH, LineH);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 11);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 11);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 11);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 11);

    SumE = _mm_srai_epi32(_mm_add_epi32(SumE, Add2nd), 11);
      SumF = _mm_srai_epi32(_mm_add_epi32(SumF, Add2nd), 11);
      SumG = _mm_srai_epi32(_mm_add_epi32(SumG, Add2nd), 11);
      SumH = _mm_srai_epi32(_mm_add_epi32(SumH, Add2nd), 11);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));

    _mm_storeu_si128((__m128i*)(Dst+16), _mm_packs_epi32(SumE, SumF));
      _mm_storeu_si128((__m128i*)(Dst+24), _mm_packs_epi32(SumG, SumH));
      Dst += 32;
    }
  }    
}
void xTransform::TransformDCT_32x32_SSEv3(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth)  //horizontal transform 2 symmetry (xT32e_DCTreduction, vertical transform 3 symmetries. slower than DCT_32x32_SSEv2
{
   static const __m128i xT32e_DCT[16] = {
  _mm_setr_epi16(64, 64, 64, 64, 64, 64, 64, 64),  
  _mm_setr_epi16(90, 87, 80, 70, 57, 43, 25, 9),  
  _mm_setr_epi16(89, 75, 50, 18, -18, -50, -75, -89),
  _mm_setr_epi16(87, 57, 9, -43, -80, -90, -70, -25),  
  _mm_setr_epi16(83, 36, -36, -83, -83, -36, 36, 83),  
  _mm_setr_epi16(80, 9, -70, -87, -25, 57, 90, 43),  
  _mm_setr_epi16(75, -18, -89, -50, 50, 89, 18, -75),  
  _mm_setr_epi16(70, -43, -87, 9, 90, 25, -80, -57),  
  _mm_setr_epi16(64, -64, -64, 64, 64, -64, -64, 64),  
  _mm_setr_epi16(57, -80, -25, 90, -9, -87, 43, 70),    
  _mm_setr_epi16(50, -89, 18, 75, -75, -18, 89, -50),  
  _mm_setr_epi16(43, -90, 57, 25, -87, 70, 9, -80),  
  _mm_setr_epi16(36, -83, 83, -36, -36, 83, -83, 36),    
  _mm_setr_epi16(25, -70, 90, -80, 43, 9, -57, 87),  
  _mm_setr_epi16(18, -50, 75, -89, 89, -75, 50, -18),  
  _mm_setr_epi16(9, -25, 43, -57, 70, -80, 87, -90),  
  };
    static const __m128i xT32o_DCT[32] = {
  _mm_setr_epi16(90, 90, 88, 85, 82, 78, 73, 67),    _mm_setr_epi16(61, 54, 46, 38, 31, 22, 13, 4),  
  _mm_setr_epi16(90, 82, 67, 46, 22, -4, -31, -54),  _mm_setr_epi16(-73, -85, -90, -88, -78, -61, -38, -13),  
  _mm_setr_epi16(88, 67, 31, -13, -54, -82, -90, -78),_mm_setr_epi16(-46, -4, 38, 73, 90, 85, 61, 22),  
  _mm_setr_epi16(85, 46, -13, -67, -90, -73, -22, 38),_mm_setr_epi16(82, 88, 54, -4, -61, -90, -78, -31),  
  _mm_setr_epi16(82, 22, -54, -90, -61, 13, 78, 85),  _mm_setr_epi16(31, -46, -90, -67, 4, 73, 88, 38),  
  _mm_setr_epi16(78, -4, -82, -73, 13, 85, 67, -22),  _mm_setr_epi16(-88, -61, 31, 90, 54, -38, -90, -46),  
  _mm_setr_epi16(73, -31, -90, -22, 78, 67, -38, -90),_mm_setr_epi16(-13, 82, 61, -46, -88, -4, 85, 54),  
  _mm_setr_epi16(67, -54, -78, 38, 85, -22, -90, 4),  _mm_setr_epi16(90, 13, -88, -31, 82, 46, -73, -61),
  _mm_setr_epi16(61, -73, -46, 82, 31, -88, -13, 90),  _mm_setr_epi16(-4, -90, 22, 85, -38, -78, 54, 67),  
  _mm_setr_epi16(54, -85, -4, 88, -46, -61, 82, 13),  _mm_setr_epi16(-90, 38, 67, -78, -22, 90, -31, -73),
  _mm_setr_epi16(46, -90, 38, 54, -90, 31, 61, -88),  _mm_setr_epi16(22, 67, -85, 13, 73, -82, 4, 78),
  _mm_setr_epi16(38, -88, 73, -4, -67, 90, -46, -31),  _mm_setr_epi16(85, -78, 13, 61, -90, 54, 22, -82),
  _mm_setr_epi16(31, -78, 90, -61, 4, 54, -88, 82),  _mm_setr_epi16(-38, -22, 73, -90, 67, -13, -46, 85),
  _mm_setr_epi16(22, -61, 85, -90, 73, -38, -4, 46),  _mm_setr_epi16(-78, 90, -82, 54, -13, -31, 67, -88),
  _mm_setr_epi16(13, -38, 61, -78, 88, -90, 85, -73),  _mm_setr_epi16(54, -31, 4, 22, -46, 67, -82, 90),
  _mm_setr_epi16(4, -13, 22, -31, 38, -46, 54, -61),  _mm_setr_epi16(67, -73, 78, -82, 85, -88, 90, -90),
  };

  const int32 Shift1st = BitDepth - 4;
  const __m128i Add1st = _mm_set1_epi32(1<<(Shift1st-1));
  const __m128i Add2nd = _mm_set1_epi32(1024);
  const __m128i add_3nd = _mm_set1_epi32(16);

  __m128i Tmp[128];

  //load and horizontal transform
  for(int32 j=0; j<32; j++)
  {
    __m128i tr[32];
    __m128i LineA = _mm_loadu_si128((__m128i*)(Src  ));
    __m128i LineB = _mm_loadu_si128((__m128i*)(Src+8));
  __m128i LineC = _mm_loadu_si128((__m128i*)(Src+16));
  __m128i LineD = _mm_loadu_si128((__m128i*)(Src+24));
  Src += SrcStride;    

    //reverse order CD
    __m128i LineDrev = _mm_shufflehi_epi16(LineD, (0<<6)|(1<<4)|(2<<2)|(3<<0));
    LineDrev = _mm_shufflelo_epi16(LineDrev, (0<<6)|(1<<4)|(2<<2)|(3<<0));
    LineDrev = _mm_shuffle_epi32  (LineDrev, (1<<6)|(0<<4)|(3<<2)|(2<<0));

  __m128i LineCrev = _mm_shufflehi_epi16(LineC, (0<<6)|(1<<4)|(2<<2)|(3<<0));
    LineCrev = _mm_shufflelo_epi16(LineCrev, (0<<6)|(1<<4)|(2<<2)|(3<<0));
    LineCrev = _mm_shuffle_epi32  (LineCrev, (1<<6)|(0<<4)|(3<<2)|(2<<0));
  
  __m128i LineBrev = _mm_shufflehi_epi16(LineB, (0<<6)|(1<<4)|(2<<2)|(3<<0));
  LineBrev = _mm_shufflelo_epi16(LineBrev, (0<<6)|(1<<4)|(2<<2)|(3<<0));
    LineBrev = _mm_shuffle_epi32  (LineBrev, (1<<6)|(0<<4)|(3<<2)|(2<<0));
  
  __m128i Line0  = _mm_add_epi16(_mm_add_epi16(_mm_add_epi16(LineA,LineB),LineC),LineD);
  __m128i Line1A = _mm_sub_epi16(LineA, LineDrev);
  __m128i Line1B = _mm_sub_epi16(LineB, LineCrev);
  __m128i Line2  = _mm_add_epi16(_mm_sub_epi16(_mm_sub_epi16(LineA,LineBrev),LineC),LineDrev);
  //__m128i Line3A = Line1A
  //__m128i Line3B = Line1B
  __m128i Line4 = _mm_sub_epi16(_mm_add_epi16(_mm_sub_epi16(LineA,LineB),LineC),LineD);
  //__m128i Line5A = Line1A
  //__m128i Line5B = Line1B
  __m128i Line6 = _mm_add_epi16(_mm_sub_epi16(_mm_sub_epi16(LineA,LineBrev),LineC),LineDrev);
  //__m128i Line7A = Line1A
  //__m128i Line7B = Line1B
    for(int32 k=0; k<32; k+=8)
    {
    tr[k]   = _mm_madd_epi16(Line0,xT32e_DCT[k>>1]);
    tr[k+1] = _mm_add_epi32(_mm_madd_epi16(Line1A, xT32o_DCT[k]),_mm_madd_epi16(Line1B, xT32o_DCT[k+1]));
    tr[k+2] = _mm_madd_epi16(Line2,xT32e_DCT[(k>>1)+1]);
    tr[k+3] = _mm_add_epi32(_mm_madd_epi16(Line1A, xT32o_DCT[k+2]),_mm_madd_epi16(Line1B, xT32o_DCT[k+2+1]));
    tr[k+4] = _mm_madd_epi16(Line4,xT32e_DCT[(k>>1)+2]);
    tr[k+5] = _mm_add_epi32(_mm_madd_epi16(Line1A, xT32o_DCT[k+4]),_mm_madd_epi16(Line1B, xT32o_DCT[k+4+1]));
    tr[k+6] = _mm_madd_epi16(Line6,xT32e_DCT[(k>>1)+3]);
    tr[k+7] = _mm_add_epi32(_mm_madd_epi16(Line1A, xT32o_DCT[k+6]),_mm_madd_epi16(Line1B, xT32o_DCT[k+6+1]));
    }

    for(int32 i=0; i<16; i++)tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add1st), Shift1st);

  Tmp[4*j  ] = _mm_packs_epi32(tr[0], tr[1]);
    Tmp[4*j+1] = _mm_packs_epi32(tr[2], tr[3]);
  Tmp[4*j+2] = _mm_packs_epi32(tr[4], tr[5]);
  Tmp[4*j+3] = _mm_packs_epi32(tr[6], tr[7]);
  }

  //vertical transform
  for(int32 i=0; i<32; i+=4)
  {  
    //even even
    if(i==0)
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

    __m128i SumE = _mm_setzero_si128();
      __m128i SumF = _mm_setzero_si128();
      __m128i SumG = _mm_setzero_si128();
      __m128i SumH = _mm_setzero_si128();

      for(int32 j=0, k=0; j<8; j++, k+=4)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
    __m128i Line_1C = Tmp[k+2];
    __m128i Line_1D = Tmp[k+3];

        __m128i Line_2A = Tmp[60-k  ];
        __m128i Line_2B = Tmp[60-k+1];
    __m128i Line_2C = Tmp[60-k+2];
        __m128i Line_2D = Tmp[60-k+3];

        __m128i Line_3A = Tmp[64+k  ];
        __m128i Line_3B = Tmp[64+k+1];
    __m128i Line_3C = Tmp[64+k+2];
        __m128i Line_3D = Tmp[64+k+3];

        __m128i Line_4A = Tmp[124-k  ];
        __m128i Line_4B = Tmp[124-k+1];
    __m128i Line_4C = Tmp[124-k+2];
        __m128i Line_4D = Tmp[124-k+3];

        __m128i LineA = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1A), _mm_cvtepi16_epi32(Line_2A)),
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3A), _mm_cvtepi16_epi32(Line_4A)));
        __m128i LineB = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2A,8))),
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4A,8))));
        __m128i LineC = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1B), _mm_cvtepi16_epi32(Line_2B)), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3B), _mm_cvtepi16_epi32(Line_4B)));
        __m128i LineD = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2B,8))), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4B,8))));
    
    __m128i LineE = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1C), _mm_cvtepi16_epi32(Line_2C)),
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3C), _mm_cvtepi16_epi32(Line_4C)));
        __m128i LineF = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1C,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2C,8))),
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3C,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4C,8))));
        __m128i LineG = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1D), _mm_cvtepi16_epi32(Line_2D)), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3D), _mm_cvtepi16_epi32(Line_4D)));
        __m128i LineH = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1D,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2D,8))), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3D,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4D,8))));
    
        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
    
    SumE = _mm_add_epi32(SumE, LineE);
        SumF = _mm_add_epi32(SumF, LineF);
        SumG = _mm_add_epi32(SumG, LineG);
        SumH = _mm_add_epi32(SumH, LineH);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, add_3nd), 5);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, add_3nd), 5);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, add_3nd), 5);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, add_3nd), 5);

    SumE = _mm_srai_epi32(_mm_add_epi32(SumE, add_3nd), 5);
      SumF = _mm_srai_epi32(_mm_add_epi32(SumF, add_3nd), 5);
      SumG = _mm_srai_epi32(_mm_add_epi32(SumG, add_3nd), 5);
      SumH = _mm_srai_epi32(_mm_add_epi32(SumH, add_3nd), 5);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));

    _mm_storeu_si128((__m128i*)(Dst+16), _mm_packs_epi32(SumE, SumF));
      _mm_storeu_si128((__m128i*)(Dst+24), _mm_packs_epi32(SumG, SumH));
      Dst += 32;
    }
    else
    {//+|-|-|+||+|-|-|+
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

    __m128i SumE = _mm_setzero_si128();
      __m128i SumF = _mm_setzero_si128();
      __m128i SumG = _mm_setzero_si128();
      __m128i SumH = _mm_setzero_si128();

      for(int32 j=0, k=0; j<4; j++, k+=4)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
    __m128i Line_1C = Tmp[k+2];
    __m128i Line_1D = Tmp[k+3];

    __m128i Line_11A = Tmp[28-k  ];
        __m128i Line_11B = Tmp[28-k+1];
    __m128i Line_11C = Tmp[28-k+2];
    __m128i Line_11D = Tmp[28-k+3];

    __m128i Line_12A = Tmp[32+k  ];
        __m128i Line_12B = Tmp[32+k+1];
    __m128i Line_12C = Tmp[32+k+2];
    __m128i Line_12D = Tmp[32+k+3];

        __m128i Line_2A = Tmp[60-k  ];
        __m128i Line_2B = Tmp[60-k+1];
    __m128i Line_2C = Tmp[60-k+2];
        __m128i Line_2D = Tmp[60-k+3];

        __m128i Line_3A = Tmp[64+k  ];
        __m128i Line_3B = Tmp[64+k+1];
    __m128i Line_3C = Tmp[64+k+2];
        __m128i Line_3D = Tmp[64+k+3];

    __m128i Line_31A = Tmp[92-k  ];
        __m128i Line_31B = Tmp[92-k+1];
    __m128i Line_31C = Tmp[92-k+2];
    __m128i Line_31D = Tmp[92-k+3];

    __m128i Line_32A = Tmp[96+k  ];
        __m128i Line_32B = Tmp[96+k+1];
    __m128i Line_32C = Tmp[96+k+2];
    __m128i Line_32D = Tmp[96+k+3];

        __m128i Line_4A = Tmp[124-k  ];
        __m128i Line_4B = Tmp[124-k+1];
    __m128i Line_4C = Tmp[124-k+2];
        __m128i Line_4D = Tmp[124-k+3];
    
    __m128i LineA;
    __m128i LineB;
    __m128i LineC;
    __m128i LineD;
    __m128i LineE;
    __m128i LineF;
    __m128i LineG;
    __m128i LineH;

    __m128i LineATmp=_mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_11A), _mm_cvtepi16_epi32(Line_12A)),
              _mm_add_epi32(_mm_cvtepi16_epi32(Line_31A), _mm_cvtepi16_epi32(Line_32A)));
         LineA = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1A), _mm_cvtepi16_epi32(Line_2A)),
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3A), _mm_cvtepi16_epi32(Line_4A)));
     __m128i LineBTmp=_mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_11A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_12A,8))),
              _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_31A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_32A,8))));
         LineB = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2A,8))),
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3A,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4A,8))));
         __m128i LineCTmp=_mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_11B), _mm_cvtepi16_epi32(Line_12B)),
              _mm_add_epi32(_mm_cvtepi16_epi32(Line_31B), _mm_cvtepi16_epi32(Line_32B)));
     LineC = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1B), _mm_cvtepi16_epi32(Line_2B)), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3B), _mm_cvtepi16_epi32(Line_4B)));
         __m128i LineDTmp=_mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_11B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_12B,8))),
              _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_31B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_32B,8))));
     LineD = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2B,8))), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3B,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4B,8))));
     __m128i LineETmp=_mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_11C), _mm_cvtepi16_epi32(Line_12C)),
              _mm_add_epi32(_mm_cvtepi16_epi32(Line_31C), _mm_cvtepi16_epi32(Line_32C)));
     LineE = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1C), _mm_cvtepi16_epi32(Line_2C)),
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3C), _mm_cvtepi16_epi32(Line_4C)));
     __m128i LineFTmp=_mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_11C,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_12C,8))),
              _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_31C,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_32C,8))));
         LineF = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1C,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2C,8))),
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3C,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4C,8))));
         __m128i LineGTmp=_mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_11D), _mm_cvtepi16_epi32(Line_12D)),
              _mm_add_epi32(_mm_cvtepi16_epi32(Line_31D), _mm_cvtepi16_epi32(Line_32D)));
     LineG = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(Line_1D), _mm_cvtepi16_epi32(Line_2D)), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(Line_3D), _mm_cvtepi16_epi32(Line_4D)));
     __m128i LineHTmp=_mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_11D,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_12D,8))),
              _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_31D,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_32D,8))));
         LineH = _mm_add_epi32(_mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_1D,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_2D,8))), 
                        _mm_add_epi32(_mm_cvtepi16_epi32(_mm_srli_si128(Line_3D,8)), _mm_cvtepi16_epi32(_mm_srli_si128(Line_4D,8))));
    
     if((i&4))
     {
      LineA = _mm_sub_epi32(LineA,LineATmp);
      LineB = _mm_sub_epi32(LineB,LineBTmp);
      LineC = _mm_sub_epi32(LineC,LineCTmp);
      LineD = _mm_sub_epi32(LineD,LineDTmp);
      LineE = _mm_sub_epi32(LineE,LineETmp);
      LineF = _mm_sub_epi32(LineF,LineFTmp);
      LineG = _mm_sub_epi32(LineG,LineGTmp);
      LineH = _mm_sub_epi32(LineH,LineHTmp);
     }
     else
     {
      LineA = _mm_add_epi32(LineA,LineATmp);
      LineB = _mm_add_epi32(LineB,LineBTmp);
      LineC = _mm_add_epi32(LineC,LineCTmp);
      LineD = _mm_add_epi32(LineD,LineDTmp);
      LineE = _mm_add_epi32(LineE,LineETmp);
      LineF = _mm_add_epi32(LineF,LineFTmp);
      LineG = _mm_add_epi32(LineG,LineGTmp);
      LineH = _mm_add_epi32(LineH,LineHTmp);
     }

    __m128i TX = _mm_set1_epi32(m_TrM32x32[i][j]);

    LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        LineC = _mm_mullo_epi32(LineC, TX);
        LineD = _mm_mullo_epi32(LineD, TX);

    LineE = _mm_mullo_epi32(LineE, TX);
        LineF = _mm_mullo_epi32(LineF, TX);
        LineG = _mm_mullo_epi32(LineG, TX);
        LineH = _mm_mullo_epi32(LineH, TX);

        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);

    SumE = _mm_add_epi32(SumE, LineE);
        SumF = _mm_add_epi32(SumF, LineF);
        SumG = _mm_add_epi32(SumG, LineG);
        SumH = _mm_add_epi32(SumH, LineH);

      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 11);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 11);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 11);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 11);

    SumE = _mm_srai_epi32(_mm_add_epi32(SumE, Add2nd), 11);
      SumF = _mm_srai_epi32(_mm_add_epi32(SumF, Add2nd), 11);
      SumG = _mm_srai_epi32(_mm_add_epi32(SumG, Add2nd), 11);
      SumH = _mm_srai_epi32(_mm_add_epi32(SumH, Add2nd), 11);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));

    _mm_storeu_si128((__m128i*)(Dst+16), _mm_packs_epi32(SumE, SumF));
      _mm_storeu_si128((__m128i*)(Dst+24), _mm_packs_epi32(SumG, SumH));
      Dst += 32;
    }
    //odd //  +||-
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

    __m128i SumE = _mm_setzero_si128();
      __m128i SumF = _mm_setzero_si128();
      __m128i SumG = _mm_setzero_si128();
      __m128i SumH = _mm_setzero_si128();

      for(int32 j=0, k=0; j<16; j++, k+=4)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
    __m128i Line_1C = Tmp[k+2];
        __m128i Line_1D = Tmp[k+3];

        __m128i Line_2A = Tmp[124-k  ];
        __m128i Line_2B = Tmp[124-k+1];
    __m128i Line_2C = Tmp[124-k+2];
        __m128i Line_2D = Tmp[124-k+3];

        __m128i Line1A = _mm_cvtepi16_epi32(Line_1A);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1A,8));
        __m128i Line1C = _mm_cvtepi16_epi32(Line_1B);
        __m128i Line1D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1B,8));

    __m128i Line1E = _mm_cvtepi16_epi32(Line_1C);
        __m128i Line1F = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1C,8));
        __m128i Line1G = _mm_cvtepi16_epi32(Line_1D);
        __m128i Line1H = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1D,8));

        __m128i Line2A = _mm_cvtepi16_epi32(Line_2A);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2A,8));
        __m128i Line2C = _mm_cvtepi16_epi32(Line_2B);
        __m128i Line2D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2B,8));

    __m128i Line2E = _mm_cvtepi16_epi32(Line_2C);
        __m128i Line2F = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2C,8));
        __m128i Line2G = _mm_cvtepi16_epi32(Line_2D);
        __m128i Line2H = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2D,8));

        __m128i LineA = _mm_sub_epi32(Line1A, Line2A);
        __m128i LineB = _mm_sub_epi32(Line1B, Line2B);
        __m128i LineC = _mm_sub_epi32(Line1C, Line2C);
        __m128i LineD = _mm_sub_epi32(Line1D, Line2D);
    
    __m128i LineE = _mm_sub_epi32(Line1E, Line2E);
        __m128i LineF = _mm_sub_epi32(Line1F, Line2F);
        __m128i LineG = _mm_sub_epi32(Line1G, Line2G);
        __m128i LineH = _mm_sub_epi32(Line1H, Line2H);

        __m128i TX = _mm_set1_epi32(m_TrM32x32[i+1][j]);
        LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        LineC = _mm_mullo_epi32(LineC, TX);
        LineD = _mm_mullo_epi32(LineD, TX);
    LineE = _mm_mullo_epi32(LineE, TX);
        LineF = _mm_mullo_epi32(LineF, TX);
        LineG = _mm_mullo_epi32(LineG, TX);
        LineH = _mm_mullo_epi32(LineH, TX);

        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
    SumE = _mm_add_epi32(SumE, LineE);
        SumF = _mm_add_epi32(SumF, LineF);
        SumG = _mm_add_epi32(SumG, LineG);
        SumH = _mm_add_epi32(SumH, LineH);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 11);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 11);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 11);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 11);

    SumE = _mm_srai_epi32(_mm_add_epi32(SumE, Add2nd), 11);
      SumF = _mm_srai_epi32(_mm_add_epi32(SumF, Add2nd), 11);
      SumG = _mm_srai_epi32(_mm_add_epi32(SumG, Add2nd), 11);
      SumH = _mm_srai_epi32(_mm_add_epi32(SumH, Add2nd), 11);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));
    //SumH=_mm_set1_epi32(0);
    _mm_storeu_si128((__m128i*)(Dst+16), _mm_packs_epi32(SumE, SumF));
      _mm_storeu_si128((__m128i*)(Dst+24), _mm_packs_epi32(SumG, SumH));
      Dst += 32;
    }
    //even odd +--+
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();
    __m128i SumE = _mm_setzero_si128();
      __m128i SumF = _mm_setzero_si128();
      __m128i SumG = _mm_setzero_si128();
      __m128i SumH = _mm_setzero_si128();

      for(int32 j=0, k=0; j<8; j++, k+=4)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
    __m128i Line_1C = Tmp[k+2];
    __m128i Line_1D = Tmp[k+3];

        __m128i Line_2A = Tmp[60-k  ];
        __m128i Line_2B = Tmp[60-k+1];
    __m128i Line_2C = Tmp[60-k+2];
        __m128i Line_2D = Tmp[60-k+3];

        __m128i Line_3A = Tmp[64+k  ];
        __m128i Line_3B = Tmp[64+k+1];
    __m128i Line_3C = Tmp[64+k+2];
        __m128i Line_3D = Tmp[64+k+3];

        __m128i Line_4A = Tmp[124-k  ];
        __m128i Line_4B = Tmp[124-k+1];
    __m128i Line_4C = Tmp[124-k+2];
        __m128i Line_4D = Tmp[124-k+3];

        __m128i Line1A = _mm_cvtepi16_epi32(Line_1A);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1A,8));
        __m128i Line1C = _mm_cvtepi16_epi32(Line_1B);
        __m128i Line1D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1B,8));

    __m128i Line1E = _mm_cvtepi16_epi32(Line_1C);
        __m128i Line1F = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1C,8));
        __m128i Line1G = _mm_cvtepi16_epi32(Line_1D);
        __m128i Line1H = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1D,8));

        __m128i Line2A = _mm_cvtepi16_epi32(Line_2A);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2A,8));
        __m128i Line2C = _mm_cvtepi16_epi32(Line_2B);
        __m128i Line2D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2B,8));

    __m128i Line2E = _mm_cvtepi16_epi32(Line_2C);
        __m128i Line2F = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2C,8));
        __m128i Line2G = _mm_cvtepi16_epi32(Line_2D);
        __m128i Line2H = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2D,8));

        __m128i Line3A = _mm_cvtepi16_epi32(Line_3A);
        __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_3A,8));
        __m128i Line3C = _mm_cvtepi16_epi32(Line_3B);
        __m128i Line3D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_3B,8));

    __m128i Line3E = _mm_cvtepi16_epi32(Line_3C);
        __m128i Line3F = _mm_cvtepi16_epi32(_mm_srli_si128( Line_3C,8));
        __m128i Line3G = _mm_cvtepi16_epi32(Line_3D);
        __m128i Line3H = _mm_cvtepi16_epi32(_mm_srli_si128( Line_3D,8));

        __m128i Line4A = _mm_cvtepi16_epi32(Line_4A);
        __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_4A,8));
        __m128i Line4C = _mm_cvtepi16_epi32(Line_4B);
        __m128i Line4D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_4B,8));

    __m128i Line4E = _mm_cvtepi16_epi32(Line_4C);
        __m128i Line4F = _mm_cvtepi16_epi32(_mm_srli_si128( Line_4C,8));
        __m128i Line4G = _mm_cvtepi16_epi32(Line_4D);
        __m128i Line4H = _mm_cvtepi16_epi32(_mm_srli_si128( Line_4D,8));

        __m128i LineA = _mm_sub_epi32(_mm_add_epi32(Line1A, Line4A), _mm_add_epi32(Line3A, Line2A));
        __m128i LineB = _mm_sub_epi32(_mm_add_epi32(Line1B, Line4B), _mm_add_epi32(Line3B, Line2B));
        __m128i LineC = _mm_sub_epi32(_mm_add_epi32(Line1C, Line4C), _mm_add_epi32(Line3C, Line2C));
        __m128i LineD = _mm_sub_epi32(_mm_add_epi32(Line1D, Line4D), _mm_add_epi32(Line3D, Line2D));

    __m128i LineE = _mm_sub_epi32(_mm_add_epi32(Line1E, Line4E), _mm_add_epi32(Line3E, Line2E));
        __m128i LineF = _mm_sub_epi32(_mm_add_epi32(Line1F, Line4F), _mm_add_epi32(Line3F, Line2F));
        __m128i LineG = _mm_sub_epi32(_mm_add_epi32(Line1G, Line4G), _mm_add_epi32(Line3G, Line2G));
        __m128i LineH = _mm_sub_epi32(_mm_add_epi32(Line1H, Line4H), _mm_add_epi32(Line3H, Line2H));

        __m128i TX = _mm_set1_epi32(m_TrM32x32[i+2][j]);
        LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        LineC = _mm_mullo_epi32(LineC, TX);
        LineD = _mm_mullo_epi32(LineD, TX);
    
    LineE = _mm_mullo_epi32(LineE, TX);
        LineF = _mm_mullo_epi32(LineF, TX);
        LineG = _mm_mullo_epi32(LineG, TX);
        LineH = _mm_mullo_epi32(LineH, TX);

        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
    SumE = _mm_add_epi32(SumE, LineE);
        SumF = _mm_add_epi32(SumF, LineF);
        SumG = _mm_add_epi32(SumG, LineG);
        SumH = _mm_add_epi32(SumH, LineH);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 11);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 11);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 11);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 11);

    SumE = _mm_srai_epi32(_mm_add_epi32(SumE, Add2nd), 11);
      SumF = _mm_srai_epi32(_mm_add_epi32(SumF, Add2nd), 11);
      SumG = _mm_srai_epi32(_mm_add_epi32(SumG, Add2nd), 11);
      SumH = _mm_srai_epi32(_mm_add_epi32(SumH, Add2nd), 11);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));

    _mm_storeu_si128((__m128i*)(Dst+16), _mm_packs_epi32(SumE, SumF));
      _mm_storeu_si128((__m128i*)(Dst+24), _mm_packs_epi32(SumG, SumH));
      Dst += 32;
    }
    //odd
    {
      __m128i SumA = _mm_setzero_si128();
      __m128i SumB = _mm_setzero_si128();
      __m128i SumC = _mm_setzero_si128();
      __m128i SumD = _mm_setzero_si128();

    __m128i SumE = _mm_setzero_si128();
      __m128i SumF = _mm_setzero_si128();
      __m128i SumG = _mm_setzero_si128();
      __m128i SumH = _mm_setzero_si128();

      for(int32 j=0, k=0; j<16; j++, k+=4)
      {
        __m128i Line_1A = Tmp[k  ];
        __m128i Line_1B = Tmp[k+1];
    __m128i Line_1C = Tmp[k+2];
        __m128i Line_1D = Tmp[k+3];

        __m128i Line_2A = Tmp[124-k  ];
        __m128i Line_2B = Tmp[124-k+1];
    __m128i Line_2C = Tmp[124-k+2];
        __m128i Line_2D = Tmp[124-k+3];

        __m128i Line1A = _mm_cvtepi16_epi32(Line_1A);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1A,8));
        __m128i Line1C = _mm_cvtepi16_epi32(Line_1B);
        __m128i Line1D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1B,8));

    __m128i Line1E = _mm_cvtepi16_epi32(Line_1C);
        __m128i Line1F = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1C,8));
        __m128i Line1G = _mm_cvtepi16_epi32(Line_1D);
        __m128i Line1H = _mm_cvtepi16_epi32(_mm_srli_si128( Line_1D,8));

        __m128i Line2A = _mm_cvtepi16_epi32(Line_2A);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2A,8));
        __m128i Line2C = _mm_cvtepi16_epi32(Line_2B);
        __m128i Line2D = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2B,8));

    __m128i Line2E = _mm_cvtepi16_epi32(Line_2C);
        __m128i Line2F = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2C,8));
        __m128i Line2G = _mm_cvtepi16_epi32(Line_2D);
        __m128i Line2H = _mm_cvtepi16_epi32(_mm_srli_si128( Line_2D,8));

        __m128i LineA = _mm_sub_epi32(Line1A, Line2A);
        __m128i LineB = _mm_sub_epi32(Line1B, Line2B);
        __m128i LineC = _mm_sub_epi32(Line1C, Line2C);
        __m128i LineD = _mm_sub_epi32(Line1D, Line2D);

    __m128i LineE = _mm_sub_epi32(Line1E, Line2E);
        __m128i LineF = _mm_sub_epi32(Line1F, Line2F);
        __m128i LineG = _mm_sub_epi32(Line1G, Line2G);
        __m128i LineH = _mm_sub_epi32(Line1H, Line2H);

        __m128i TX = _mm_set1_epi32(m_TrM32x32[i+3][j]);
        LineA = _mm_mullo_epi32(LineA, TX);
        LineB = _mm_mullo_epi32(LineB, TX);
        LineC = _mm_mullo_epi32(LineC, TX);
        LineD = _mm_mullo_epi32(LineD, TX);

    LineE = _mm_mullo_epi32(LineE, TX);
        LineF = _mm_mullo_epi32(LineF, TX);
        LineG = _mm_mullo_epi32(LineG, TX);
        LineH = _mm_mullo_epi32(LineH, TX);

        SumA = _mm_add_epi32(SumA, LineA);
        SumB = _mm_add_epi32(SumB, LineB);
        SumC = _mm_add_epi32(SumC, LineC);
        SumD = _mm_add_epi32(SumD, LineD);
     
    SumE = _mm_add_epi32(SumE, LineE);
        SumF = _mm_add_epi32(SumF, LineF);
        SumG = _mm_add_epi32(SumG, LineG);
        SumH = _mm_add_epi32(SumH, LineH);
      }
      SumA = _mm_srai_epi32(_mm_add_epi32(SumA, Add2nd), 11);
      SumB = _mm_srai_epi32(_mm_add_epi32(SumB, Add2nd), 11);
      SumC = _mm_srai_epi32(_mm_add_epi32(SumC, Add2nd), 11);
      SumD = _mm_srai_epi32(_mm_add_epi32(SumD, Add2nd), 11);

    SumE = _mm_srai_epi32(_mm_add_epi32(SumE, Add2nd), 11);
      SumF = _mm_srai_epi32(_mm_add_epi32(SumF, Add2nd), 11);
      SumG = _mm_srai_epi32(_mm_add_epi32(SumG, Add2nd), 11);
      SumH = _mm_srai_epi32(_mm_add_epi32(SumH, Add2nd), 11);

      _mm_storeu_si128((__m128i*)(Dst  ), _mm_packs_epi32(SumA, SumB));
      _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(SumC, SumD));

    _mm_storeu_si128((__m128i*)(Dst+16), _mm_packs_epi32(SumE, SumF));
      _mm_storeu_si128((__m128i*)(Dst+24), _mm_packs_epi32(SumG, SumH));
      Dst += 32;
    }
  }    
}
void xTransform::TransformDCT_64x64_STD_M(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[32],O[32];
  int32 EE[16],EO[16];
  int32 EEE[8],EEO[8];
  int32 EEEE[4],EEEO[4];
  int32 EEEEE[2],EEEEO[2];
  int32 Tmp[64][64];
  int32 Shift1st = BitDepth - 3;
  int32 Add1st   = 1<<(Shift1st-1); 

  for (int32 j=0; j<64; j++)
  {    
    for (int32 k=0;k<32;k++)
    {
      E[k] = Src[k] + Src[63-k];
      O[k] = Src[k] - Src[63-k];
    } 
    Src += SrcStride;

    for (int32 k=0;k<16;k++)
    {
      EE[k] = E[k] + E[31-k];
      EO[k] = E[k] - E[31-k];
    }
    for (int32 k=0;k<8;k++)
    {
      EEE[k] = EE[k] + EE[15-k];
      EEO[k] = EE[k] - EE[15-k];
    }
    for (int32 k=0;k<4;k++)
    {
      EEEE[k] = EEE[k] + EEE[7-k];
      EEEO[k] = EEE[k] - EEE[7-k];
    }
    EEEEE[0] = EEEE[0] + EEEE[3];    
    EEEEO[0] = EEEE[0] - EEEE[3];
    EEEEE[1] = EEEE[1] + EEEE[2];
    EEEEO[1] = EEEE[1] - EEEE[2];

    Tmp[ 0][j] = (m_TrM64x64[ 0][0]*EEEEE[0] + m_TrM64x64[ 0][1]*EEEEE[1] + Add1st)>>Shift1st;
    Tmp[32][j] = (m_TrM64x64[32][0]*EEEEE[0] + m_TrM64x64[32][1]*EEEEE[1] + Add1st)>>Shift1st;
    Tmp[16][j] = (m_TrM64x64[16][0]*EEEEO[0] + m_TrM64x64[16][1]*EEEEO[1] + Add1st)>>Shift1st;
    Tmp[48][j] = (m_TrM64x64[48][0]*EEEEO[0] + m_TrM64x64[48][1]*EEEEO[1] + Add1st)>>Shift1st;

    for (int32 k=8;k<64;k+=16)
    {
      Tmp[k][j] = (m_TrM64x64[k][0]*EEEO[0] + m_TrM64x64[k][1]*EEEO[1] + m_TrM64x64[k][2]*EEEO[2] + m_TrM64x64[k][3]*EEEO[3] + Add1st)>>Shift1st;
    }    
    for (int32 k=4;k<64;k+=8)
    {
      Tmp[k][j] = (m_TrM64x64[k][0]*EEO[0] + m_TrM64x64[k][1]*EEO[1] + m_TrM64x64[k][2]*EEO[2] + m_TrM64x64[k][3]*EEO[3] + m_TrM64x64[k][4]*EEO[4] + m_TrM64x64[k][5]*EEO[5] + m_TrM64x64[k][6]*EEO[6] + m_TrM64x64[k][7]*EEO[7] + Add1st)>>Shift1st;
    }   
    for (int32 k=2;k<64;k+=4)
    {
      Tmp[k][j] = (m_TrM64x64[k][ 0]*EO[ 0] + m_TrM64x64[k][ 1]*EO[ 1] + m_TrM64x64[k][ 2]*EO[ 2] + m_TrM64x64[k][ 3]*EO[ 3] + m_TrM64x64[k][ 4]*EO[ 4] + m_TrM64x64[k][ 5]*EO[ 5] + m_TrM64x64[k][ 6]*EO[ 6] + m_TrM64x64[k][ 7]*EO[ 7] + m_TrM64x64[k][ 8]*EO[ 8] + m_TrM64x64[k][ 9]*EO[ 9] + m_TrM64x64[k][10]*EO[10] + m_TrM64x64[k][11]*EO[11] + m_TrM64x64[k][12]*EO[12] + m_TrM64x64[k][13]*EO[13] + m_TrM64x64[k][14]*EO[14] + m_TrM64x64[k][15]*EO[15] + Add1st)>>Shift1st;
    }
    for (int32 k=1;k<64;k+=2)
    {
      Tmp[k][j] = (m_TrM64x64[k][ 0]*O[ 0] + m_TrM64x64[k][ 1]*O[ 1] + m_TrM64x64[k][ 2]*O[ 2] + m_TrM64x64[k][ 3]*O[ 3] + m_TrM64x64[k][ 4]*O[ 4] + m_TrM64x64[k][ 5]*O[ 5] + m_TrM64x64[k][ 6]*O[ 6] + m_TrM64x64[k][ 7]*O[ 7] + m_TrM64x64[k][ 8]*O[ 8] + m_TrM64x64[k][ 9]*O[ 9] + m_TrM64x64[k][10]*O[10] + m_TrM64x64[k][11]*O[11] + m_TrM64x64[k][12]*O[12] + m_TrM64x64[k][13]*O[13] + m_TrM64x64[k][14]*O[14] + m_TrM64x64[k][15]*O[15] + m_TrM64x64[k][16]*O[16] + m_TrM64x64[k][17]*O[17] + m_TrM64x64[k][18]*O[18] + m_TrM64x64[k][19]*O[19] + m_TrM64x64[k][20]*O[20] + m_TrM64x64[k][21]*O[21] + m_TrM64x64[k][22]*O[22] + m_TrM64x64[k][23]*O[23] + m_TrM64x64[k][24]*O[24] + m_TrM64x64[k][25]*O[25] + m_TrM64x64[k][26]*O[26] + m_TrM64x64[k][27]*O[27] + m_TrM64x64[k][28]*O[28] + m_TrM64x64[k][29]*O[29] + m_TrM64x64[k][30]*O[30] + m_TrM64x64[k][31]*O[31] + Add1st)>>Shift1st;
    } 
  }

  for (int32 j=0; j<64; j++)
  {    
    for (int32 k=0;k<32;k++)
    {
      E[k] = Tmp[j][k] + Tmp[j][63-k];
      O[k] = Tmp[j][k] - Tmp[j][63-k];
    } 

    for (int32 k=0;k<16;k++)
    {
      EE[k] = E[k] + E[31-k];
      EO[k] = E[k] - E[31-k];
    }
    for (int32 k=0;k<8;k++)
    {
      EEE[k] = EE[k] + EE[15-k];
      EEO[k] = EE[k] - EE[15-k];
    }
    for (int32 k=0;k<4;k++)
    {
      EEEE[k] = EEE[k] + EEE[7-k];
      EEEO[k] = EEE[k] - EEE[7-k];
    }
    EEEEE[0] = EEEE[0] + EEEE[3];    
    EEEEO[0] = EEEE[0] - EEEE[3];
    EEEEE[1] = EEEE[1] + EEEE[2];
    EEEEO[1] = EEEE[1] - EEEE[2];

    Dst[ 0*64] = (m_TrM64x64[ 0][0]*EEEEE[0] + m_TrM64x64[ 0][1]*EEEEE[1] + 2048)>>12;
    Dst[32*64] = (m_TrM64x64[32][0]*EEEEE[0] + m_TrM64x64[32][1]*EEEEE[1] + 2048)>>12;
    Dst[16*64] = (m_TrM64x64[16][0]*EEEEO[0] + m_TrM64x64[16][1]*EEEEO[1] + 2048)>>12;
    Dst[48*64] = (m_TrM64x64[48][0]*EEEEO[0] + m_TrM64x64[48][1]*EEEEO[1] + 2048)>>12;

    for (int32 k=8;k<64;k+=16)
    {
      Dst[k*64] = (m_TrM64x64[k][0]*EEEO[0] + m_TrM64x64[k][1]*EEEO[1] + m_TrM64x64[k][2]*EEEO[2] + m_TrM64x64[k][3]*EEEO[3] + 2048)>>12;
    }       
    for (int32 k=4;k<64;k+=8)
    {
      Dst[k*64] = (m_TrM64x64[k][0]*EEO[0] + m_TrM64x64[k][1]*EEO[1] + m_TrM64x64[k][2]*EEO[2] + m_TrM64x64[k][3]*EEO[3] + 
        m_TrM64x64[k][4]*EEO[4] + m_TrM64x64[k][5]*EEO[5] + m_TrM64x64[k][6]*EEO[6] + m_TrM64x64[k][7]*EEO[7] + 2048)>>12;
    }       
    for (int32 k=2;k<64;k+=4)
    {
      Dst[k*64] = (m_TrM64x64[k][ 0]*EO[ 0] + m_TrM64x64[k][ 1]*EO[ 1] + m_TrM64x64[k][ 2]*EO[ 2] + m_TrM64x64[k][ 3]*EO[ 3] + 
        m_TrM64x64[k][ 4]*EO[ 4] + m_TrM64x64[k][ 5]*EO[ 5] + m_TrM64x64[k][ 6]*EO[ 6] + m_TrM64x64[k][ 7]*EO[ 7] +
        m_TrM64x64[k][ 8]*EO[ 8] + m_TrM64x64[k][ 9]*EO[ 9] + m_TrM64x64[k][10]*EO[10] + m_TrM64x64[k][11]*EO[11] + 
        m_TrM64x64[k][12]*EO[12] + m_TrM64x64[k][13]*EO[13] + m_TrM64x64[k][14]*EO[14] + m_TrM64x64[k][15]*EO[15] + 2048)>>12;
    }
    for (int32 k=1;k<64;k+=2)
    {
      Dst[k*64] = (m_TrM64x64[k][ 0]*O[ 0] + m_TrM64x64[k][ 1]*O[ 1] + m_TrM64x64[k][ 2]*O[ 2] + m_TrM64x64[k][ 3]*O[ 3] + 
        m_TrM64x64[k][ 4]*O[ 4] + m_TrM64x64[k][ 5]*O[ 5] + m_TrM64x64[k][ 6]*O[ 6] + m_TrM64x64[k][ 7]*O[ 7] +
        m_TrM64x64[k][ 8]*O[ 8] + m_TrM64x64[k][ 9]*O[ 9] + m_TrM64x64[k][10]*O[10] + m_TrM64x64[k][11]*O[11] + 
        m_TrM64x64[k][12]*O[12] + m_TrM64x64[k][13]*O[13] + m_TrM64x64[k][14]*O[14] + m_TrM64x64[k][15]*O[15] + 
        m_TrM64x64[k][16]*O[16] + m_TrM64x64[k][17]*O[17] + m_TrM64x64[k][18]*O[18] + m_TrM64x64[k][19]*O[19] + 
        m_TrM64x64[k][20]*O[20] + m_TrM64x64[k][21]*O[21] + m_TrM64x64[k][22]*O[22] + m_TrM64x64[k][23]*O[23] +
        m_TrM64x64[k][24]*O[24] + m_TrM64x64[k][25]*O[25] + m_TrM64x64[k][26]*O[26] + m_TrM64x64[k][27]*O[27] + 
        m_TrM64x64[k][28]*O[28] + m_TrM64x64[k][29]*O[29] + m_TrM64x64[k][30]*O[30] + m_TrM64x64[k][31]*O[31] +2048)>>12;
    }  
    Dst++;
  }
}

void xTransform::InvTransformDCT_4x4_STD_C(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[2],O[2];
  int32 Tmp[4][4];
  int32 Shift2nd = 20 - BitDepth; 
  int32 Add2nd   = 1<<(Shift2nd-1); 

  for(int32 j=0;j<4;j++) //vertical transform
  {
    O[0] = 83*Src[4] + 36*Src[12]; //read with transposition
    O[1] = 36*Src[4] - 83*Src[12];
    E[0] = (Src[0] + Src[8])<<6;
    E[1] = (Src[0] - Src[8])<<6;
    Src++;

    Tmp[j][0]    = xClipS16((E[0] + O[0] + 64)>>7);
    Tmp[j][1]    = xClipS16((E[1] + O[1] + 64)>>7);
    Tmp[j][2]    = xClipS16((E[1] - O[1] + 64)>>7);
    Tmp[j][3]    = xClipS16((E[0] - O[0] + 64)>>7);
  }

  for(int32 j=0;j<4;j++) //horizontal transform
  {
    O[0] = 83*Tmp[1][j] + 36*Tmp[3][j];  //read with transposition
    O[1] = 36*Tmp[1][j] - 83*Tmp[3][j];
    E[0] = (Tmp[0][j] + Tmp[2][j])<<6;
    E[1] = (Tmp[0][j] - Tmp[2][j])<<6;

    Dst[0]  = xClipS16((E[0] + O[0] + Add2nd)>>Shift2nd);
    Dst[1]  = xClipS16((E[1] + O[1] + Add2nd)>>Shift2nd);
    Dst[2]  = xClipS16((E[1] - O[1] + Add2nd)>>Shift2nd);
    Dst[3]  = xClipS16((E[0] - O[0] + Add2nd)>>Shift2nd);
    Dst += DstStride;
  }
}
void xTransform::InvTransformDCT_4x4_STD_M(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[2],O[2];
  int32 Tmp[4][4];
  int32 Shift2nd = 20 - BitDepth; 
  int32 Add2nd   = 1<<(Shift2nd-1); 

  for(int32 j=0;j<4;j++) //vertical transform
  {
    O[0] = m_TrM4x4[1][0]*Src[ 4] + m_TrM4x4[3][0]*Src[12]; //read with transposition
    O[1] = m_TrM4x4[1][1]*Src[ 4] + m_TrM4x4[3][1]*Src[12];
    E[0] = m_TrM4x4[0][0]*Src[ 0] + m_TrM4x4[2][0]*Src[ 8];
    E[1] = m_TrM4x4[0][1]*Src[ 0] + m_TrM4x4[2][1]*Src[ 8];
    Src++;

    Tmp[j][0]    = xClipS16((E[0] + O[0] + 64)>>7);
    Tmp[j][1]    = xClipS16((E[1] + O[1] + 64)>>7);
    Tmp[j][2]    = xClipS16((E[1] - O[1] + 64)>>7);
    Tmp[j][3]    = xClipS16((E[0] - O[0] + 64)>>7);
  }

  for(int32 j=0;j<4;j++) //horizontal transform
  {
    O[0] = m_TrM4x4[1][0]*Tmp[1][j] + m_TrM4x4[3][0]*Tmp[3][j]; //read with transposition
    O[1] = m_TrM4x4[1][1]*Tmp[1][j] + m_TrM4x4[3][1]*Tmp[3][j];
    E[0] = m_TrM4x4[0][0]*Tmp[0][j] + m_TrM4x4[2][0]*Tmp[2][j];
    E[1] = m_TrM4x4[0][1]*Tmp[0][j] + m_TrM4x4[2][1]*Tmp[2][j];

    Dst[0]  = xClipS16((E[0] + O[0] + Add2nd)>>Shift2nd);
    Dst[1]  = xClipS16((E[1] + O[1] + Add2nd)>>Shift2nd);
    Dst[2]  = xClipS16((E[1] - O[1] + Add2nd)>>Shift2nd);
    Dst[3]  = xClipS16((E[0] - O[0] + Add2nd)>>Shift2nd);
    Dst += DstStride;
  }
}
void xTransform::InvTransformDCT_4x4_SSE(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth) //Src and Dst could be the same
{
  __m128i Tmp_0, Tmp_1;
  const int32 Shift2nd = 20 - BitDepth; 
  const __m128i xiT4_0_1 = _mm_setr_epi16(64, 83, 64, 36, 64, 36,-64,-83);
  const __m128i xiT4_2_3 = _mm_setr_epi16(64,-36,-64, 83, 64,-83, 64,-36);
  const __m128i Add1st  = _mm_set1_epi32(  64);
  const __m128i Add2nd  = _mm_set1_epi32(1<<(Shift2nd-1));

  //load and transpose
  {
    Tmp_0 = _mm_load_si128((__m128i*)(Src  ));
    Tmp_1 = _mm_load_si128((__m128i*)(Src+8));

    __m128i tr_0 = _mm_unpacklo_epi16(Tmp_0, Tmp_1);
    __m128i tr_1 = _mm_unpackhi_epi16(Tmp_0, Tmp_1);  
    Tmp_0 = _mm_unpacklo_epi16(tr_0, tr_1);
    Tmp_1 = _mm_unpackhi_epi16(tr_0, tr_1);
  }

  //vertical transforms
  {
    __m128i line_0 = _mm_unpacklo_epi64(Tmp_0, Tmp_0);
    __m128i line_1 = _mm_unpackhi_epi64(Tmp_0, Tmp_0);
    __m128i line_2 = _mm_unpacklo_epi64(Tmp_1, Tmp_1);
    __m128i line_3 = _mm_unpackhi_epi64(Tmp_1, Tmp_1);

    __m128i t_0 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_0, xiT4_0_1), _mm_madd_epi16(line_0, xiT4_2_3)), Add1st), 7);
    __m128i t_1 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_1, xiT4_0_1), _mm_madd_epi16(line_1, xiT4_2_3)), Add1st), 7);
    __m128i t_2 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_2, xiT4_0_1), _mm_madd_epi16(line_2, xiT4_2_3)), Add1st), 7);
    __m128i t_3 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_3, xiT4_0_1), _mm_madd_epi16(line_3, xiT4_2_3)), Add1st), 7);

    Tmp_0 = _mm_packs_epi32(t_0, t_1);
    Tmp_1 = _mm_packs_epi32(t_2, t_3);
  }

  //transpose
  {
    __m128i tr_0 = _mm_unpacklo_epi16(Tmp_0, Tmp_1);
    __m128i tr_1 = _mm_unpackhi_epi16(Tmp_0, Tmp_1);  
    Tmp_0 = _mm_unpacklo_epi16(tr_0, tr_1);
    Tmp_1 = _mm_unpackhi_epi16(tr_0, tr_1);  
  }

  //horizontal transforms
  {
    __m128i line_0 = _mm_unpacklo_epi64(Tmp_0, Tmp_0);
    __m128i line_1 = _mm_unpackhi_epi64(Tmp_0, Tmp_0);
    __m128i line_2 = _mm_unpacklo_epi64(Tmp_1, Tmp_1);
    __m128i line_3 = _mm_unpackhi_epi64(Tmp_1, Tmp_1);

    __m128i t_0 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_0, xiT4_0_1), _mm_madd_epi16(line_0, xiT4_2_3)), Add2nd), Shift2nd);
    __m128i t_1 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_1, xiT4_0_1), _mm_madd_epi16(line_1, xiT4_2_3)), Add2nd), Shift2nd);
    __m128i t_2 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_2, xiT4_0_1), _mm_madd_epi16(line_2, xiT4_2_3)), Add2nd), Shift2nd);
    __m128i t_3 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_3, xiT4_0_1), _mm_madd_epi16(line_3, xiT4_2_3)), Add2nd), Shift2nd);

    Tmp_0 = _mm_packs_epi32(t_0, t_1);
    Tmp_1 = _mm_packs_epi32(t_2, t_3);
  }

  //store in random area with DstStride
  _mm_storel_epi64((__m128i*)Dst, Tmp_0);
  Tmp_0 = _mm_srli_si128(Tmp_0, 8);
  Dst += DstStride;
  _mm_storel_epi64((__m128i*)Dst, Tmp_0);
  Dst += DstStride;
  _mm_storel_epi64((__m128i*)Dst, Tmp_1);
  Tmp_1 = _mm_srli_si128(Tmp_1, 8);
  Dst += DstStride;
  _mm_storel_epi64((__m128i*)Dst, Tmp_1); 
}
void xTransform::InvTransformDCT_4x4_SSE(int16* restrict Src, int16* restrict Dst, uint32 BitDepth) //Src and Dst could be the same ////no DstStride version - continous Dst buffer
{
  __m128i Tmp_0, Tmp_1;
  const int32 Shift2nd = 20 - BitDepth; 
  const __m128i xiT4_0_1 = _mm_setr_epi16(64, 83, 64, 36, 64, 36,-64,-83);
  const __m128i xiT4_2_3 = _mm_setr_epi16(64,-36,-64, 83, 64,-83, 64,-36);
  const __m128i Add1st  = _mm_set1_epi32(  64);
  const __m128i Add2nd  = _mm_set1_epi32(1<<(Shift2nd-1));

  //load and transpose
  {
    Tmp_0 = _mm_load_si128((__m128i*)(Src  ));
    Tmp_1 = _mm_load_si128((__m128i*)(Src+8));

    __m128i tr_0 = _mm_unpacklo_epi16(Tmp_0, Tmp_1);
    __m128i tr_1 = _mm_unpackhi_epi16(Tmp_0, Tmp_1);  
    Tmp_0 = _mm_unpacklo_epi16(tr_0, tr_1);
    Tmp_1 = _mm_unpackhi_epi16(tr_0, tr_1);
  }

  //vertical transforms
  {
    __m128i line_0 = _mm_unpacklo_epi64(Tmp_0, Tmp_0);
    __m128i line_1 = _mm_unpackhi_epi64(Tmp_0, Tmp_0);
    __m128i line_2 = _mm_unpacklo_epi64(Tmp_1, Tmp_1);
    __m128i line_3 = _mm_unpackhi_epi64(Tmp_1, Tmp_1);

    __m128i t_0 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_0, xiT4_0_1), _mm_madd_epi16(line_0, xiT4_2_3)), Add1st), 7);
    __m128i t_1 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_1, xiT4_0_1), _mm_madd_epi16(line_1, xiT4_2_3)), Add1st), 7);
    __m128i t_2 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_2, xiT4_0_1), _mm_madd_epi16(line_2, xiT4_2_3)), Add1st), 7);
    __m128i t_3 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_3, xiT4_0_1), _mm_madd_epi16(line_3, xiT4_2_3)), Add1st), 7);

    Tmp_0 = _mm_packs_epi32(t_0, t_1);
    Tmp_1 = _mm_packs_epi32(t_2, t_3);
  }

  //transpose
  {
    __m128i tr_0 = _mm_unpacklo_epi16(Tmp_0, Tmp_1);
    __m128i tr_1 = _mm_unpackhi_epi16(Tmp_0, Tmp_1);  
    Tmp_0 = _mm_unpacklo_epi16(tr_0, tr_1);
    Tmp_1 = _mm_unpackhi_epi16(tr_0, tr_1);  
  }

  //horizontal transforms
  {
    __m128i line_0 = _mm_unpacklo_epi64(Tmp_0, Tmp_0);
    __m128i line_1 = _mm_unpackhi_epi64(Tmp_0, Tmp_0);
    __m128i line_2 = _mm_unpacklo_epi64(Tmp_1, Tmp_1);
    __m128i line_3 = _mm_unpackhi_epi64(Tmp_1, Tmp_1);

    __m128i t_0 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_0, xiT4_0_1), _mm_madd_epi16(line_0, xiT4_2_3)), Add2nd), Shift2nd);
    __m128i t_1 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_1, xiT4_0_1), _mm_madd_epi16(line_1, xiT4_2_3)), Add2nd), Shift2nd);
    __m128i t_2 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_2, xiT4_0_1), _mm_madd_epi16(line_2, xiT4_2_3)), Add2nd), Shift2nd);
    __m128i t_3 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_3, xiT4_0_1), _mm_madd_epi16(line_3, xiT4_2_3)), Add2nd), Shift2nd);

    Tmp_0 = _mm_packs_epi32(t_0, t_1);
    Tmp_1 = _mm_packs_epi32(t_2, t_3);
  }

  //store
  _mm_store_si128((__m128i*)(Dst  ), Tmp_0);  
  _mm_store_si128((__m128i*)(Dst+8), Tmp_1); 
}
void xTransform::InvTransformDCT_8x8_STD_C(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[4],O[4];
  int32 EE[2],EO[2];
  int32 Tmp[8][8];
  int32 Shift2nd = 20 - BitDepth; 
  int32 Add2nd   = 1<<(Shift2nd-1); 

  for (int32 j=0; j<8; j++) //vertical transform
  {
    O[0] = 89*Src[1*8] + 75*Src[3*8] + 50*Src[5*8] + 18*Src[7*8]; //read with transposition
    O[1] = 75*Src[1*8] - 18*Src[3*8] - 89*Src[5*8] - 50*Src[7*8];
    O[2] = 50*Src[1*8] - 89*Src[3*8] + 18*Src[5*8] + 75*Src[7*8];
    O[3] = 18*Src[1*8] - 50*Src[3*8] + 75*Src[5*8] - 89*Src[7*8];

    EO[0] = 83*Src[2*8] + 36*Src[6*8];
    EO[1] = 36*Src[2*8] - 83*Src[6*8];
    EE[0] = (Src[0*8] + Src[4*8])<<6;
    EE[1] = (Src[0*8] - Src[4*8])<<6;
    Src++;

    E[0] = EE[0] + EO[0];
    E[3] = EE[0] - EO[0];
    E[1] = EE[1] + EO[1];
    E[2] = EE[1] - EO[1];

    for(int32 k=0;k<4;k++)
    {
      Tmp[k  ][j] = xClipS16((E[  k] + O[  k] + 64)>>7);
      Tmp[k+4][j] = xClipS16((E[3-k] - O[3-k] + 64)>>7);
    }
  }

  for (int32 j=0; j<8; j++) //horizontal transform
  {    
    O[0] = 89*Tmp[j][1] + 75*Tmp[j][3] + 50*Tmp[j][5] + 18*Tmp[j][7]; //read with transposition
    O[1] = 75*Tmp[j][1] - 18*Tmp[j][3] - 89*Tmp[j][5] - 50*Tmp[j][7];
    O[2] = 50*Tmp[j][1] - 89*Tmp[j][3] + 18*Tmp[j][5] + 75*Tmp[j][7];
    O[3] = 18*Tmp[j][1] - 50*Tmp[j][3] + 75*Tmp[j][5] - 89*Tmp[j][7];

    EO[0] = 83*Tmp[j][2] + 36*Tmp[j][6];
    EO[1] = 36*Tmp[j][2] - 83*Tmp[j][6];
    EE[0] = (Tmp[j][0] + Tmp[j][4])<<6;
    EE[1] = (Tmp[j][0] - Tmp[j][4])<<6;

    E[0] = EE[0] + EO[0];
    E[3] = EE[0] - EO[0];
    E[1] = EE[1] + EO[1];
    E[2] = EE[1] - EO[1];

    for(int32 k=0;k<4;k++)
    {
      Dst[k  ] = xClipS16((E[  k] + O[  k] + Add2nd)>>Shift2nd);
      Dst[k+4] = xClipS16((E[3-k] - O[3-k] + Add2nd)>>Shift2nd);
    }

    Dst+= DstStride;
  }  
}
void xTransform::InvTransformDCT_8x8_STD_M(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[4],O[4];
  int32 EE[2],EO[2];
  int32 Tmp[8][8];
  int32 Shift2nd = 20 - BitDepth; 
  int32 Add2nd   = 1<<(Shift2nd-1); 

  for (int32 j=0; j<8; j++) //vertical transform
  {
    for (int32 k=0;k<4;k++)
    {
      O[k] = m_TrM8x8[ 1][k]*Src[1*8] + m_TrM8x8[ 3][k]*Src[3*8] + m_TrM8x8[ 5][k]*Src[5*8] + m_TrM8x8[ 7][k]*Src[7*8];
    }

    EO[0] = m_TrM8x8[2][0]*Src[ 2*8 ] + m_TrM8x8[6][0]*Src[ 6*8 ];
    EO[1] = m_TrM8x8[2][1]*Src[ 2*8 ] + m_TrM8x8[6][1]*Src[ 6*8 ];
    EE[0] = m_TrM8x8[0][0]*Src[ 0*8 ] + m_TrM8x8[4][0]*Src[ 4*8 ];
    EE[1] = m_TrM8x8[0][1]*Src[ 0*8 ] + m_TrM8x8[4][1]*Src[ 4*8 ];

    EO[0] = 83*Src[2*8] + 36*Src[6*8];
    EO[1] = 36*Src[2*8] - 83*Src[6*8];
    EE[0] = (Src[0*8] + Src[4*8])<<6;
    EE[1] = (Src[0*8] - Src[4*8])<<6;
    Src++;

    E[0] = EE[0] + EO[0];
    E[3] = EE[0] - EO[0];
    E[1] = EE[1] + EO[1];
    E[2] = EE[1] - EO[1];

    for(int32 k=0;k<4;k++)
    {
      Tmp[k  ][j] = xClipS16((E[  k] + O[  k] + 64)>>7);
      Tmp[k+4][j] = xClipS16((E[3-k] - O[3-k] + 64)>>7);
    }
  }

  for (int32 j=0; j<8; j++) //horizontal transform
  {   
    for (int32 k=0;k<4;k++)
    {
      O[k] = m_TrM8x8[ 1][k]*Tmp[j][1] + m_TrM8x8[ 3][k]*Tmp[j][3] + m_TrM8x8[ 5][k]*Tmp[j][5] + m_TrM8x8[ 7][k]*Tmp[j][7];
    }

    EO[0] = m_TrM8x8[2][0]*Tmp[j][2] + m_TrM8x8[6][0]*Tmp[j][6];
    EO[1] = m_TrM8x8[2][1]*Tmp[j][2] + m_TrM8x8[6][1]*Tmp[j][6];
    EE[0] = m_TrM8x8[0][0]*Tmp[j][0] + m_TrM8x8[4][0]*Tmp[j][4];
    EE[1] = m_TrM8x8[0][1]*Tmp[j][0] + m_TrM8x8[4][1]*Tmp[j][4];

    E[0] = EE[0] + EO[0];
    E[3] = EE[0] - EO[0];
    E[1] = EE[1] + EO[1];
    E[2] = EE[1] - EO[1];

    for(int32 k=0;k<4;k++)
    {
      Dst[k  ] = xClipS16((E[  k] + O[  k] + Add2nd)>>Shift2nd);
      Dst[k+4] = xClipS16((E[3-k] - O[3-k] + Add2nd)>>Shift2nd);
    }

    Dst+= DstStride;
  }  
}
void xTransform::InvTransformDCT_8x8_SSE(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth)
{
  __m128i Tmp[8];
 const __m128i xiT8[8] = {_mm_setr_epi16(64, 89, 83, 75, 64, 50, 36, 18),
                          _mm_setr_epi16(64, 75, 36,-18,-64,-89,-83,-50),
                          _mm_setr_epi16(64, 50,-36,-89,-64, 18, 83, 75),
                          _mm_setr_epi16(64, 18,-83,-50, 64, 75,-36,-89),
                          _mm_setr_epi16(64,-18,-83, 50, 64,-75,-36, 89),
                          _mm_setr_epi16(64,-50,-36, 89,-64,-18, 83,-75),
                          _mm_setr_epi16(64,-75, 36, 18,-64, 89,-83, 50),
                          _mm_setr_epi16(64,-89, 83,-75, 64,-50, 36,-18)};

  const int32 Shift2nd = 20 - BitDepth; 
  const __m128i Add1st  = _mm_set1_epi32(  64);
  const __m128i Add2nd  = _mm_set1_epi32(1<<(Shift2nd-1));

  //load
  for(int32 j=0; j<8; j++)
  {
    Tmp[j] = _mm_loadu_si128((__m128i*)Src);
    Src += 8;   
  } 

  //transpose  
  {
    __m128i tr_A0 = _mm_unpacklo_epi16(Tmp[0], Tmp[4]);
    __m128i tr_A1 = _mm_unpackhi_epi16(Tmp[0], Tmp[4]);
    __m128i tr_A2 = _mm_unpacklo_epi16(Tmp[1], Tmp[5]);
    __m128i tr_A3 = _mm_unpackhi_epi16(Tmp[1], Tmp[5]);
    __m128i tr_A4 = _mm_unpacklo_epi16(Tmp[2], Tmp[6]);
    __m128i tr_A5 = _mm_unpackhi_epi16(Tmp[2], Tmp[6]);
    __m128i tr_A6 = _mm_unpacklo_epi16(Tmp[3], Tmp[7]);
    __m128i tr_A7 = _mm_unpackhi_epi16(Tmp[3], Tmp[7]);

    __m128i tr_B0 = _mm_unpacklo_epi16(tr_A0, tr_A4);
    __m128i tr_B1 = _mm_unpackhi_epi16(tr_A0, tr_A4);
    __m128i tr_B2 = _mm_unpacklo_epi16(tr_A1, tr_A5);
    __m128i tr_B3 = _mm_unpackhi_epi16(tr_A1, tr_A5);
    __m128i tr_B4 = _mm_unpacklo_epi16(tr_A2, tr_A6);
    __m128i tr_B5 = _mm_unpackhi_epi16(tr_A2, tr_A6);
    __m128i tr_B6 = _mm_unpacklo_epi16(tr_A3, tr_A7);
    __m128i tr_B7 = _mm_unpackhi_epi16(tr_A3, tr_A7);

    Tmp[0] = _mm_unpacklo_epi16(tr_B0, tr_B4);
    Tmp[1] = _mm_unpackhi_epi16(tr_B0, tr_B4);
    Tmp[2] = _mm_unpacklo_epi16(tr_B1, tr_B5);
    Tmp[3] = _mm_unpackhi_epi16(tr_B1, tr_B5);
    Tmp[4] = _mm_unpacklo_epi16(tr_B2, tr_B6);
    Tmp[5] = _mm_unpackhi_epi16(tr_B2, tr_B6);
    Tmp[6] = _mm_unpacklo_epi16(tr_B3, tr_B7);
    Tmp[7] = _mm_unpackhi_epi16(tr_B3, tr_B7);
  } 
  
  //vertical transform
  for(int32 j=0; j<8; j++)
  {
    __m128i line_j = Tmp[j];
        
    __m128i tr_0 = _mm_madd_epi16(line_j, xiT8[0]);
    __m128i tr_1 = _mm_madd_epi16(line_j, xiT8[1]);
    __m128i tr_2 = _mm_madd_epi16(line_j, xiT8[2]);
    __m128i tr_3 = _mm_madd_epi16(line_j, xiT8[3]);
    __m128i tr_4 = _mm_madd_epi16(line_j, xiT8[4]);
    __m128i tr_5 = _mm_madd_epi16(line_j, xiT8[5]);
    __m128i tr_6 = _mm_madd_epi16(line_j, xiT8[6]);
    __m128i tr_7 = _mm_madd_epi16(line_j, xiT8[7]);

    __m128i tr_01 = _mm_hadd_epi32(tr_0, tr_1);
    __m128i tr_23 = _mm_hadd_epi32(tr_2, tr_3);
    __m128i tr_45 = _mm_hadd_epi32(tr_4, tr_5);
    __m128i tr_67 = _mm_hadd_epi32(tr_6, tr_7);

    __m128i tr_0123 = _mm_hadd_epi32(tr_01, tr_23);
    __m128i tr_4567 = _mm_hadd_epi32(tr_45, tr_67);
    
    tr_0123 = _mm_add_epi32(tr_0123, Add1st);
    tr_0123 = _mm_srai_epi32(tr_0123, 7);

    tr_4567 = _mm_add_epi32(tr_4567, Add1st);
    tr_4567 = _mm_srai_epi32(tr_4567, 7);
  
    Tmp[j] = _mm_packs_epi32(tr_0123, tr_4567); 
  } 
  
  //transpose  
  {
    __m128i tr_A0 = _mm_unpacklo_epi16(Tmp[0], Tmp[4]);
    __m128i tr_A1 = _mm_unpackhi_epi16(Tmp[0], Tmp[4]);
    __m128i tr_A2 = _mm_unpacklo_epi16(Tmp[1], Tmp[5]);
    __m128i tr_A3 = _mm_unpackhi_epi16(Tmp[1], Tmp[5]);
    __m128i tr_A4 = _mm_unpacklo_epi16(Tmp[2], Tmp[6]);
    __m128i tr_A5 = _mm_unpackhi_epi16(Tmp[2], Tmp[6]);
    __m128i tr_A6 = _mm_unpacklo_epi16(Tmp[3], Tmp[7]);
    __m128i tr_A7 = _mm_unpackhi_epi16(Tmp[3], Tmp[7]);

    __m128i tr_B0 = _mm_unpacklo_epi16(tr_A0, tr_A4);
    __m128i tr_B1 = _mm_unpackhi_epi16(tr_A0, tr_A4);
    __m128i tr_B2 = _mm_unpacklo_epi16(tr_A1, tr_A5);
    __m128i tr_B3 = _mm_unpackhi_epi16(tr_A1, tr_A5);
    __m128i tr_B4 = _mm_unpacklo_epi16(tr_A2, tr_A6);
    __m128i tr_B5 = _mm_unpackhi_epi16(tr_A2, tr_A6);
    __m128i tr_B6 = _mm_unpacklo_epi16(tr_A3, tr_A7);
    __m128i tr_B7 = _mm_unpackhi_epi16(tr_A3, tr_A7);

    Tmp[0] = _mm_unpacklo_epi16(tr_B0, tr_B4);
    Tmp[1] = _mm_unpackhi_epi16(tr_B0, tr_B4);
    Tmp[2] = _mm_unpacklo_epi16(tr_B1, tr_B5);
    Tmp[3] = _mm_unpackhi_epi16(tr_B1, tr_B5);
    Tmp[4] = _mm_unpacklo_epi16(tr_B2, tr_B6);
    Tmp[5] = _mm_unpackhi_epi16(tr_B2, tr_B6);
    Tmp[6] = _mm_unpacklo_epi16(tr_B3, tr_B7);
    Tmp[7] = _mm_unpackhi_epi16(tr_B3, tr_B7);
  }  

  //horizontal transform and store
  for(int32 j=0; j<8; j++)
  {
    __m128i line_j = Tmp[j];    
    
    __m128i tr_0 = _mm_madd_epi16(line_j, xiT8[0]);
    __m128i tr_1 = _mm_madd_epi16(line_j, xiT8[1]);
    __m128i tr_2 = _mm_madd_epi16(line_j, xiT8[2]);
    __m128i tr_3 = _mm_madd_epi16(line_j, xiT8[3]);
    __m128i tr_4 = _mm_madd_epi16(line_j, xiT8[4]);
    __m128i tr_5 = _mm_madd_epi16(line_j, xiT8[5]);
    __m128i tr_6 = _mm_madd_epi16(line_j, xiT8[6]);
    __m128i tr_7 = _mm_madd_epi16(line_j, xiT8[7]);

    __m128i tr_01 = _mm_hadd_epi32(tr_0, tr_1);
    __m128i tr_23 = _mm_hadd_epi32(tr_2, tr_3);
    __m128i tr_45 = _mm_hadd_epi32(tr_4, tr_5);
    __m128i tr_67 = _mm_hadd_epi32(tr_6, tr_7);

    __m128i tr_0123 = _mm_hadd_epi32(tr_01, tr_23);
    __m128i tr_4567 = _mm_hadd_epi32(tr_45, tr_67);
    
    tr_0123 = _mm_add_epi32(tr_0123, Add2nd);
    tr_0123 = _mm_srai_epi32(tr_0123, Shift2nd);

    tr_4567 = _mm_add_epi32(tr_4567, Add2nd);
    tr_4567 = _mm_srai_epi32(tr_4567, Shift2nd);
  
    __m128i tr = _mm_packs_epi32(tr_0123, tr_4567); 
    _mm_storeu_si128((__m128i*)Dst, tr);
    Dst += DstStride; 
  }    
}
void xTransform::InvTransformDCT_8x8_SSEv2(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth)
{
  __m128i Tmp[8];
  __m128i Tmp2[8];
 const __m128i xiT8[8] = {_mm_setr_epi16(64, 89, 83, 75, 64, 50, 36, 18),
                          _mm_setr_epi16(64, 75, 36,-18,-64,-89,-83,-50),
                          _mm_setr_epi16(64, 50,-36,-89,-64, 18, 83, 75),
                          _mm_setr_epi16(64, 18,-83,-50, 64, 75,-36,-89),
                          _mm_setr_epi16(64,-18,-83, 50, 64,-75,-36, 89),
                          _mm_setr_epi16(64,-50,-36, 89,-64,-18, 83,-75),
                          _mm_setr_epi16(64,-75, 36, 18,-64, 89,-83, 50),
                          _mm_setr_epi16(64,-89, 83,-75, 64,-50, 36,-18)};

  const int32 Shift2nd = 20 - BitDepth; 
  const __m128i Add1st  = _mm_set1_epi32(  64);
  const __m128i Add2nd  = _mm_set1_epi32(1<<(Shift2nd-1));

  //load
  for(int32 j=0; j<8; j++)
  {
    Tmp[j] = _mm_loadu_si128((__m128i*)Src);
    Src += 8;   
  }  
  
  //vertical transform  
  for(int32 i=0; i<4; i++) //per O and E
  {   
    __m128i SumEA = _mm_setzero_si128();
    __m128i SumEB = _mm_setzero_si128();
    __m128i SumOA = _mm_setzero_si128();
    __m128i SumOB = _mm_setzero_si128();

    for(int32 j=0; j<8; j+=2)
    {
      //Even
      {
      __m128i Line1 = Tmp[j  ];
      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));
      __m128i TX = _mm_set1_epi32(m_TrM8x8[j][i]);
      Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);
      SumEA = _mm_add_epi32(SumEA, Line1A);
      SumEB = _mm_add_epi32(SumEB, Line1B);  
      }
      //Odd
      {
      __m128i Line1 = Tmp[j+1 ];
      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));
      __m128i TX = _mm_set1_epi32(m_TrM8x8[j+1][i]);
      Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);
      SumOA = _mm_add_epi32(SumOA, Line1A);
      SumOB = _mm_add_epi32(SumOB, Line1B); 
      }
    }

    __m128i EpOA = _mm_add_epi32(SumEA, SumOA);
    __m128i EpOB = _mm_add_epi32(SumEB, SumOB);
    __m128i EmOA = _mm_sub_epi32(SumEA, SumOA);
    __m128i EmOB = _mm_sub_epi32(SumEB, SumOB);

    EpOA = _mm_srai_epi32(_mm_add_epi32(EpOA, Add1st),7);
    EpOB = _mm_srai_epi32(_mm_add_epi32(EpOB, Add1st),7);
    EmOA = _mm_srai_epi32(_mm_add_epi32(EmOA, Add1st),7);
    EmOB = _mm_srai_epi32(_mm_add_epi32(EmOB, Add1st),7);

    Tmp2[i  ] = _mm_packs_epi32(EpOA, EpOB);
    Tmp2[7-i] = _mm_packs_epi32(EmOA, EmOB);     
  }

  //horizontal transform and store
  for(int32 j=0; j<8; j++)
  {
    __m128i line_j = Tmp2[j];    
    
    __m128i tr_0 = _mm_madd_epi16(line_j, xiT8[0]);
    __m128i tr_1 = _mm_madd_epi16(line_j, xiT8[1]);
    __m128i tr_2 = _mm_madd_epi16(line_j, xiT8[2]);
    __m128i tr_3 = _mm_madd_epi16(line_j, xiT8[3]);
    __m128i tr_4 = _mm_madd_epi16(line_j, xiT8[4]);
    __m128i tr_5 = _mm_madd_epi16(line_j, xiT8[5]);
    __m128i tr_6 = _mm_madd_epi16(line_j, xiT8[6]);
    __m128i tr_7 = _mm_madd_epi16(line_j, xiT8[7]);

    __m128i tr_01 = _mm_hadd_epi32(tr_0, tr_1);
    __m128i tr_23 = _mm_hadd_epi32(tr_2, tr_3);
    __m128i tr_45 = _mm_hadd_epi32(tr_4, tr_5);
    __m128i tr_67 = _mm_hadd_epi32(tr_6, tr_7);

    __m128i tr_0123 = _mm_hadd_epi32(tr_01, tr_23);
    __m128i tr_4567 = _mm_hadd_epi32(tr_45, tr_67);
    
    tr_0123 = _mm_add_epi32(tr_0123, Add2nd);
    tr_0123 = _mm_srai_epi32(tr_0123, Shift2nd);

    tr_4567 = _mm_add_epi32(tr_4567, Add2nd);
    tr_4567 = _mm_srai_epi32(tr_4567, Shift2nd);
  
    __m128i tr = _mm_packs_epi32(tr_0123, tr_4567); 
    _mm_storeu_si128((__m128i*)Dst, tr);
    Dst += DstStride; 
  }    
}
void xTransform::InvTransformDCT_8x8_SSEv3(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth)//vertical transform uses 2 symmetries, horizontal transform uses 0 symmetries
{
  __m128i Tmp[8];
  __m128i Tmp2[8];
 const __m128i xiT8[8] = {_mm_setr_epi16(64, 89, 83, 75, 64, 50, 36, 18),
                          _mm_setr_epi16(64, 75, 36,-18,-64,-89,-83,-50),
                          _mm_setr_epi16(64, 50,-36,-89,-64, 18, 83, 75),
                          _mm_setr_epi16(64, 18,-83,-50, 64, 75,-36,-89),
                          _mm_setr_epi16(64,-18,-83, 50, 64,-75,-36, 89),
                          _mm_setr_epi16(64,-50,-36, 89,-64,-18, 83,-75),
                          _mm_setr_epi16(64,-75, 36, 18,-64, 89,-83, 50),
                          _mm_setr_epi16(64,-89, 83,-75, 64,-50, 36,-18)};

  const int32 Shift2nd = 20 - BitDepth; 
  const __m128i Add1st  = _mm_set1_epi32(  64);
  const __m128i Add2nd  = _mm_set1_epi32(1<<(Shift2nd-1));

  //load
  for(int32 j=0; j<8; j++)
  {
    Tmp[j] = _mm_loadu_si128((__m128i*)Src);
    Src += 8;   
  }  
  
  //vertical transform  
  for(int32 i=0; i<2; i++) //per O and E
  {   
    __m128i SumEAe = _mm_setzero_si128();//even lines + +
    __m128i SumEBe = _mm_setzero_si128();

    __m128i SumOA = _mm_setzero_si128();//odd lines
    __m128i SumOB = _mm_setzero_si128();

  __m128i SumOAm = _mm_setzero_si128();
    __m128i SumOBm = _mm_setzero_si128();

  __m128i SumEAo = _mm_setzero_si128();//even lines + -
    __m128i SumEBo = _mm_setzero_si128();
    for(int32 j=0; j<8; j+=4)
    {
      //Even Even + +
      {
      __m128i Line1 = Tmp[j  ];
      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));
      __m128i TX = _mm_set1_epi32(m_TrM8x8[j][i]);
      Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);
      SumEAe = _mm_add_epi32(SumEAe, Line1A);
      SumEBe = _mm_add_epi32(SumEBe, Line1B);  
      }
      //Odd x y
      {
      __m128i Line1 = Tmp[j+1 ];
      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));
      __m128i TX = _mm_set1_epi32(m_TrM8x8[j+1][i]);
    __m128i TXm= _mm_set1_epi32(m_TrM8x8[j+1][3-i]);

    __m128i Line1Am = _mm_mullo_epi32(Line1A, TXm);
      __m128i Line1Bm = _mm_mullo_epi32(Line1B, TXm);
      Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);
    
      SumOA = _mm_add_epi32(SumOA, Line1A);
      SumOB = _mm_add_epi32(SumOB, Line1B); 

    SumOAm = _mm_add_epi32(SumOAm, Line1Am);
      SumOBm = _mm_add_epi32(SumOBm, Line1Bm); 
      }
    //Even odd + -
    {
      __m128i Line1 = Tmp[j+2 ];
      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));
      __m128i TX = _mm_set1_epi32(m_TrM8x8[j+2][i]);
      Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);
      SumEAo = _mm_add_epi32(SumEAo, Line1A);
      SumEBo = _mm_add_epi32(SumEBo, Line1B);  
    }
    //Odd x y
      {
      __m128i Line1 = Tmp[j+3 ];
      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));
      __m128i TX = _mm_set1_epi32(m_TrM8x8[j+3][i]);
    __m128i TXm= _mm_set1_epi32(m_TrM8x8[j+3][3-i]);

    __m128i Line1Am = _mm_mullo_epi32(Line1A, TXm);
      __m128i Line1Bm = _mm_mullo_epi32(Line1B, TXm);
      Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);
    
      SumOA = _mm_add_epi32(SumOA, Line1A);
      SumOB = _mm_add_epi32(SumOB, Line1B); 

    SumOAm = _mm_add_epi32(SumOAm, Line1Am);
      SumOBm = _mm_add_epi32(SumOBm, Line1Bm); 
      }
    }

  __m128i SumEAeo=_mm_add_epi32(SumEAe,SumEAo);
  __m128i SumEBeo=_mm_add_epi32(SumEBe,SumEBo);

  __m128i DifEAeo=_mm_sub_epi32(SumEAe,SumEAo);
  __m128i DifEBeo=_mm_sub_epi32(SumEBe,SumEBo);

  //line0
    __m128i EpOA = _mm_add_epi32(SumEAeo,SumOA);
    __m128i EpOB = _mm_add_epi32(SumEBeo,SumOB);
   
  //line3
  __m128i ElOA = _mm_add_epi32(DifEAeo,SumOAm);
  __m128i ElOB = _mm_add_epi32(DifEBeo,SumOBm);
  //line4
  __m128i EkOA = _mm_sub_epi32(DifEAeo,SumOAm);
  __m128i EkOB = _mm_sub_epi32(DifEBeo,SumOBm);
  
   //line7
  __m128i EmOA = _mm_sub_epi32(SumEAeo, SumOA);
    __m128i EmOB = _mm_sub_epi32(SumEBeo, SumOB);

    EpOA = _mm_srai_epi32(_mm_add_epi32(EpOA, Add1st),7);
    EpOB = _mm_srai_epi32(_mm_add_epi32(EpOB, Add1st),7);
  
  EkOA = _mm_srai_epi32(_mm_add_epi32(EkOA, Add1st),7);
    EkOB = _mm_srai_epi32(_mm_add_epi32(EkOB, Add1st),7);

  ElOA = _mm_srai_epi32(_mm_add_epi32(ElOA, Add1st),7);
    ElOB = _mm_srai_epi32(_mm_add_epi32(ElOB, Add1st),7);

    EmOA = _mm_srai_epi32(_mm_add_epi32(EmOA, Add1st),7);
    EmOB = _mm_srai_epi32(_mm_add_epi32(EmOB, Add1st),7);

    Tmp2[i  ] = _mm_packs_epi32(EpOA, EpOB);

  Tmp2[3-i] = _mm_packs_epi32(ElOA,ElOB);
  Tmp2[4+i] = _mm_packs_epi32(EkOA,EkOB);

    Tmp2[7-i] = _mm_packs_epi32(EmOA, EmOB);     
  }

  //horizontal transform and store
  for(int32 j=0; j<8; j++)
  {
    __m128i line_j = Tmp2[j];    
    
    __m128i tr_0 = _mm_madd_epi16(line_j, xiT8[0]);
    __m128i tr_1 = _mm_madd_epi16(line_j, xiT8[1]);
    __m128i tr_2 = _mm_madd_epi16(line_j, xiT8[2]);
    __m128i tr_3 = _mm_madd_epi16(line_j, xiT8[3]);
    __m128i tr_4 = _mm_madd_epi16(line_j, xiT8[4]);
    __m128i tr_5 = _mm_madd_epi16(line_j, xiT8[5]);
    __m128i tr_6 = _mm_madd_epi16(line_j, xiT8[6]);
    __m128i tr_7 = _mm_madd_epi16(line_j, xiT8[7]);

    __m128i tr_01 = _mm_hadd_epi32(tr_0, tr_1);
    __m128i tr_23 = _mm_hadd_epi32(tr_2, tr_3);
    __m128i tr_45 = _mm_hadd_epi32(tr_4, tr_5);
    __m128i tr_67 = _mm_hadd_epi32(tr_6, tr_7);

    __m128i tr_0123 = _mm_hadd_epi32(tr_01, tr_23);
    __m128i tr_4567 = _mm_hadd_epi32(tr_45, tr_67);
    
    tr_0123 = _mm_add_epi32(tr_0123, Add2nd);
    tr_0123 = _mm_srai_epi32(tr_0123, Shift2nd);

    tr_4567 = _mm_add_epi32(tr_4567, Add2nd);
    tr_4567 = _mm_srai_epi32(tr_4567, Shift2nd);
  
    __m128i tr = _mm_packs_epi32(tr_0123, tr_4567); 
    _mm_storeu_si128((__m128i*)Dst, tr);
    Dst += DstStride; 
  }    
}
void xTransform::InvTransformDCT_16x16_STD_C(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[8],O[8];
  int32 EE[4],EO[4];
  int32 EEE[2],EEO[2];
  int32 Tmp[16][16];
  int32 Shift2nd = 20 - BitDepth; 
  int32 Add2nd   = 1<<(Shift2nd-1); 

  for(int32 j=0; j<16; j++) //vertical transform
  {
    O[0] = 90*Src[ 16] + 87*Src[ 3*16] + 80*Src[ 5*16] + 70*Src[ 7*16] + 57*Src[ 9*16] + 43*Src[11*16] + 25*Src[13*16] +  9*Src[15*16];
    O[1] = 87*Src[ 16] + 57*Src[ 3*16] +  9*Src[ 5*16] - 43*Src[ 7*16] - 80*Src[ 9*16] - 90*Src[11*16] - 70*Src[13*16] - 25*Src[15*16];
    O[2] = 80*Src[ 16] +  9*Src[ 3*16] - 70*Src[ 5*16] - 87*Src[ 7*16] - 25*Src[ 9*16] + 57*Src[11*16] + 90*Src[13*16] + 43*Src[15*16];
    O[3] = 70*Src[ 16] - 43*Src[ 3*16] - 87*Src[ 5*16] +  9*Src[ 7*16] + 90*Src[ 9*16] + 25*Src[11*16] - 80*Src[13*16] - 57*Src[15*16];
    O[4] = 57*Src[ 16] - 80*Src[ 3*16] - 25*Src[ 5*16] + 90*Src[ 7*16] -  9*Src[ 9*16] - 87*Src[11*16] + 43*Src[13*16] + 70*Src[15*16];
    O[5] = 43*Src[ 16] - 90*Src[ 3*16] + 57*Src[ 5*16] + 25*Src[ 7*16] - 87*Src[ 9*16] + 70*Src[11*16] +  9*Src[13*16] - 80*Src[15*16];
    O[6] = 25*Src[ 16] - 70*Src[ 3*16] + 90*Src[ 5*16] - 80*Src[ 7*16] + 43*Src[ 9*16] +  9*Src[11*16] - 57*Src[13*16] + 87*Src[15*16];
    O[7] =  9*Src[ 16] - 25*Src[ 3*16] + 43*Src[ 5*16] - 57*Src[ 7*16] + 70*Src[ 9*16] - 80*Src[11*16] + 87*Src[13*16] - 90*Src[15*16];

    EO[0] = 89*Src[ 2*16] + 75*Src[ 6*16] + 50*Src[10*16] + 18*Src[14*16];
    EO[1] = 75*Src[ 2*16] - 18*Src[ 6*16] - 89*Src[10*16] - 50*Src[14*16];
    EO[2] = 50*Src[ 2*16] - 89*Src[ 6*16] + 18*Src[10*16] + 75*Src[14*16];
    EO[3] = 18*Src[ 2*16] - 50*Src[ 6*16] + 75*Src[10*16] - 89*Src[14*16];

    EEO[0] = 83*Src[ 4*16 ] + 36*Src[ 12*16];
    EEO[1] = 36*Src[ 4*16 ] - 83*Src[ 12*16];
    EEE[0] = (Src[ 0    ] + Src[ 8*16 ])<<6;    
    EEE[1] = (Src[ 0    ] - Src[ 8*16 ])<<6;
    Src++;

    for (int32 k=0;k<2;k++)
    {
      EE[k] = EEE[k] + EEO[k];
      EE[k+2] = EEE[1-k] - EEO[1-k];
    }  

    for (int32 k=0;k<4;k++)
    {
      E[k] = EE[k] + EO[k];
      E[k+4] = EE[3-k] - EO[3-k];
    }

    for (int32 k=0;k<8;k++)
    {
      Tmp[j][k  ] = xClipS16((E[  k] + O[  k] + 64)>>7);
      Tmp[j][k+8] = xClipS16((E[7-k] - O[7-k] + 64)>>7);
    }
  }

  for(int32 j=0; j<16; j++) //horizotal transform
  {
    O[0] = 90*Tmp[1][j] + 87*Tmp[3][j] + 80*Tmp[5][j] + 70*Tmp[7][j] + 57*Tmp[9][j] + 43*Tmp[11][j] + 25*Tmp[13][j] +  9*Tmp[15][j];
    O[1] = 87*Tmp[1][j] + 57*Tmp[3][j] +  9*Tmp[5][j] - 43*Tmp[7][j] - 80*Tmp[9][j] - 90*Tmp[11][j] - 70*Tmp[13][j] - 25*Tmp[15][j];
    O[2] = 80*Tmp[1][j] +  9*Tmp[3][j] - 70*Tmp[5][j] - 87*Tmp[7][j] - 25*Tmp[9][j] + 57*Tmp[11][j] + 90*Tmp[13][j] + 43*Tmp[15][j];
    O[3] = 70*Tmp[1][j] - 43*Tmp[3][j] - 87*Tmp[5][j] +  9*Tmp[7][j] + 90*Tmp[9][j] + 25*Tmp[11][j] - 80*Tmp[13][j] - 57*Tmp[15][j];
    O[4] = 57*Tmp[1][j] - 80*Tmp[3][j] - 25*Tmp[5][j] + 90*Tmp[7][j] -  9*Tmp[9][j] - 87*Tmp[11][j] + 43*Tmp[13][j] + 70*Tmp[15][j];
    O[5] = 43*Tmp[1][j] - 90*Tmp[3][j] + 57*Tmp[5][j] + 25*Tmp[7][j] - 87*Tmp[9][j] + 70*Tmp[11][j] +  9*Tmp[13][j] - 80*Tmp[15][j];
    O[6] = 25*Tmp[1][j] - 70*Tmp[3][j] + 90*Tmp[5][j] - 80*Tmp[7][j] + 43*Tmp[9][j] +  9*Tmp[11][j] - 57*Tmp[13][j] + 87*Tmp[15][j];
    O[7] =  9*Tmp[1][j] - 25*Tmp[3][j] + 43*Tmp[5][j] - 57*Tmp[7][j] + 70*Tmp[9][j] - 80*Tmp[11][j] + 87*Tmp[13][j] - 90*Tmp[15][j];

    EO[0] = 89*Tmp[2][j] + 75*Tmp[6][j] + 50*Tmp[10][j] + 18*Tmp[14][j];
    EO[1] = 75*Tmp[2][j] - 18*Tmp[6][j] - 89*Tmp[10][j] - 50*Tmp[14][j];
    EO[2] = 50*Tmp[2][j] - 89*Tmp[6][j] + 18*Tmp[10][j] + 75*Tmp[14][j];
    EO[3] = 18*Tmp[2][j] - 50*Tmp[6][j] + 75*Tmp[10][j] - 89*Tmp[14][j];

    EEO[0] = 83*Tmp[4][j] + 36*Tmp[12][j];
    EEO[1] = 36*Tmp[4][j] - 83*Tmp[12][j];
    EEE[0] = (Tmp[0][j] + Tmp[8][j])<<6;
    EEE[1] = (Tmp[0][j] - Tmp[8][j])<<6;

    for (int32 k=0;k<2;k++)
    {
      EE[k  ] = EEE[  k] + EEO[  k];
      EE[k+2] = EEE[1-k] - EEO[1-k];
    }   

    for (int32 k=0;k<4;k++)
    {
      E[k  ] = EE[  k] + EO[  k];
      E[k+4] = EE[3-k] - EO[3-k];
    }

    for (int32 k=0;k<8;k++)
    {
      Dst[k  ] = xClipS16((E[  k] + O[  k] + Add2nd)>>Shift2nd);
      Dst[k+8] = xClipS16((E[7-k] - O[7-k] + Add2nd)>>Shift2nd);
    }
    Dst += DstStride;
  }
}
void xTransform::InvTransformDCT_16x16_STD_M(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[8],O[8];
  int32 EE[4],EO[4];
  int32 EEE[2],EEO[2];
  int32 Tmp[16][16];
  int32 Shift2nd = 20 - BitDepth; 
  int32 Add2nd   = 1<<(Shift2nd-1); 

  for(int32 j=0; j<16; j++) //vertical transform
  {
    for (int32 k=0;k<8;k++)
    {
      O[k] = m_TrM16x16[ 1][k]*Src[ 0*16] + m_TrM16x16[ 3][k]*Src[ 3*16] + m_TrM16x16[ 5][k]*Src[ 5*16] + m_TrM16x16[ 7][k]*Src[ 7*16] + 
             m_TrM16x16[ 9][k]*Src[ 9*16] + m_TrM16x16[11][k]*Src[11*16] + m_TrM16x16[13][k]*Src[13*16] + m_TrM16x16[15][k]*Src[15*16];
    }
    for (int32 k=0;k<4;k++)
    {
      EO[k] = m_TrM16x16[ 2][k]*Src[ 2*16] + m_TrM16x16[ 6][k]*Src[ 6*16] + m_TrM16x16[10][k]*Src[10*16] + m_TrM16x16[14][k]*Src[14*16];
    }

    EEO[0] = 83*Src[ 4*16 ] + 36*Src[ 12*16];
    EEO[1] = 36*Src[ 4*16 ] - 83*Src[ 12*16];
    EEE[0] = (Src[ 0    ] + Src[ 8*16 ])<<6;    
    EEE[1] = (Src[ 0    ] - Src[ 8*16 ])<<6;
    Src++;

    for (int32 k=0;k<2;k++)
    {
      EE[k] = EEE[k] + EEO[k];
      EE[k+2] = EEE[1-k] - EEO[1-k];
    }  

    for (int32 k=0;k<4;k++)
    {
      E[k] = EE[k] + EO[k];
      E[k+4] = EE[3-k] - EO[3-k];
    }

    for (int32 k=0;k<8;k++)
    {
      Tmp[j][k  ] = xClipS16((E[  k] + O[  k] + 64)>>7);
      Tmp[j][k+8] = xClipS16((E[7-k] - O[7-k] + 64)>>7);
    }
  }

  for(int32 j=0; j<16; j++) //horizotal transform
  {

    for (int32 k=0;k<8;k++)
    {
      O[k] = m_TrM16x16[ 1][k]*Tmp[ 0][j] + m_TrM16x16[ 3][k]*Tmp[ 3][j] + m_TrM16x16[ 5][k]*Tmp[ 5][j] + m_TrM16x16[ 7][k]*Tmp[ 7][j] + 
             m_TrM16x16[ 9][k]*Tmp[ 9][j] + m_TrM16x16[11][k]*Tmp[11][j] + m_TrM16x16[13][k]*Tmp[13][j] + m_TrM16x16[15][k]*Tmp[15][j];
    }
    for (int32 k=0;k<4;k++)
    {
      EO[k] = m_TrM16x16[ 2][k]*Tmp[ 2][j] + m_TrM16x16[ 6][k]*Tmp[ 6][j] + m_TrM16x16[10][k]*Tmp[10][j] + m_TrM16x16[14][k]*Tmp[14][j];
    }  

    EEO[0] = 83*Tmp[4][j] + 36*Tmp[12][j];
    EEO[1] = 36*Tmp[4][j] - 83*Tmp[12][j];
    EEE[0] = (Tmp[0][j] + Tmp[8][j])<<6;
    EEE[1] = (Tmp[0][j] - Tmp[8][j])<<6;

    for (int32 k=0;k<2;k++)
    {
      EE[k  ] = EEE[  k] + EEO[  k];
      EE[k+2] = EEE[1-k] - EEO[1-k];
    }   

    for (int32 k=0;k<4;k++)
    {
      E[k  ] = EE[  k] + EO[  k];
      E[k+4] = EE[3-k] - EO[3-k];
    }

    for (int32 k=0;k<8;k++)
    {
      Dst[k  ] = xClipS16((E[  k] + O[  k] + Add2nd)>>Shift2nd);
      Dst[k+8] = xClipS16((E[7-k] - O[7-k] + Add2nd)>>Shift2nd);
    }
    Dst += DstStride;
  }
}
void xTransform::InvTransformDCT_16x16_SSE(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth) //transpose -> vertical transform -> transpose -> horizontal transform
{
  static const __m128i xiT16_DCT[32] = {
  _mm_setr_epi16( 64, 90, 89, 87, 83, 80, 75, 70), _mm_setr_epi16( 64, 57, 50, 43, 36, 25, 18,  9),
  _mm_setr_epi16( 64, 87, 75, 57, 36,  9,-18,-43), _mm_setr_epi16(-64,-80,-89,-90,-83,-70,-50,-25),
  _mm_setr_epi16( 64, 80, 50,  9,-36,-70,-89,-87), _mm_setr_epi16(-64,-25, 18, 57, 83, 90, 75, 43),
  _mm_setr_epi16( 64, 70, 18,-43,-83,-87,-50,  9), _mm_setr_epi16( 64, 90, 75, 25,-36,-80,-89,-57),
  _mm_setr_epi16( 64, 57,-18,-80,-83,-25, 50, 90), _mm_setr_epi16( 64, -9,-75,-87,-36, 43, 89, 70),
  _mm_setr_epi16( 64, 43,-50,-90,-36, 57, 89, 25), _mm_setr_epi16(-64,-87,-18, 70, 83,  9,-75,-80),
  _mm_setr_epi16( 64, 25,-75,-70, 36, 90, 18,-80), _mm_setr_epi16(-64, 43, 89,  9,-83,-57, 50, 87),
  _mm_setr_epi16( 64,  9,-89,-25, 83, 43,-75,-57), _mm_setr_epi16( 64, 70,-50,-80, 36, 87,-18,-90),
  _mm_setr_epi16( 64, -9,-89, 25, 83,-43,-75, 57), _mm_setr_epi16( 64,-70,-50, 80, 36,-87,-18, 90),
  _mm_setr_epi16( 64,-25,-75, 70, 36,-90, 18, 80), _mm_setr_epi16(-64,-43, 89, -9,-83, 57, 50,-87),
  _mm_setr_epi16( 64,-43,-50, 90,-36,-57, 89,-25), _mm_setr_epi16(-64, 87,-18,-70, 83, -9,-75, 80),
  _mm_setr_epi16( 64,-57,-18, 80,-83, 25, 50,-90), _mm_setr_epi16( 64,  9,-75, 87,-36,-43, 89,-70),
  _mm_setr_epi16( 64,-70, 18, 43,-83, 87,-50, -9), _mm_setr_epi16( 64,-90, 75,-25,-36, 80,-89, 57),
  _mm_setr_epi16( 64,-80, 50, -9,-36, 70,-89, 87), _mm_setr_epi16(-64, 25, 18,-57, 83,-90, 75,-43),
  _mm_setr_epi16( 64,-87, 75,-57, 36, -9,-18, 43), _mm_setr_epi16(-64, 80,-89, 90,-83, 70,-50, 25),
  _mm_setr_epi16( 64,-90, 89,-87, 83,-80, 75,-70), _mm_setr_epi16( 64,-57, 50,-43, 36,-25, 18, -9)};

  const int32 Shift2nd = 20 - BitDepth; 
  const __m128i Add1st  = _mm_set1_epi32(  64);
  const __m128i Add2nd  = _mm_set1_epi32(1<<(Shift2nd-1));
  __m128i tr[16];
  __m128i Tmp_A[32];
  __m128i Tmp_B[32];

  //load
  for(int32 j=0; j<32; j+=2)
  {
    Tmp_B[j  ] = _mm_loadu_si128((__m128i*)(Src  ));
    Tmp_B[j+1] = _mm_loadu_si128((__m128i*)(Src+8));
    Src += 16;   
  } 

  //transpose  
  for(int32 i=0; i<16; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[16+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[16+i]);
  }

  for(int32 i=0; i<16; i++)
  {
    Tmp_B[2*i  ] = _mm_unpacklo_epi16(Tmp_A[0+i], Tmp_A[16+i]);
    Tmp_B[2*i+1] = _mm_unpackhi_epi16(Tmp_A[0+i], Tmp_A[16+i]);
  }

  for(int32 i=0; i<16; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[16+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[16+i]);
  }

  for(int32 i=0; i<16; i++)
  {
    Tmp_B[2*i  ] = _mm_unpacklo_epi16(Tmp_A[0+i], Tmp_A[16+i]);
    Tmp_B[2*i+1] = _mm_unpackhi_epi16(Tmp_A[0+i], Tmp_A[16+i]);
  }
  
  //vertical transform
  for(int32 j=0; j<16; j++)
  {
    __m128i LineA = Tmp_B[2*j  ];
    __m128i LineB = Tmp_B[2*j+1];

    for(int32 i=0, k=0; i<32; i+=2, k++)
    {
      tr[k] = _mm_add_epi32(_mm_madd_epi16(LineA, xiT16_DCT[i  ]), _mm_madd_epi16(LineB, xiT16_DCT[i+1]));
    }

    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add1st), 7);

    Tmp_B[2*j  ] = _mm_packs_epi32(tr[0], tr[1]);
    Tmp_B[2*j+1] = _mm_packs_epi32(tr[2], tr[3]);
  }
  
  //transpose  
  for(int32 i=0; i<16; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[16+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[16+i]);
  }

  for(int32 i=0; i<16; i++)
  {
    Tmp_B[2*i  ] = _mm_unpacklo_epi16(Tmp_A[0+i], Tmp_A[16+i]);
    Tmp_B[2*i+1] = _mm_unpackhi_epi16(Tmp_A[0+i], Tmp_A[16+i]);
  }

  for(int32 i=0; i<16; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[16+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[16+i]);
  }

  for(int32 i=0; i<16; i++)
  {
    Tmp_B[2*i  ] = _mm_unpacklo_epi16(Tmp_A[0+i], Tmp_A[16+i]);
    Tmp_B[2*i+1] = _mm_unpackhi_epi16(Tmp_A[0+i], Tmp_A[16+i]);
  }

  //horizontal transform and store
  for(int32 j=0; j<16; j++)
  {
    __m128i LineA = Tmp_B[2*j  ];
    __m128i LineB = Tmp_B[2*j+1];   

    for(int32 i=0, k=0; i<32; i+=2, k++)
    {
      tr[k] = _mm_add_epi32(_mm_madd_epi16(LineA, xiT16_DCT[i  ]), _mm_madd_epi16(LineB, xiT16_DCT[i+1]));
    }

    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add2nd), Shift2nd);

    _mm_storeu_si128((__m128i*)(Dst ), _mm_packs_epi32(tr[0], tr[1]));
    _mm_storeu_si128((__m128i*)(Dst+8), _mm_packs_epi32(tr[2], tr[3]));
    Dst += 16; 
  }  
}
void xTransform::InvTransformDCT_16x16_SSEv2(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth) //vertical transform uses one symmetry, horizontal transform uses 0 symmetries
{
  static const __m128i xiT16_DCT[32] = {
  _mm_setr_epi16( 64, 90, 89, 87, 83, 80, 75, 70), _mm_setr_epi16( 64, 57, 50, 43, 36, 25, 18,  9),
  _mm_setr_epi16( 64, 87, 75, 57, 36,  9,-18,-43), _mm_setr_epi16(-64,-80,-89,-90,-83,-70,-50,-25),
  _mm_setr_epi16( 64, 80, 50,  9,-36,-70,-89,-87), _mm_setr_epi16(-64,-25, 18, 57, 83, 90, 75, 43),
  _mm_setr_epi16( 64, 70, 18,-43,-83,-87,-50,  9), _mm_setr_epi16( 64, 90, 75, 25,-36,-80,-89,-57),
  _mm_setr_epi16( 64, 57,-18,-80,-83,-25, 50, 90), _mm_setr_epi16( 64, -9,-75,-87,-36, 43, 89, 70),
  _mm_setr_epi16( 64, 43,-50,-90,-36, 57, 89, 25), _mm_setr_epi16(-64,-87,-18, 70, 83,  9,-75,-80),
  _mm_setr_epi16( 64, 25,-75,-70, 36, 90, 18,-80), _mm_setr_epi16(-64, 43, 89,  9,-83,-57, 50, 87),
  _mm_setr_epi16( 64,  9,-89,-25, 83, 43,-75,-57), _mm_setr_epi16( 64, 70,-50,-80, 36, 87,-18,-90),
  _mm_setr_epi16( 64, -9,-89, 25, 83,-43,-75, 57), _mm_setr_epi16( 64,-70,-50, 80, 36,-87,-18, 90),
  _mm_setr_epi16( 64,-25,-75, 70, 36,-90, 18, 80), _mm_setr_epi16(-64,-43, 89, -9,-83, 57, 50,-87),
  _mm_setr_epi16( 64,-43,-50, 90,-36,-57, 89,-25), _mm_setr_epi16(-64, 87,-18,-70, 83, -9,-75, 80),
  _mm_setr_epi16( 64,-57,-18, 80,-83, 25, 50,-90), _mm_setr_epi16( 64,  9,-75, 87,-36,-43, 89,-70),
  _mm_setr_epi16( 64,-70, 18, 43,-83, 87,-50, -9), _mm_setr_epi16( 64,-90, 75,-25,-36, 80,-89, 57),
  _mm_setr_epi16( 64,-80, 50, -9,-36, 70,-89, 87), _mm_setr_epi16(-64, 25, 18,-57, 83,-90, 75,-43),
  _mm_setr_epi16( 64,-87, 75,-57, 36, -9,-18, 43), _mm_setr_epi16(-64, 80,-89, 90,-83, 70,-50, 25),
  _mm_setr_epi16( 64,-90, 89,-87, 83,-80, 75,-70), _mm_setr_epi16( 64,-57, 50,-43, 36,-25, 18, -9)};

  const int32 Shift2nd = 20 - BitDepth; 
  const __m128i Add1st  = _mm_set1_epi32(  64);
  const __m128i Add2nd  = _mm_set1_epi32(1<<(Shift2nd-1));
  __m128i tr[16];
  __m128i Tmp[32];
  __m128i Tmp_B[32];

  //load
  for(int j=0; j<32; j+=2)
  {
    Tmp[j  ] = _mm_loadu_si128((__m128i*)(Src  ));
    Tmp[j+1] = _mm_loadu_si128((__m128i*)(Src+8));
    Src += 16;   
  } 
  
  //vertical transform
  for(int32 i=0; i<8; i++)
  { 
  __m128i SumEA = _mm_setzero_si128();
    __m128i SumEB = _mm_setzero_si128();
    __m128i SumOA = _mm_setzero_si128();
    __m128i SumOB = _mm_setzero_si128();
    
  __m128i Sum2EA = _mm_setzero_si128();
    __m128i Sum2EB = _mm_setzero_si128();
  __m128i Sum2OA = _mm_setzero_si128();
    __m128i Sum2OB = _mm_setzero_si128();
  
    for(int32 j=0; j<32; j+=4)
    {
      //Even
      {
      __m128i Line1 = Tmp[j  ];
    __m128i Line2 = Tmp[j+1];

      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2A = _mm_cvtepi16_epi32(Line2);
      __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));
    
      __m128i TX = _mm_set1_epi32(m_TrM16x16[j>>1][i]);
    Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);

    Line2A = _mm_mullo_epi32(Line2A, TX);
      Line2B = _mm_mullo_epi32(Line2B, TX);

      SumEA = _mm_add_epi32(SumEA, Line1A);
      SumEB = _mm_add_epi32(SumEB, Line1B); 

    Sum2EA = _mm_add_epi32(Sum2EA, Line2A);
      Sum2EB = _mm_add_epi32(Sum2EB, Line2B); 
      }
      //Odd
      {
      __m128i Line1 = Tmp[j+2];
    __m128i Line2 = Tmp[j+3];

      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));
    
    __m128i Line2A = _mm_cvtepi16_epi32(Line2);
      __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

      __m128i TX = _mm_set1_epi32(m_TrM16x16[(j>>1)+1][i]);

      Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);

    Line2A = _mm_mullo_epi32(Line2A, TX);
      Line2B = _mm_mullo_epi32(Line2B, TX);

      SumOA = _mm_add_epi32(SumOA, Line1A);
      SumOB = _mm_add_epi32(SumOB, Line1B); 

    Sum2OA = _mm_add_epi32(Sum2OA, Line2A);
      Sum2OB = _mm_add_epi32(Sum2OB, Line2B); 
      }
  }
  __m128i EpOA = _mm_add_epi32(SumEA, SumOA);
    __m128i EpOB = _mm_add_epi32(SumEB, SumOB);
    __m128i EmOA = _mm_sub_epi32(SumEA, SumOA);
    __m128i EmOB = _mm_sub_epi32(SumEB, SumOB);
  
  __m128i Ep2OA = _mm_add_epi32(Sum2EA, Sum2OA); 
    __m128i Ep2OB = _mm_add_epi32(Sum2EB, Sum2OB); 
    __m128i Em2OA = _mm_sub_epi32(Sum2EA, Sum2OA);
    __m128i Em2OB = _mm_sub_epi32(Sum2EB, Sum2OB);

    EpOA = _mm_srai_epi32(_mm_add_epi32(EpOA, Add1st),7);
    EpOB = _mm_srai_epi32(_mm_add_epi32(EpOB, Add1st),7);
    EmOA = _mm_srai_epi32(_mm_add_epi32(EmOA, Add1st),7);
    EmOB = _mm_srai_epi32(_mm_add_epi32(EmOB, Add1st),7);

  Ep2OA = _mm_srai_epi32(_mm_add_epi32(Ep2OA, Add1st),7);
    Ep2OB = _mm_srai_epi32(_mm_add_epi32(Ep2OB, Add1st),7);
    Em2OA = _mm_srai_epi32(_mm_add_epi32(Em2OA, Add1st),7);
    Em2OB = _mm_srai_epi32(_mm_add_epi32(Em2OB, Add1st),7);

    Tmp_B[2*i   ]= _mm_packs_epi32(EpOA, EpOB);
  Tmp_B[2*i+1 ] = _mm_packs_epi32(Ep2OA, Ep2OB);

    Tmp_B[30-2*i  ]= _mm_packs_epi32(EmOA, EmOB);  
  Tmp_B[30-2*i+1] = _mm_packs_epi32(Em2OA, Em2OB);  
   }

   //horizontal transform and store
  for(int32 j=0; j<16; j++)
  {
    __m128i LineA = Tmp_B[2*j];
    __m128i LineB = Tmp_B[2*j+1];

    for(int32 i=0, k=0; i<32; i+=2, k++)
    {
      tr[k] = _mm_add_epi32(_mm_madd_epi16(LineA, xiT16_DCT[i  ]), _mm_madd_epi16(LineB, xiT16_DCT[i+1]));
    }

    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add2nd), Shift2nd);
  
    __m128i tr1=  _mm_packs_epi32(tr[0], tr[1]);
  __m128i tr2= _mm_packs_epi32(tr[2], tr[3]);
  _mm_storeu_si128((__m128i*)Dst, tr1);
  _mm_storeu_si128((__m128i*)(Dst+8), tr2);
    Dst += DstStride; 
  }
}
void xTransform::InvTransformDCT_16x16_SSEv3(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth)//vertical transform uses 2 symmetries, horizontal tranform uses 0 symmetries
{
  
static const __m128i xiT16_DCT[32] = {
  _mm_setr_epi16( 64, 90, 89, 87, 83, 80, 75, 70), _mm_setr_epi16( 64, 57, 50, 43, 36, 25, 18,  9),
  _mm_setr_epi16( 64, 87, 75, 57, 36,  9,-18,-43), _mm_setr_epi16(-64,-80,-89,-90,-83,-70,-50,-25),
  _mm_setr_epi16( 64, 80, 50,  9,-36,-70,-89,-87), _mm_setr_epi16(-64,-25, 18, 57, 83, 90, 75, 43),
  _mm_setr_epi16( 64, 70, 18,-43,-83,-87,-50,  9), _mm_setr_epi16( 64, 90, 75, 25,-36,-80,-89,-57),
  _mm_setr_epi16( 64, 57,-18,-80,-83,-25, 50, 90), _mm_setr_epi16( 64, -9,-75,-87,-36, 43, 89, 70),
  _mm_setr_epi16( 64, 43,-50,-90,-36, 57, 89, 25), _mm_setr_epi16(-64,-87,-18, 70, 83,  9,-75,-80),
  _mm_setr_epi16( 64, 25,-75,-70, 36, 90, 18,-80), _mm_setr_epi16(-64, 43, 89,  9,-83,-57, 50, 87),
  _mm_setr_epi16( 64,  9,-89,-25, 83, 43,-75,-57), _mm_setr_epi16( 64, 70,-50,-80, 36, 87,-18,-90),
  _mm_setr_epi16( 64, -9,-89, 25, 83,-43,-75, 57), _mm_setr_epi16( 64,-70,-50, 80, 36,-87,-18, 90),
  _mm_setr_epi16( 64,-25,-75, 70, 36,-90, 18, 80), _mm_setr_epi16(-64,-43, 89, -9,-83, 57, 50,-87),
  _mm_setr_epi16( 64,-43,-50, 90,-36,-57, 89,-25), _mm_setr_epi16(-64, 87,-18,-70, 83, -9,-75, 80),
  _mm_setr_epi16( 64,-57,-18, 80,-83, 25, 50,-90), _mm_setr_epi16( 64,  9,-75, 87,-36,-43, 89,-70),
  _mm_setr_epi16( 64,-70, 18, 43,-83, 87,-50, -9), _mm_setr_epi16( 64,-90, 75,-25,-36, 80,-89, 57),
  _mm_setr_epi16( 64,-80, 50, -9,-36, 70,-89, 87), _mm_setr_epi16(-64, 25, 18,-57, 83,-90, 75,-43),
  _mm_setr_epi16( 64,-87, 75,-57, 36, -9,-18, 43), _mm_setr_epi16(-64, 80,-89, 90,-83, 70,-50, 25),
  _mm_setr_epi16( 64,-90, 89,-87, 83,-80, 75,-70), _mm_setr_epi16( 64,-57, 50,-43, 36,-25, 18, -9)};


  const int32 Shift2nd = 20 - BitDepth; 
  const __m128i Add1st  = _mm_set1_epi32(  64);
  const __m128i Add2nd  = _mm_set1_epi32(1<<(Shift2nd-1));

  __m128i Tmp[32];
  __m128i Tmp2[32];
  __m128i tr[16];
  //load
  for(int j=0; j<32; j+=2)
  {
    Tmp[j  ] = _mm_loadu_si128((__m128i*)(Src  ));
    Tmp[j+1] = _mm_loadu_si128((__m128i*)(Src+8));
    Src += 16;   
  } 
  
  //vertical transform  
  for(int32 i=0; i<4; i++) 
  { 
    __m128i SumEAe = _mm_setzero_si128();//even lines + +
    __m128i SumEBe = _mm_setzero_si128();
  __m128i Sum2EAe = _mm_setzero_si128();
    __m128i Sum2EBe = _mm_setzero_si128();

    __m128i SumOA = _mm_setzero_si128();//odd lines
    __m128i SumOB = _mm_setzero_si128();
  __m128i Sum2OA = _mm_setzero_si128();
    __m128i Sum2OB = _mm_setzero_si128();

  __m128i SumOAm = _mm_setzero_si128();
    __m128i SumOBm = _mm_setzero_si128();
  __m128i Sum2OAm = _mm_setzero_si128();
    __m128i Sum2OBm = _mm_setzero_si128();

  __m128i SumEAo = _mm_setzero_si128();//even lines + -
    __m128i SumEBo = _mm_setzero_si128();
  __m128i Sum2EAo = _mm_setzero_si128();
    __m128i Sum2EBo = _mm_setzero_si128();

    for(int32 j=0; j<32; j+=8)
    {
      //Even Even + +
      {
      __m128i Line1 = Tmp[j  ];
      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+1];
      __m128i Line2A = _mm_cvtepi16_epi32(Line2);
      __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

      __m128i TX = _mm_set1_epi32(m_TrM16x16[j>>1][i]);

      Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);

    Line2A = _mm_mullo_epi32(Line2A, TX);
      Line2B = _mm_mullo_epi32(Line2B, TX);

      SumEAe = _mm_add_epi32(SumEAe, Line1A);
      SumEBe = _mm_add_epi32(SumEBe, Line1B);  

    Sum2EAe = _mm_add_epi32(Sum2EAe, Line2A);
      Sum2EBe = _mm_add_epi32(Sum2EBe, Line2B);  
      }
      //Odd x y
      {
      __m128i Line1 = Tmp[j+2 ];
      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+3 ];
      __m128i Line2A = _mm_cvtepi16_epi32(Line2);
      __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));
      
    __m128i TX = _mm_set1_epi32(m_TrM16x16[(j>>1)+1][i]);
    __m128i TXm= _mm_set1_epi32(m_TrM16x16[(j>>1)+1][7-i]);

    __m128i Line1Am = _mm_mullo_epi32(Line1A, TXm);
      __m128i Line1Bm = _mm_mullo_epi32(Line1B, TXm);
      Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);

    __m128i Line2Am = _mm_mullo_epi32(Line2A, TXm);
      __m128i Line2Bm = _mm_mullo_epi32(Line2B, TXm);
      Line2A = _mm_mullo_epi32(Line2A, TX);
      Line2B = _mm_mullo_epi32(Line2B, TX);
    
      SumOA = _mm_add_epi32(SumOA, Line1A);
      SumOB = _mm_add_epi32(SumOB, Line1B); 

    SumOAm = _mm_add_epi32(SumOAm, Line1Am);
      SumOBm = _mm_add_epi32(SumOBm, Line1Bm); 
    
    Sum2OA = _mm_add_epi32(Sum2OA, Line2A);
      Sum2OB = _mm_add_epi32(Sum2OB, Line2B); 

    Sum2OAm = _mm_add_epi32(Sum2OAm, Line2Am);
      Sum2OBm = _mm_add_epi32(Sum2OBm, Line2Bm); 
      }
    //Even odd + -
    {
      __m128i Line1 = Tmp[j+4 ];
      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+5 ];
      __m128i Line2A = _mm_cvtepi16_epi32(Line2);
      __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

      __m128i TX = _mm_set1_epi32(m_TrM16x16[(j>>1)+2][i]);
      Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);
      SumEAo = _mm_add_epi32(SumEAo, Line1A);
      SumEBo = _mm_add_epi32(SumEBo, Line1B);  

    Line2A = _mm_mullo_epi32(Line2A, TX);
      Line2B = _mm_mullo_epi32(Line2B, TX);
      Sum2EAo = _mm_add_epi32(Sum2EAo, Line2A);
      Sum2EBo = _mm_add_epi32(Sum2EBo, Line2B);
    }
    //Odd x y
      {
      __m128i Line1 = Tmp[j+6 ];
      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+7 ];
      __m128i Line2A = _mm_cvtepi16_epi32(Line2);
      __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

      __m128i TX = _mm_set1_epi32(m_TrM16x16[(j>>1)+3][i]);
    __m128i TXm= _mm_set1_epi32(m_TrM16x16[(j>>1)+3][7-i]);
  
    __m128i Line1Am = _mm_mullo_epi32(Line1A, TXm);
      __m128i Line1Bm = _mm_mullo_epi32(Line1B, TXm);
      Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);
  
    __m128i Line2Am = _mm_mullo_epi32(Line2A, TXm);
      __m128i Line2Bm = _mm_mullo_epi32(Line2B, TXm);
      Line2A = _mm_mullo_epi32(Line2A, TX);
      Line2B = _mm_mullo_epi32(Line2B, TX);

      SumOA = _mm_add_epi32(SumOA, Line1A);
      SumOB = _mm_add_epi32(SumOB, Line1B); 

    SumOAm = _mm_add_epi32(SumOAm, Line1Am);
      SumOBm = _mm_add_epi32(SumOBm, Line1Bm); 

    Sum2OA = _mm_add_epi32(Sum2OA, Line2A);
      Sum2OB = _mm_add_epi32(Sum2OB, Line2B); 

    Sum2OAm = _mm_add_epi32(Sum2OAm, Line2Am);
      Sum2OBm = _mm_add_epi32(Sum2OBm, Line2Bm); 
      }
    }
  
  __m128i SumEAeo=_mm_add_epi32(SumEAe,SumEAo);
  __m128i SumEBeo=_mm_add_epi32(SumEBe,SumEBo);
  __m128i Sum2EAeo=_mm_add_epi32(Sum2EAe,Sum2EAo);
  __m128i Sum2EBeo=_mm_add_epi32(Sum2EBe,Sum2EBo);

  __m128i DifEAeo=_mm_sub_epi32(SumEAe,SumEAo);
  __m128i DifEBeo=_mm_sub_epi32(SumEBe,SumEBo);
  __m128i Dif2EAeo=_mm_sub_epi32(Sum2EAe,Sum2EAo);
  __m128i Dif2EBeo=_mm_sub_epi32(Sum2EBe,Sum2EBo);

  //line0
    __m128i EpOA = _mm_add_epi32(SumEAeo,SumOA);
    __m128i EpOB = _mm_add_epi32(SumEBeo,SumOB);
  __m128i Ep2OA = _mm_add_epi32(Sum2EAeo,Sum2OA);
    __m128i Ep2OB = _mm_add_epi32(Sum2EBeo,Sum2OB);
   
  //line3
  __m128i ElOA = _mm_add_epi32(DifEAeo,SumOAm);
  __m128i ElOB = _mm_add_epi32(DifEBeo,SumOBm);
  __m128i El2OA = _mm_add_epi32(Dif2EAeo,Sum2OAm);
  __m128i El2OB = _mm_add_epi32(Dif2EBeo,Sum2OBm);
  //line4
  __m128i EkOA = _mm_sub_epi32(DifEAeo,SumOAm);
  __m128i EkOB = _mm_sub_epi32(DifEBeo,SumOBm);
  __m128i Ek2OA = _mm_sub_epi32(Dif2EAeo,Sum2OAm);
  __m128i Ek2OB = _mm_sub_epi32(Dif2EBeo,Sum2OBm);
   //line7
  __m128i EmOA = _mm_sub_epi32(SumEAeo, SumOA);
    __m128i EmOB = _mm_sub_epi32(SumEBeo, SumOB);
  __m128i Em2OA = _mm_sub_epi32(Sum2EAeo, Sum2OA);
    __m128i Em2OB = _mm_sub_epi32(Sum2EBeo, Sum2OB);

    EpOA = _mm_srai_epi32(_mm_add_epi32(EpOA, Add1st),7);
    EpOB = _mm_srai_epi32(_mm_add_epi32(EpOB, Add1st),7);
  Ep2OA = _mm_srai_epi32(_mm_add_epi32(Ep2OA, Add1st),7);
    Ep2OB = _mm_srai_epi32(_mm_add_epi32(Ep2OB, Add1st),7);
  
  EkOA = _mm_srai_epi32(_mm_add_epi32(EkOA, Add1st),7);
    EkOB = _mm_srai_epi32(_mm_add_epi32(EkOB, Add1st),7);
  Ek2OA = _mm_srai_epi32(_mm_add_epi32(Ek2OA, Add1st),7);
    Ek2OB = _mm_srai_epi32(_mm_add_epi32(Ek2OB, Add1st),7);

  ElOA = _mm_srai_epi32(_mm_add_epi32(ElOA, Add1st),7);
    ElOB = _mm_srai_epi32(_mm_add_epi32(ElOB, Add1st),7);
  El2OA = _mm_srai_epi32(_mm_add_epi32(El2OA, Add1st),7);
    El2OB = _mm_srai_epi32(_mm_add_epi32(El2OB, Add1st),7);

    EmOA = _mm_srai_epi32(_mm_add_epi32(EmOA, Add1st),7);
    EmOB = _mm_srai_epi32(_mm_add_epi32(EmOB, Add1st),7);
  Em2OA = _mm_srai_epi32(_mm_add_epi32(Em2OA, Add1st),7);
    Em2OB = _mm_srai_epi32(_mm_add_epi32(Em2OB, Add1st),7);
  
    Tmp2[2*i  ] = _mm_packs_epi32(EpOA, EpOB);
  Tmp2[2*i+1] = _mm_packs_epi32(Ep2OA, Ep2OB);

  Tmp2[14-2*i] = _mm_packs_epi32(ElOA,ElOB);
  Tmp2[14-2*i+1] = _mm_packs_epi32(El2OA,El2OB);

  Tmp2[16+2*i] = _mm_packs_epi32(EkOA,EkOB);
  Tmp2[16+2*i+1] = _mm_packs_epi32(Ek2OA,Ek2OB);

    Tmp2[30-2*i] = _mm_packs_epi32(EmOA, EmOB);     
  Tmp2[30-2*i+1] = _mm_packs_epi32(Em2OA, Em2OB); 
  } 

  //horizontal transform and store
  for(int32 j=0; j<16; j++)
  {
    __m128i LineA = Tmp2[2*j  ];
    __m128i LineB = Tmp2[2*j+1];   

    for(int32 i=0, k=0; i<32; i+=2, k++)
    {
      tr[k] = _mm_add_epi32(_mm_madd_epi16(LineA, xiT16_DCT[i  ]), _mm_madd_epi16(LineB, xiT16_DCT[i+1]));
    }

    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add2nd), Shift2nd);

    __m128i tr1=  _mm_packs_epi32(tr[0], tr[1]);
  __m128i tr2= _mm_packs_epi32(tr[2], tr[3]);
  _mm_storeu_si128((__m128i*)Dst, tr1);
  _mm_storeu_si128((__m128i*)(Dst+8), tr2);
    Dst += DstStride; 
  }  
}
void xTransform::InvTransformDCT_16x16_SSEv4(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth)//vertical transform uses 3 symmetries, horizontal transform uses 0 symmetries
{
  
static const __m128i xiT16_DCT[32] = {
  _mm_setr_epi16( 64, 90, 89, 87, 83, 80, 75, 70), _mm_setr_epi16( 64, 57, 50, 43, 36, 25, 18,  9),
  _mm_setr_epi16( 64, 87, 75, 57, 36,  9,-18,-43), _mm_setr_epi16(-64,-80,-89,-90,-83,-70,-50,-25),
  _mm_setr_epi16( 64, 80, 50,  9,-36,-70,-89,-87), _mm_setr_epi16(-64,-25, 18, 57, 83, 90, 75, 43),
  _mm_setr_epi16( 64, 70, 18,-43,-83,-87,-50,  9), _mm_setr_epi16( 64, 90, 75, 25,-36,-80,-89,-57),
  _mm_setr_epi16( 64, 57,-18,-80,-83,-25, 50, 90), _mm_setr_epi16( 64, -9,-75,-87,-36, 43, 89, 70),
  _mm_setr_epi16( 64, 43,-50,-90,-36, 57, 89, 25), _mm_setr_epi16(-64,-87,-18, 70, 83,  9,-75,-80),
  _mm_setr_epi16( 64, 25,-75,-70, 36, 90, 18,-80), _mm_setr_epi16(-64, 43, 89,  9,-83,-57, 50, 87),
  _mm_setr_epi16( 64,  9,-89,-25, 83, 43,-75,-57), _mm_setr_epi16( 64, 70,-50,-80, 36, 87,-18,-90),
  _mm_setr_epi16( 64, -9,-89, 25, 83,-43,-75, 57), _mm_setr_epi16( 64,-70,-50, 80, 36,-87,-18, 90),
  _mm_setr_epi16( 64,-25,-75, 70, 36,-90, 18, 80), _mm_setr_epi16(-64,-43, 89, -9,-83, 57, 50,-87),
  _mm_setr_epi16( 64,-43,-50, 90,-36,-57, 89,-25), _mm_setr_epi16(-64, 87,-18,-70, 83, -9,-75, 80),
  _mm_setr_epi16( 64,-57,-18, 80,-83, 25, 50,-90), _mm_setr_epi16( 64,  9,-75, 87,-36,-43, 89,-70),
  _mm_setr_epi16( 64,-70, 18, 43,-83, 87,-50, -9), _mm_setr_epi16( 64,-90, 75,-25,-36, 80,-89, 57),
  _mm_setr_epi16( 64,-80, 50, -9,-36, 70,-89, 87), _mm_setr_epi16(-64, 25, 18,-57, 83,-90, 75,-43),
  _mm_setr_epi16( 64,-87, 75,-57, 36, -9,-18, 43), _mm_setr_epi16(-64, 80,-89, 90,-83, 70,-50, 25),
  _mm_setr_epi16( 64,-90, 89,-87, 83,-80, 75,-70), _mm_setr_epi16( 64,-57, 50,-43, 36,-25, 18, -9)};


  const int32 Shift2nd = 20 - BitDepth; 
  const __m128i Add1st  = _mm_set1_epi32(  64);
  const __m128i Add2nd  = _mm_set1_epi32(1<<(Shift2nd-1));

  __m128i Tmp[32];
  __m128i Tmp2[32];
  __m128i tr[16];
  //load
  for(int j=0; j<32; j+=2)
  {
    Tmp[j  ] = _mm_loadu_si128((__m128i*)(Src  ));
    Tmp[j+1] = _mm_loadu_si128((__m128i*)(Src+8));
    Src += 16;   
  } 
  
  //vertical transform  
  for(int32 i=0; i<2; i++) 
  {   
    // |1A 1B 2A 2B| - 16 samples

    __m128i Sum1QA= _mm_setzero_si128();
    __m128i Sum1QB= _mm_setzero_si128();
    __m128i Sum2QA= _mm_setzero_si128();
    __m128i Sum2QB= _mm_setzero_si128();

    __m128i Sum1WA= _mm_setzero_si128();
    __m128i Sum1WB= _mm_setzero_si128();
    __m128i Sum2WA= _mm_setzero_si128();
    __m128i Sum2WB= _mm_setzero_si128();

    __m128i Sum1EA= _mm_setzero_si128();
    __m128i Sum1EB= _mm_setzero_si128();
    __m128i Sum2EA= _mm_setzero_si128();
    __m128i Sum2EB= _mm_setzero_si128();

    __m128i Sum1RA= _mm_setzero_si128();
    __m128i Sum1RB= _mm_setzero_si128();
    __m128i Sum2RA= _mm_setzero_si128();
    __m128i Sum2RB= _mm_setzero_si128();

    __m128i Sum1TA= _mm_setzero_si128();
    __m128i Sum1TB= _mm_setzero_si128();
    __m128i Sum2TA= _mm_setzero_si128();
    __m128i Sum2TB= _mm_setzero_si128();

    __m128i Sum1YA= _mm_setzero_si128();
    __m128i Sum1YB= _mm_setzero_si128();
    __m128i Sum2YA= _mm_setzero_si128();
    __m128i Sum2YB= _mm_setzero_si128();

    __m128i Sum1UA= _mm_setzero_si128();
    __m128i Sum1UB= _mm_setzero_si128();
    __m128i Sum2UA= _mm_setzero_si128();
    __m128i Sum2UB= _mm_setzero_si128();

    __m128i Sum1IA= _mm_setzero_si128();
    __m128i Sum1IB= _mm_setzero_si128();
    __m128i Sum2IA= _mm_setzero_si128();
    __m128i Sum2IB= _mm_setzero_si128();

    __m128i Sum1OA= _mm_setzero_si128();
    __m128i Sum1OB= _mm_setzero_si128();
    __m128i Sum2OA= _mm_setzero_si128();
    __m128i Sum2OB= _mm_setzero_si128();

    
    for(int32 j=0; j<32; j+=16)
    {
    //line0
    {
    __m128i Line1=Tmp[j+0];
    __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+1];
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i TX = _mm_set1_epi32(m_TrM16x16[j>>1][i]);

    Line1A = _mm_mullo_epi32(Line1A, TX);
        Line1B = _mm_mullo_epi32(Line1B, TX);

      Line2A = _mm_mullo_epi32(Line2A, TX);
        Line2B = _mm_mullo_epi32(Line2B, TX);

    Sum1QA=_mm_add_epi32(Sum1QA,Line1A);
    Sum1QB=_mm_add_epi32(Sum1QB,Line1B);

    Sum2QA=_mm_add_epi32(Sum2QA,Line2A);
    Sum2QB=_mm_add_epi32(Sum2QB,Line2B);

    }
    //line1
    {
    __m128i Line1=Tmp[j+2];
    __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+3];
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i TX = _mm_set1_epi32(m_TrM16x16[(j>>1)+1][i]);
    __m128i TXR = _mm_set1_epi32(m_TrM16x16[(j>>1)+1][7-i]);
    __m128i TXU = _mm_set1_epi32(m_TrM16x16[(j>>1)+1][3-i]);
    __m128i TXY = _mm_set1_epi32(m_TrM16x16[(j>>1)+1][4+i]);

    __m128i Line1AR = _mm_mullo_epi32(Line1A, TXR);
        __m128i Line1BR = _mm_mullo_epi32(Line1B, TXR);
    __m128i Line2AR = _mm_mullo_epi32(Line2A, TXR);
        __m128i Line2BR = _mm_mullo_epi32(Line2B, TXR);

    __m128i Line1AU = _mm_mullo_epi32(Line1A, TXU);
        __m128i Line1BU = _mm_mullo_epi32(Line1B, TXU);
    __m128i Line2AU = _mm_mullo_epi32(Line2A, TXU);
        __m128i Line2BU = _mm_mullo_epi32(Line2B, TXU);

    __m128i Line1AY = _mm_mullo_epi32(Line1A, TXY);
        __m128i Line1BY = _mm_mullo_epi32(Line1B, TXY);
    __m128i Line2AY = _mm_mullo_epi32(Line2A, TXY);
        __m128i Line2BY = _mm_mullo_epi32(Line2B, TXY);

    Line1A = _mm_mullo_epi32(Line1A, TX);
        Line1B = _mm_mullo_epi32(Line1B, TX);
      Line2A = _mm_mullo_epi32(Line2A, TX);
        Line2B = _mm_mullo_epi32(Line2B, TX);

    Sum1OA=_mm_add_epi32(Sum1OA,Line1A);
    Sum1OB=_mm_add_epi32(Sum1OB,Line1B);
    Sum2OA=_mm_add_epi32(Sum2OA,Line2A);
    Sum2OB=_mm_add_epi32(Sum2OB,Line2B);

    Sum1RA=_mm_add_epi32(Sum1RA,Line1AR);
    Sum1RB=_mm_add_epi32(Sum1RB,Line1BR);
    Sum2RA=_mm_add_epi32(Sum2RA,Line2AR);
    Sum2RB=_mm_add_epi32(Sum2RB,Line2BR);

    Sum1UA=_mm_add_epi32(Sum1UA,Line1AU);
    Sum1UB=_mm_add_epi32(Sum1UB,Line1BU);
    Sum2UA=_mm_add_epi32(Sum2UA,Line2AU);
    Sum2UB=_mm_add_epi32(Sum2UB,Line2BU);

    Sum1YA=_mm_add_epi32(Sum1YA,Line1AY);
    Sum1YB=_mm_add_epi32(Sum1YB,Line1BY);
    Sum2YA=_mm_add_epi32(Sum2YA,Line2AY);
    Sum2YB=_mm_add_epi32(Sum2YB,Line2BY);

    }
    //line2
    {
    __m128i Line1=Tmp[j+4];
    __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+5];
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i TX = _mm_set1_epi32(m_TrM16x16[(j>>1)+2][i]);
    __m128i TXI = _mm_set1_epi32(m_TrM16x16[(j>>1)+2][3-i]);

    __m128i Line1IA = _mm_mullo_epi32(Line1A, TXI);
        __m128i Line1IB = _mm_mullo_epi32(Line1B, TXI);
      __m128i Line2IA = _mm_mullo_epi32(Line2A, TXI);
        __m128i Line2IB = _mm_mullo_epi32(Line2B, TXI);

    Line1A = _mm_mullo_epi32(Line1A, TX);
        Line1B = _mm_mullo_epi32(Line1B, TX);
      Line2A = _mm_mullo_epi32(Line2A, TX);
        Line2B = _mm_mullo_epi32(Line2B, TX);

    Sum1EA=_mm_add_epi32(Sum1EA,Line1A);
    Sum1EB=_mm_add_epi32(Sum1EB,Line1B);
    Sum2EA=_mm_add_epi32(Sum2EA,Line2A);
    Sum2EB=_mm_add_epi32(Sum2EB,Line2B);

    Sum1IA=_mm_add_epi32(Sum1IA,Line1IA);
    Sum1IB=_mm_add_epi32(Sum1IB,Line1IB);
    Sum2IA=_mm_add_epi32(Sum2IA,Line2IA);
    Sum2IB=_mm_add_epi32(Sum2IB,Line2IB);

    }
    //line3
    {
    __m128i Line1=Tmp[j+6];
    __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+7];
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i TX = _mm_set1_epi32(m_TrM16x16[(j>>1)+3][i]);
    __m128i TXT = _mm_set1_epi32(m_TrM16x16[(j>>1)+3][7-i]);
    __m128i TXU = _mm_set1_epi32(m_TrM16x16[(j>>1)+3][3-i]);
    __m128i TXY = _mm_set1_epi32(m_TrM16x16[(j>>1)+3][4+i]);

    __m128i Line1TA = _mm_mullo_epi32(Line1A, TXT);
        __m128i Line1TB = _mm_mullo_epi32(Line1B, TXT);
      __m128i Line2TA = _mm_mullo_epi32(Line2A, TXT);
        __m128i Line2TB = _mm_mullo_epi32(Line2B, TXT);

    __m128i Line1AU = _mm_mullo_epi32(Line1A, TXU);
        __m128i Line1BU = _mm_mullo_epi32(Line1B, TXU);
    __m128i Line2AU = _mm_mullo_epi32(Line2A, TXU);
        __m128i Line2BU = _mm_mullo_epi32(Line2B, TXU);

    __m128i Line1AY = _mm_mullo_epi32(Line1A, TXY);
        __m128i Line1BY = _mm_mullo_epi32(Line1B, TXY);
    __m128i Line2AY = _mm_mullo_epi32(Line2A, TXY);
        __m128i Line2BY = _mm_mullo_epi32(Line2B, TXY);

    Line1A = _mm_mullo_epi32(Line1A, TX);
        Line1B = _mm_mullo_epi32(Line1B, TX);
      Line2A = _mm_mullo_epi32(Line2A, TX);
        Line2B = _mm_mullo_epi32(Line2B, TX);

    Sum1OA=_mm_add_epi32(Sum1OA,Line1A);
    Sum1OB=_mm_add_epi32(Sum1OB,Line1B);
    Sum2OA=_mm_add_epi32(Sum2OA,Line2A);
    Sum2OB=_mm_add_epi32(Sum2OB,Line2B);

    Sum1TA=_mm_add_epi32(Sum1TA,Line1TA);
    Sum1TB=_mm_add_epi32(Sum1TB,Line1TB);
    Sum2TA=_mm_add_epi32(Sum2TA,Line2TA);
    Sum2TB=_mm_add_epi32(Sum2TB,Line2TB);

    Sum1UA=_mm_add_epi32(Sum1UA,Line1AU);
    Sum1UB=_mm_add_epi32(Sum1UB,Line1BU);
    Sum2UA=_mm_add_epi32(Sum2UA,Line2AU);
    Sum2UB=_mm_add_epi32(Sum2UB,Line2BU);

    Sum1YA=_mm_add_epi32(Sum1YA,Line1AY);
    Sum1YB=_mm_add_epi32(Sum1YB,Line1BY);
    Sum2YA=_mm_add_epi32(Sum2YA,Line2AY);
    Sum2YB=_mm_add_epi32(Sum2YB,Line2BY);
    }
    //line4
    {
    __m128i Line1=Tmp[j+8];
    __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+9];
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i TX = _mm_set1_epi32(m_TrM16x16[(j>>1)+4][i]);

    Line1A = _mm_mullo_epi32(Line1A, TX);
        Line1B = _mm_mullo_epi32(Line1B, TX);
      Line2A = _mm_mullo_epi32(Line2A, TX);
        Line2B = _mm_mullo_epi32(Line2B, TX);

    Sum1WA=_mm_add_epi32(Sum1WA,Line1A);
    Sum1WB=_mm_add_epi32(Sum1WB,Line1B);
    Sum2WA=_mm_add_epi32(Sum2WA,Line2A);
    Sum2WB=_mm_add_epi32(Sum2WB,Line2B);
    }
    //line5
    {
      __m128i Line1=Tmp[j+10];
    __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+11];
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i TX = _mm_set1_epi32(m_TrM16x16[(j>>1)+5][i]);
    __m128i TXR = _mm_set1_epi32(m_TrM16x16[(j>>1)+5][7-i]);
    __m128i TXU = _mm_set1_epi32(m_TrM16x16[(j>>1)+5][3-i]);
    __m128i TXY = _mm_set1_epi32(m_TrM16x16[(j>>1)+5][4+i]);

    __m128i Line1AR = _mm_mullo_epi32(Line1A, TXR);
        __m128i Line1BR = _mm_mullo_epi32(Line1B, TXR);
    __m128i Line2AR = _mm_mullo_epi32(Line2A, TXR);
        __m128i Line2BR = _mm_mullo_epi32(Line2B, TXR);

    __m128i Line1AU = _mm_mullo_epi32(Line1A, TXU);
        __m128i Line1BU = _mm_mullo_epi32(Line1B, TXU);
    __m128i Line2AU = _mm_mullo_epi32(Line2A, TXU);
        __m128i Line2BU = _mm_mullo_epi32(Line2B, TXU);

    __m128i Line1AY = _mm_mullo_epi32(Line1A, TXY);
        __m128i Line1BY = _mm_mullo_epi32(Line1B, TXY);
    __m128i Line2AY = _mm_mullo_epi32(Line2A, TXY);
        __m128i Line2BY = _mm_mullo_epi32(Line2B, TXY);

    Line1A = _mm_mullo_epi32(Line1A, TX);
        Line1B = _mm_mullo_epi32(Line1B, TX);
      Line2A = _mm_mullo_epi32(Line2A, TX);
        Line2B = _mm_mullo_epi32(Line2B, TX);

    Sum1OA=_mm_add_epi32(Sum1OA,Line1A);
    Sum1OB=_mm_add_epi32(Sum1OB,Line1B);
    Sum2OA=_mm_add_epi32(Sum2OA,Line2A);
    Sum2OB=_mm_add_epi32(Sum2OB,Line2B);

    Sum1RA=_mm_add_epi32(Sum1RA,Line1AR);
    Sum1RB=_mm_add_epi32(Sum1RB,Line1BR);
    Sum2RA=_mm_add_epi32(Sum2RA,Line2AR);
    Sum2RB=_mm_add_epi32(Sum2RB,Line2BR);

    Sum1UA=_mm_add_epi32(Sum1UA,Line1AU);
    Sum1UB=_mm_add_epi32(Sum1UB,Line1BU);
    Sum2UA=_mm_add_epi32(Sum2UA,Line2AU);
    Sum2UB=_mm_add_epi32(Sum2UB,Line2BU);

    Sum1YA=_mm_add_epi32(Sum1YA,Line1AY);
    Sum1YB=_mm_add_epi32(Sum1YB,Line1BY);
    Sum2YA=_mm_add_epi32(Sum2YA,Line2AY);
    Sum2YB=_mm_add_epi32(Sum2YB,Line2BY);
    }
    //line6
    {
    __m128i Line1=Tmp[j+12];
    __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+13];
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i TX = _mm_set1_epi32(m_TrM16x16[(j>>1)+6][i]);
    __m128i TXI = _mm_set1_epi32(m_TrM16x16[(j>>1)+6][3-i]);

    __m128i Line1IA = _mm_mullo_epi32(Line1A, TXI);
        __m128i Line1IB = _mm_mullo_epi32(Line1B, TXI);
      __m128i Line2IA = _mm_mullo_epi32(Line2A, TXI);
        __m128i Line2IB = _mm_mullo_epi32(Line2B, TXI);

    Line1A = _mm_mullo_epi32(Line1A, TX);
        Line1B = _mm_mullo_epi32(Line1B, TX);
      Line2A = _mm_mullo_epi32(Line2A, TX);
        Line2B = _mm_mullo_epi32(Line2B, TX);

    Sum1EA=_mm_add_epi32(Sum1EA,Line1A);
    Sum1EB=_mm_add_epi32(Sum1EB,Line1B);
    Sum2EA=_mm_add_epi32(Sum2EA,Line2A);
    Sum2EB=_mm_add_epi32(Sum2EB,Line2B);

    Sum1IA=_mm_add_epi32(Sum1IA,Line1IA);
    Sum1IB=_mm_add_epi32(Sum1IB,Line1IB);
    Sum2IA=_mm_add_epi32(Sum2IA,Line2IA);
    Sum2IB=_mm_add_epi32(Sum2IB,Line2IB);
    }
    //line7
    {
    __m128i Line1=Tmp[j+14];
    __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+15];
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i TX = _mm_set1_epi32(m_TrM16x16[(j>>1)+7][i]);
    __m128i TXT = _mm_set1_epi32(m_TrM16x16[(j>>1)+7][7-i]);
    __m128i TXU = _mm_set1_epi32(m_TrM16x16[(j>>1)+7][3-i]);
    __m128i TXY = _mm_set1_epi32(m_TrM16x16[(j>>1)+7][4+i]);

    __m128i Line1TA = _mm_mullo_epi32(Line1A, TXT);
        __m128i Line1TB = _mm_mullo_epi32(Line1B, TXT);
      __m128i Line2TA = _mm_mullo_epi32(Line2A, TXT);
        __m128i Line2TB = _mm_mullo_epi32(Line2B, TXT);

    __m128i Line1AU = _mm_mullo_epi32(Line1A, TXU);
        __m128i Line1BU = _mm_mullo_epi32(Line1B, TXU);
    __m128i Line2AU = _mm_mullo_epi32(Line2A, TXU);
        __m128i Line2BU = _mm_mullo_epi32(Line2B, TXU);

    __m128i Line1AY = _mm_mullo_epi32(Line1A, TXY);
        __m128i Line1BY = _mm_mullo_epi32(Line1B, TXY);
    __m128i Line2AY = _mm_mullo_epi32(Line2A, TXY);
        __m128i Line2BY = _mm_mullo_epi32(Line2B, TXY);

    Line1A = _mm_mullo_epi32(Line1A, TX);
        Line1B = _mm_mullo_epi32(Line1B, TX);
      Line2A = _mm_mullo_epi32(Line2A, TX);
        Line2B = _mm_mullo_epi32(Line2B, TX);

    Sum1OA=_mm_add_epi32(Sum1OA,Line1A);
    Sum1OB=_mm_add_epi32(Sum1OB,Line1B);
    Sum2OA=_mm_add_epi32(Sum2OA,Line2A);
    Sum2OB=_mm_add_epi32(Sum2OB,Line2B);

    Sum1TA=_mm_add_epi32(Sum1TA,Line1TA);
    Sum1TB=_mm_add_epi32(Sum1TB,Line1TB);
    Sum2TA=_mm_add_epi32(Sum2TA,Line2TA);
    Sum2TB=_mm_add_epi32(Sum2TB,Line2TB);

    Sum1UA=_mm_add_epi32(Sum1UA,Line1AU);
    Sum1UB=_mm_add_epi32(Sum1UB,Line1BU);
    Sum2UA=_mm_add_epi32(Sum2UA,Line2AU);
    Sum2UB=_mm_add_epi32(Sum2UB,Line2BU);

    Sum1YA=_mm_add_epi32(Sum1YA,Line1AY);
    Sum1YB=_mm_add_epi32(Sum1YB,Line1BY);
    Sum2YA=_mm_add_epi32(Sum2YA,Line2AY);
    Sum2YB=_mm_add_epi32(Sum2YB,Line2BY);
    }
    }
  __m128i Sum1QWA= _mm_add_epi32(Sum1QA,Sum1WA);
  __m128i Sum1QWB= _mm_add_epi32(Sum1QB,Sum1WB);
  __m128i Sum2QWA= _mm_add_epi32(Sum2QA,Sum2WA);
  __m128i Sum2QWB= _mm_add_epi32(Sum2QB,Sum2WB);

  __m128i Dif1QWA= _mm_sub_epi32(Sum1QA,Sum1WA);
  __m128i Dif1QWB= _mm_sub_epi32(Sum1QB,Sum1WB);
  __m128i Dif2QWA= _mm_sub_epi32(Sum2QA,Sum2WA);
  __m128i Dif2QWB= _mm_sub_epi32(Sum2QB,Sum2WB);
  
  //line 0
  __m128i Line0_1A= _mm_add_epi32(_mm_add_epi32(Sum1QWA,Sum1EA),Sum1OA);
  __m128i Line0_1B= _mm_add_epi32(_mm_add_epi32(Sum1QWB,Sum1EB),Sum1OB);
  __m128i Line0_2A= _mm_add_epi32(_mm_add_epi32(Sum2QWA,Sum2EA),Sum2OA);
  __m128i Line0_2B= _mm_add_epi32(_mm_add_epi32(Sum2QWB,Sum2EB),Sum2OB);

  //line 3
  __m128i Line3_1A= _mm_add_epi32(_mm_add_epi32(Dif1QWA,Sum1IA),Sum1UA);
  __m128i Line3_1B= _mm_add_epi32(_mm_add_epi32(Dif1QWB,Sum1IB),Sum1UB);
  __m128i Line3_2A= _mm_add_epi32(_mm_add_epi32(Dif2QWA,Sum2IA),Sum2UA);
  __m128i Line3_2B= _mm_add_epi32(_mm_add_epi32(Dif2QWB,Sum2IB),Sum2UB);

  //line 4
  __m128i Line4_1A= _mm_add_epi32(_mm_sub_epi32(Dif1QWA,Sum1IA),Sum1YA);
  __m128i Line4_1B= _mm_add_epi32(_mm_sub_epi32(Dif1QWB,Sum1IB),Sum1YB);
  __m128i Line4_2A= _mm_add_epi32(_mm_sub_epi32(Dif2QWA,Sum2IA),Sum2YA);
  __m128i Line4_2B= _mm_add_epi32(_mm_sub_epi32(Dif2QWB,Sum2IB),Sum2YB);

  //line 7
  __m128i Line7_1A= _mm_add_epi32(_mm_add_epi32(_mm_sub_epi32(Sum1QWA,Sum1EA),Sum1RA),Sum1TA);
  __m128i Line7_1B= _mm_add_epi32(_mm_add_epi32(_mm_sub_epi32(Sum1QWB,Sum1EB),Sum1RB),Sum1TB);
  __m128i Line7_2A= _mm_add_epi32(_mm_add_epi32(_mm_sub_epi32(Sum2QWA,Sum2EA),Sum2RA),Sum2TA);
  __m128i Line7_2B= _mm_add_epi32(_mm_add_epi32(_mm_sub_epi32(Sum2QWB,Sum2EB),Sum2RB),Sum2TB);
//-------------------------------------------------------------------------
  
  //line 8
  __m128i Line8_1A= _mm_sub_epi32(_mm_sub_epi32(_mm_sub_epi32(Sum1QWA,Sum1EA),Sum1RA),Sum1TA);
  __m128i Line8_1B= _mm_sub_epi32(_mm_sub_epi32(_mm_sub_epi32(Sum1QWB,Sum1EB),Sum1RB),Sum1TB);
  __m128i Line8_2A= _mm_sub_epi32(_mm_sub_epi32(_mm_sub_epi32(Sum2QWA,Sum2EA),Sum2RA),Sum2TA);
  __m128i Line8_2B= _mm_sub_epi32(_mm_sub_epi32(_mm_sub_epi32(Sum2QWB,Sum2EB),Sum2RB),Sum2TB);

  //line 11
  __m128i Line11_1A= _mm_sub_epi32(_mm_sub_epi32(Dif1QWA,Sum1IA),Sum1YA);
  __m128i Line11_1B= _mm_sub_epi32(_mm_sub_epi32(Dif1QWB,Sum1IB),Sum1YB);
  __m128i Line11_2A= _mm_sub_epi32(_mm_sub_epi32(Dif2QWA,Sum2IA),Sum2YA);
  __m128i Line11_2B= _mm_sub_epi32(_mm_sub_epi32(Dif2QWB,Sum2IB),Sum2YB);

  //line 12
  __m128i Line12_1A= _mm_sub_epi32(_mm_add_epi32(Dif1QWA,Sum1IA),Sum1UA);
  __m128i Line12_1B= _mm_sub_epi32(_mm_add_epi32(Dif1QWB,Sum1IB),Sum1UB);
  __m128i Line12_2A= _mm_sub_epi32(_mm_add_epi32(Dif2QWA,Sum2IA),Sum2UA);
  __m128i Line12_2B= _mm_sub_epi32(_mm_add_epi32(Dif2QWB,Sum2IB),Sum2UB);

  //line 15
  __m128i Line15_1A= _mm_sub_epi32(_mm_add_epi32(Sum1QWA,Sum1EA),Sum1OA);
  __m128i Line15_1B= _mm_sub_epi32(_mm_add_epi32(Sum1QWB,Sum1EB),Sum1OB);
  __m128i Line15_2A= _mm_sub_epi32(_mm_add_epi32(Sum2QWA,Sum2EA),Sum2OA);
  __m128i Line15_2B= _mm_sub_epi32(_mm_add_epi32(Sum2QWB,Sum2EB),Sum2OB);
  
  Line0_1A=_mm_srai_epi32(_mm_add_epi32(Line0_1A, Add1st),7);
  Line0_1B=_mm_srai_epi32(_mm_add_epi32(Line0_1B, Add1st),7);
  Line0_2A=_mm_srai_epi32(_mm_add_epi32(Line0_2A, Add1st),7);
  Line0_2B=_mm_srai_epi32(_mm_add_epi32(Line0_2B, Add1st),7);

  Line3_1A=_mm_srai_epi32(_mm_add_epi32(Line3_1A, Add1st),7);
  Line3_1B=_mm_srai_epi32(_mm_add_epi32(Line3_1B, Add1st),7);
  Line3_2A=_mm_srai_epi32(_mm_add_epi32(Line3_2A, Add1st),7);
  Line3_2B=_mm_srai_epi32(_mm_add_epi32(Line3_2B, Add1st),7);

  Line4_1A=_mm_srai_epi32(_mm_add_epi32(Line4_1A, Add1st),7);
  Line4_1B=_mm_srai_epi32(_mm_add_epi32(Line4_1B, Add1st),7);
  Line4_2A=_mm_srai_epi32(_mm_add_epi32(Line4_2A, Add1st),7);
  Line4_2B=_mm_srai_epi32(_mm_add_epi32(Line4_2B, Add1st),7);

  Line7_1A=_mm_srai_epi32(_mm_add_epi32(Line7_1A, Add1st),7);
  Line7_1B=_mm_srai_epi32(_mm_add_epi32(Line7_1B, Add1st),7);
  Line7_2A=_mm_srai_epi32(_mm_add_epi32(Line7_2A, Add1st),7);
  Line7_2B=_mm_srai_epi32(_mm_add_epi32(Line7_2B, Add1st),7);
  Line8_1A=_mm_srai_epi32(_mm_add_epi32(Line8_1A, Add1st),7);
  Line8_1B=_mm_srai_epi32(_mm_add_epi32(Line8_1B, Add1st),7);
  Line8_2A=_mm_srai_epi32(_mm_add_epi32(Line8_2A, Add1st),7);
  Line8_2B=_mm_srai_epi32(_mm_add_epi32(Line8_2B, Add1st),7);

  Line11_1A=_mm_srai_epi32(_mm_add_epi32(Line11_1A, Add1st),7);
  Line11_1B=_mm_srai_epi32(_mm_add_epi32(Line11_1B, Add1st),7);
  Line11_2A=_mm_srai_epi32(_mm_add_epi32(Line11_2A, Add1st),7);
  Line11_2B=_mm_srai_epi32(_mm_add_epi32(Line11_2B, Add1st),7);

  Line12_1A=_mm_srai_epi32(_mm_add_epi32(Line12_1A, Add1st),7);
  Line12_1B=_mm_srai_epi32(_mm_add_epi32(Line12_1B, Add1st),7);
  Line12_2A=_mm_srai_epi32(_mm_add_epi32(Line12_2A, Add1st),7);
  Line12_2B=_mm_srai_epi32(_mm_add_epi32(Line12_2B, Add1st),7);

  Line15_1A=_mm_srai_epi32(_mm_add_epi32(Line15_1A, Add1st),7);
  Line15_1B=_mm_srai_epi32(_mm_add_epi32(Line15_1B, Add1st),7);
  Line15_2A=_mm_srai_epi32(_mm_add_epi32(Line15_2A, Add1st),7);
  Line15_2B=_mm_srai_epi32(_mm_add_epi32(Line15_2B, Add1st),7);

  Tmp2[2*i  ] = _mm_packs_epi32(Line0_1A, Line0_1B);
  Tmp2[2*i+1] = _mm_packs_epi32(Line0_2A, Line0_2B);

  Tmp2[6-2*i  ] = _mm_packs_epi32(Line3_1A, Line3_1B);
  Tmp2[6-2*i+1] = _mm_packs_epi32(Line3_2A, Line3_2B);
  Tmp2[8+2*i  ] = _mm_packs_epi32(Line4_1A, Line4_1B);
  Tmp2[8+2*i+1] = _mm_packs_epi32(Line4_2A, Line4_2B);

  Tmp2[14-2*i]   = _mm_packs_epi32(Line7_1A, Line7_1B);
  Tmp2[14-2*i+1] = _mm_packs_epi32(Line7_2A, Line7_2B);
  Tmp2[16+2*i]   = _mm_packs_epi32(Line8_1A, Line8_1B);
  Tmp2[16+2*i+1] = _mm_packs_epi32(Line8_2A, Line8_2B);

  Tmp2[22-2*i]   = _mm_packs_epi32(Line11_1A, Line11_1B);
  Tmp2[22-2*i+1] = _mm_packs_epi32(Line11_2A, Line11_2B);
    Tmp2[24+2*i]   = _mm_packs_epi32(Line12_1A, Line12_1B);     
  Tmp2[24+2*i+1] = _mm_packs_epi32(Line12_2A, Line12_2B); 

  Tmp2[30-2*i]   = _mm_packs_epi32(Line15_1A, Line15_1B);
  Tmp2[30-2*i+1] = _mm_packs_epi32(Line15_2A, Line15_2B);
  }

  //horizontal transform and store
  for(int32 j=0; j<16; j++)
  {
    __m128i LineA = Tmp2[2*j];
    __m128i LineB = Tmp2[2*j+1];

    for(int32 i=0, k=0; i<32; i+=2, k++)
    {
      tr[k] = _mm_add_epi32(_mm_madd_epi16(LineA, xiT16_DCT[i  ]), _mm_madd_epi16(LineB, xiT16_DCT[i+1]));
    }

    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<4; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add2nd), Shift2nd);
  
    __m128i tr1=  _mm_packs_epi32(tr[0], tr[1]);
  __m128i tr2= _mm_packs_epi32(tr[2], tr[3]);
  _mm_storeu_si128((__m128i*)Dst, tr1);
  _mm_storeu_si128((__m128i*)(Dst+8), tr2);
    Dst += DstStride; 
  }
}
void xTransform::InvTransformDCT_32x32_STD_C(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[16],O[16];
  int32 EE[8],EO[8];
  int32 EEE[4],EEO[4];
  int32 EEEE[2],EEEO[2];
  int32 Tmp[32][32];  
  int32 Shift2nd = 20 - BitDepth; 
  int32 Add2nd   = 1<<(Shift2nd-1);

  for(int32 j=0; j<32; j++)
  { 
    O[ 0] = 90*Src[32] + 90*Src[ 3*32] + 88*Src[ 5*32] + 85*Src[ 7*32] + 82*Src[ 9*32] + 78*Src[11*32] + 73*Src[13*32] + 67*Src[15*32] + 61*Src[17*32] + 54*Src[19*32] + 46*Src[21*32] + 38*Src[23*32] + 31*Src[25*32] + 22*Src[27*32] + 13*Src[29*32] +  4*Src[31*32];
    O[ 1] = 90*Src[32] + 82*Src[ 3*32] + 67*Src[ 5*32] + 46*Src[ 7*32] + 22*Src[ 9*32] -  4*Src[11*32] - 31*Src[13*32] - 54*Src[15*32] - 73*Src[17*32] - 85*Src[19*32] - 90*Src[21*32] - 88*Src[23*32] - 78*Src[25*32] - 61*Src[27*32] - 38*Src[29*32] - 13*Src[31*32];
    O[ 2] = 88*Src[32] + 67*Src[ 3*32] + 31*Src[ 5*32] - 13*Src[ 7*32] - 54*Src[ 9*32] - 82*Src[11*32] - 90*Src[13*32] - 78*Src[15*32] - 46*Src[17*32] -  4*Src[19*32] + 38*Src[21*32] + 73*Src[23*32] + 90*Src[25*32] + 85*Src[27*32] + 61*Src[29*32] + 22*Src[31*32];
    O[ 3] = 85*Src[32] + 46*Src[ 3*32] - 13*Src[ 5*32] - 67*Src[ 7*32] - 90*Src[ 9*32] - 73*Src[11*32] - 22*Src[13*32] + 38*Src[15*32] + 82*Src[17*32] + 88*Src[19*32] + 54*Src[21*32] -  4*Src[23*32] - 61*Src[25*32] - 90*Src[27*32] - 78*Src[29*32] - 31*Src[31*32];
    O[ 4] = 82*Src[32] + 22*Src[ 3*32] - 54*Src[ 5*32] - 90*Src[ 7*32] - 61*Src[ 9*32] + 13*Src[11*32] + 78*Src[13*32] + 85*Src[15*32] + 31*Src[17*32] - 46*Src[19*32] - 90*Src[21*32] - 67*Src[23*32] +  4*Src[25*32] + 73*Src[27*32] + 88*Src[29*32] + 38*Src[31*32];
    O[ 5] = 78*Src[32] -  4*Src[ 3*32] - 82*Src[ 5*32] - 73*Src[ 7*32] + 13*Src[ 9*32] + 85*Src[11*32] + 67*Src[13*32] - 22*Src[15*32] - 88*Src[17*32] - 61*Src[19*32] + 31*Src[21*32] + 90*Src[23*32] + 54*Src[25*32] - 38*Src[27*32] - 90*Src[29*32] - 46*Src[31*32];
    O[ 6] = 73*Src[32] - 31*Src[ 3*32] - 90*Src[ 5*32] - 22*Src[ 7*32] + 78*Src[ 9*32] + 67*Src[11*32] - 38*Src[13*32] - 90*Src[15*32] - 13*Src[17*32] + 82*Src[19*32] + 61*Src[21*32] - 46*Src[23*32] - 88*Src[25*32] -  4*Src[27*32] + 85*Src[29*32] + 54*Src[31*32];
    O[ 7] = 67*Src[32] - 54*Src[ 3*32] - 78*Src[ 5*32] + 38*Src[ 7*32] + 85*Src[ 9*32] - 22*Src[11*32] - 90*Src[13*32] +  4*Src[15*32] + 90*Src[17*32] + 13*Src[19*32] - 88*Src[21*32] - 31*Src[23*32] + 82*Src[25*32] + 46*Src[27*32] - 73*Src[29*32] - 61*Src[31*32];
    O[ 8] = 61*Src[32] - 73*Src[ 3*32] - 46*Src[ 5*32] + 82*Src[ 7*32] + 31*Src[ 9*32] - 88*Src[11*32] - 13*Src[13*32] + 90*Src[15*32] -  4*Src[17*32] - 90*Src[19*32] + 22*Src[21*32] + 85*Src[23*32] - 38*Src[25*32] - 78*Src[27*32] + 54*Src[29*32] + 67*Src[31*32];
    O[ 9] = 54*Src[32] - 85*Src[ 3*32] -  4*Src[ 5*32] + 88*Src[ 7*32] - 46*Src[ 9*32] - 61*Src[11*32] + 82*Src[13*32] + 13*Src[15*32] - 90*Src[17*32] + 38*Src[19*32] + 67*Src[21*32] - 78*Src[23*32] - 22*Src[25*32] + 90*Src[27*32] - 31*Src[29*32] - 73*Src[31*32];
    O[10] = 46*Src[32] - 90*Src[ 3*32] + 38*Src[ 5*32] + 54*Src[ 7*32] - 90*Src[ 9*32] + 31*Src[11*32] + 61*Src[13*32] - 88*Src[15*32] + 22*Src[17*32] + 67*Src[19*32] - 85*Src[21*32] + 13*Src[23*32] + 73*Src[25*32] - 82*Src[27*32] +  4*Src[29*32] + 78*Src[31*32];
    O[11] = 38*Src[32] - 88*Src[ 3*32] + 73*Src[ 5*32] -  4*Src[ 7*32] - 67*Src[ 9*32] + 90*Src[11*32] - 46*Src[13*32] - 31*Src[15*32] + 85*Src[17*32] - 78*Src[19*32] + 13*Src[21*32] + 61*Src[23*32] - 90*Src[25*32] + 54*Src[27*32] + 22*Src[29*32] - 82*Src[31*32];
    O[12] = 31*Src[32] - 78*Src[ 3*32] + 90*Src[ 5*32] - 61*Src[ 7*32] +  4*Src[ 9*32] + 54*Src[11*32] - 88*Src[13*32] + 82*Src[15*32] - 38*Src[17*32] - 22*Src[19*32] + 73*Src[21*32] - 90*Src[23*32] + 67*Src[25*32] - 13*Src[27*32] - 46*Src[29*32] + 85*Src[31*32];
    O[13] = 22*Src[32] - 61*Src[ 3*32] + 85*Src[ 5*32] - 90*Src[ 7*32] + 73*Src[ 9*32] - 38*Src[11*32] -  4*Src[13*32] + 46*Src[15*32] - 78*Src[17*32] + 90*Src[19*32] - 82*Src[21*32] + 54*Src[23*32] - 13*Src[25*32] - 31*Src[27*32] + 67*Src[29*32] - 88*Src[31*32];
    O[14] = 13*Src[32] - 38*Src[ 3*32] + 61*Src[ 5*32] - 78*Src[ 7*32] + 88*Src[ 9*32] - 90*Src[11*32] + 85*Src[13*32] - 73*Src[15*32] + 54*Src[17*32] - 31*Src[19*32] +  4*Src[21*32] + 22*Src[23*32] - 46*Src[25*32] + 67*Src[27*32] - 82*Src[29*32] + 90*Src[31*32];
    O[15] =  4*Src[32] - 13*Src[ 3*32] + 22*Src[ 5*32] - 31*Src[ 7*32] + 38*Src[ 9*32] - 46*Src[11*32] + 54*Src[13*32] - 61*Src[15*32] + 67*Src[17*32] - 73*Src[19*32] + 78*Src[21*32] - 82*Src[23*32] + 85*Src[25*32] - 88*Src[27*32] + 90*Src[29*32] - 90*Src[31*32];

    EO[0] = 90*Src[ 2*32  ] + 87*Src[ 6*32  ] + 80*Src[ 10*32 ] + 70*Src[ 14*32 ] + 57*Src[ 18*32 ] + 43*Src[ 22*32 ] + 25*Src[ 26*32 ] +  9*Src[ 30*32 ];
    EO[1] = 87*Src[ 2*32  ] + 57*Src[ 6*32  ] +  9*Src[ 10*32 ] - 43*Src[ 14*32 ] - 80*Src[ 18*32 ] - 90*Src[ 22*32 ] - 70*Src[ 26*32 ] - 25*Src[ 30*32 ];
    EO[2] = 80*Src[ 2*32  ] +  9*Src[ 6*32  ] - 70*Src[ 10*32 ] - 87*Src[ 14*32 ] - 25*Src[ 18*32 ] + 57*Src[ 22*32 ] + 90*Src[ 26*32 ] + 43*Src[ 30*32 ];
    EO[3] = 70*Src[ 2*32  ] - 43*Src[ 6*32  ] - 87*Src[ 10*32 ] +  9*Src[ 14*32 ] + 90*Src[ 18*32 ] + 25*Src[ 22*32 ] - 80*Src[ 26*32 ] - 57*Src[ 30*32 ];
    EO[4] = 57*Src[ 2*32  ] - 80*Src[ 6*32  ] - 25*Src[ 10*32 ] + 90*Src[ 14*32 ] -  9*Src[ 18*32 ] - 87*Src[ 22*32 ] + 43*Src[ 26*32 ] + 70*Src[ 30*32 ];
    EO[5] = 43*Src[ 2*32  ] - 90*Src[ 6*32  ] + 57*Src[ 10*32 ] + 25*Src[ 14*32 ] - 87*Src[ 18*32 ] + 70*Src[ 22*32 ] +  9*Src[ 26*32 ] - 80*Src[ 30*32 ];
    EO[6] = 25*Src[ 2*32  ] - 70*Src[ 6*32  ] + 90*Src[ 10*32 ] - 80*Src[ 14*32 ] + 43*Src[ 18*32 ] +  9*Src[ 22*32 ] - 57*Src[ 26*32 ] + 87*Src[ 30*32 ];
    EO[7] =  9*Src[ 2*32  ] - 25*Src[ 6*32  ] + 43*Src[ 10*32 ] - 57*Src[ 14*32 ] + 70*Src[ 18*32 ] - 80*Src[ 22*32 ] + 87*Src[ 26*32 ] - 90*Src[ 30*32 ];

    EEO[0] = 89*Src[ 4*32 ] + 75*Src[ 12*32 ] + 50*Src[ 20*32 ] + 18*Src[ 28*32 ];
    EEO[1] = 75*Src[ 4*32 ] - 18*Src[ 12*32 ] - 89*Src[ 20*32 ] - 50*Src[ 28*32 ];
    EEO[2] = 50*Src[ 4*32 ] - 89*Src[ 12*32 ] + 18*Src[ 20*32 ] + 75*Src[ 28*32 ];
    EEO[3] = 18*Src[ 4*32 ] - 50*Src[ 12*32 ] + 75*Src[ 20*32 ] - 89*Src[ 28*32 ];

    EEEO[0] = 83*Src[ 8*32 ] + 36*Src[ 24*32 ];
    EEEO[1] = 36*Src[ 8*32 ] - 83*Src[ 24*32 ];
    EEEE[0] = 64*Src[ 0    ] + 64*Src[ 16*32 ];    
    EEEE[1] = 64*Src[ 0    ] - 64*Src[ 16*32 ];
    Src++;

    EEE[0] = EEEE[0] + EEEO[0];
    EEE[3] = EEEE[0] - EEEO[0];
    EEE[1] = EEEE[1] + EEEO[1];
    EEE[2] = EEEE[1] - EEEO[1];    

    for (int32 k=0;k<4;k++)
    {
      EE[k] = EEE[k] + EEO[k];
      EE[k+4] = EEE[3-k] - EEO[3-k];
    }    
    for (int32 k=0;k<8;k++)
    {
      E[k] = EE[k] + EO[k];
      E[k+8] = EE[7-k] - EO[7-k];
    }    
    for (int32 k=0;k<16;k++)
    {
      Tmp[j][k]    = xClipS16((E[k] + O[k] + 64)>>7);
      Tmp[j][k+16] = xClipS16((E[15-k] - O[15-k] + 64)>>7);
    }   
  }

  for(int32 j=0; j<32; j++)
  {
    O[ 0] = 90*Tmp[ 1][j] + 90*Tmp[ 3][j] + 88*Tmp[ 5][j] + 85*Tmp[ 7][j] + 82*Tmp[ 9][j] + 78*Tmp[11][j] + 73*Tmp[13][j] + 67*Tmp[15][j] + 61*Tmp[17][j] + 54*Tmp[19][j] + 46*Tmp[21][j] + 38*Tmp[23][j] + 31*Tmp[25][j] + 22*Tmp[27][j] + 13*Tmp[29][j] + 4*Tmp[31][j];
    O[ 1] = 90*Tmp[ 1][j] + 82*Tmp[ 3][j] + 67*Tmp[ 5][j] + 46*Tmp[ 7][j] + 22*Tmp[ 9][j] -  4*Tmp[11][j] - 31*Tmp[13][j] - 54*Tmp[15][j] - 73*Tmp[17][j] - 85*Tmp[19][j] - 90*Tmp[21][j] - 88*Tmp[23][j] - 78*Tmp[25][j] - 61*Tmp[27][j] - 38*Tmp[29][j] - 13*Tmp[31][j];
    O[ 2] = 88*Tmp[ 1][j] + 67*Tmp[ 3][j] + 31*Tmp[ 5][j] - 13*Tmp[ 7][j] - 54*Tmp[ 9][j] - 82*Tmp[11][j] - 90*Tmp[13][j] - 78*Tmp[15][j] - 46*Tmp[17][j] - 4*Tmp[19][j] + 38*Tmp[21][j] + 73*Tmp[23][j] + 90*Tmp[25][j] + 85*Tmp[27][j] + 61*Tmp[29][j] + 22*Tmp[31][j];
    O[ 3] = 85*Tmp[ 1][j] + 46*Tmp[ 3][j] - 13*Tmp[ 5][j] - 67*Tmp[ 7][j] - 90*Tmp[ 9][j] - 73*Tmp[11][j] - 22*Tmp[13][j] + 38*Tmp[15][j] + 82*Tmp[17][j] + 88*Tmp[19][j] + 54*Tmp[21][j] - 4*Tmp[23][j] - 61*Tmp[25][j] - 90*Tmp[27][j] - 78*Tmp[29][j] - 31*Tmp[31][j];
    O[ 4] = 82*Tmp[ 1][j] + 22*Tmp[ 3][j] - 54*Tmp[ 5][j] - 90*Tmp[ 7][j] - 61*Tmp[ 9][j] + 13*Tmp[11][j] + 78*Tmp[13][j] + 85*Tmp[15][j] + 31*Tmp[17][j] - 46*Tmp[19][j] - 90*Tmp[21][j] - 67*Tmp[23][j] + 4*Tmp[25][j] + 73*Tmp[27][j] + 88*Tmp[29][j] + 38*Tmp[31][j];
    O[ 5] = 78*Tmp[ 1][j] -  4*Tmp[ 3][j] - 82*Tmp[ 5][j] - 73*Tmp[ 7][j] + 13*Tmp[ 9][j] + 85*Tmp[11][j] + 67*Tmp[13][j] - 22*Tmp[15][j] - 88*Tmp[17][j] - 61*Tmp[19][j] + 31*Tmp[21][j] + 90*Tmp[23][j] + 54*Tmp[25][j] - 38*Tmp[27][j] - 90*Tmp[29][j] - 46*Tmp[31][j];
    O[ 6] = 73*Tmp[ 1][j] - 31*Tmp[ 3][j] - 90*Tmp[ 5][j] - 22*Tmp[ 7][j] + 78*Tmp[ 9][j] + 67*Tmp[11][j] - 38*Tmp[13][j] - 90*Tmp[15][j] - 13*Tmp[17][j] + 82*Tmp[19][j] + 61*Tmp[21][j] - 46*Tmp[23][j] - 88*Tmp[25][j] - 4*Tmp[27][j] + 85*Tmp[29][j] + 54*Tmp[31][j];
    O[ 7] = 67*Tmp[ 1][j] - 54*Tmp[ 3][j] - 78*Tmp[ 5][j] + 38*Tmp[ 7][j] + 85*Tmp[ 9][j] - 22*Tmp[11][j] - 90*Tmp[13][j] +  4*Tmp[15][j] + 90*Tmp[17][j] + 13*Tmp[19][j] - 88*Tmp[21][j] - 31*Tmp[23][j] + 82*Tmp[25][j] + 46*Tmp[27][j] - 73*Tmp[29][j] - 61*Tmp[31][j];
    O[ 8] = 61*Tmp[ 1][j] - 73*Tmp[ 3][j] - 46*Tmp[ 5][j] + 82*Tmp[ 7][j] + 31*Tmp[ 9][j] - 88*Tmp[11][j] - 13*Tmp[13][j] + 90*Tmp[15][j] - 4*Tmp[17][j] - 90*Tmp[19][j] + 22*Tmp[21][j] + 85*Tmp[23][j] - 38*Tmp[25][j] - 78*Tmp[27][j] + 54*Tmp[29][j] + 67*Tmp[31][j];
    O[ 9] = 54*Tmp[ 1][j] - 85*Tmp[ 3][j] -  4*Tmp[ 5][j] + 88*Tmp[ 7][j] - 46*Tmp[ 9][j] - 61*Tmp[11][j] + 82*Tmp[13][j] + 13*Tmp[15][j] - 90*Tmp[17][j] + 38*Tmp[19][j] + 67*Tmp[21][j] - 78*Tmp[23][j] - 22*Tmp[25][j] + 90*Tmp[27][j] - 31*Tmp[29][j] - 73*Tmp[31][j];
    O[10] = 46*Tmp[ 1][j] - 90*Tmp[ 3][j] + 38*Tmp[ 5][j] + 54*Tmp[ 7][j] - 90*Tmp[ 9][j] + 31*Tmp[11][j] + 61*Tmp[13][j] - 88*Tmp[15][j] + 22*Tmp[17][j] + 67*Tmp[19][j] - 85*Tmp[21][j] + 13*Tmp[23][j] + 73*Tmp[25][j] - 82*Tmp[27][j] + 4*Tmp[29][j] + 78*Tmp[31][j];
    O[11] = 38*Tmp[ 1][j] - 88*Tmp[ 3][j] + 73*Tmp[ 5][j] -  4*Tmp[ 7][j] - 67*Tmp[ 9][j] + 90*Tmp[11][j] - 46*Tmp[13][j] - 31*Tmp[15][j] + 85*Tmp[17][j] - 78*Tmp[19][j] + 13*Tmp[21][j] + 61*Tmp[23][j] - 90*Tmp[25][j] + 54*Tmp[27][j] + 22*Tmp[29][j] - 82*Tmp[31][j];
    O[12] = 31*Tmp[ 1][j] - 78*Tmp[ 3][j] + 90*Tmp[ 5][j] - 61*Tmp[ 7][j] +  4*Tmp[ 9][j] + 54*Tmp[11][j] - 88*Tmp[13][j] + 82*Tmp[15][j] - 38*Tmp[17][j] - 22*Tmp[19][j] + 73*Tmp[21][j] - 90*Tmp[23][j] + 67*Tmp[25][j] - 13*Tmp[27][j] - 46*Tmp[29][j] + 85*Tmp[31][j];
    O[13] = 22*Tmp[ 1][j] - 61*Tmp[ 3][j] + 85*Tmp[ 5][j] - 90*Tmp[ 7][j] + 73*Tmp[ 9][j] - 38*Tmp[11][j] -  4*Tmp[13][j] + 46*Tmp[15][j] - 78*Tmp[17][j] + 90*Tmp[19][j] - 82*Tmp[21][j] + 54*Tmp[23][j] - 13*Tmp[25][j] - 31*Tmp[27][j] + 67*Tmp[29][j] - 88*Tmp[31][j];
    O[14] = 13*Tmp[ 1][j] - 38*Tmp[ 3][j] + 61*Tmp[ 5][j] - 78*Tmp[ 7][j] + 88*Tmp[ 9][j] - 90*Tmp[11][j] + 85*Tmp[13][j] - 73*Tmp[15][j] + 54*Tmp[17][j] - 31*Tmp[19][j] + 4*Tmp[21][j] + 22*Tmp[23][j] - 46*Tmp[25][j] + 67*Tmp[27][j] - 82*Tmp[29][j] + 90*Tmp[31][j];
    O[15] =  4*Tmp[ 1][j] - 13*Tmp[ 3][j] + 22*Tmp[ 5][j] - 31*Tmp[ 7][j] + 38*Tmp[ 9][j] - 46*Tmp[11][j] + 54*Tmp[13][j] - 61*Tmp[15][j] + 67*Tmp[17][j] - 73*Tmp[19][j] + 78*Tmp[21][j] - 82*Tmp[23][j] + 85*Tmp[25][j] - 88*Tmp[27][j] + 90*Tmp[29][j] - 90*Tmp[31][j];

    EO[0] = 90*Tmp[ 2][j] + 87*Tmp[ 6][j] + 80*Tmp[10][j] + 70*Tmp[14][j] + 57*Tmp[18][j] + 43*Tmp[22][j] + 25*Tmp[26][j] + 9*Tmp[30][j];
    EO[1] = 87*Tmp[ 2][j] + 57*Tmp[ 6][j] + 9*Tmp[10][j] - 43*Tmp[14][j] - 80*Tmp[18][j] - 90*Tmp[22][j] - 70*Tmp[26][j] - 25*Tmp[30][j];
    EO[2] = 80*Tmp[ 2][j] + 9*Tmp[ 6][j] - 70*Tmp[10][j] - 87*Tmp[14][j] - 25*Tmp[18][j] + 57*Tmp[22][j] + 90*Tmp[26][j] + 43*Tmp[30][j];
    EO[3] = 70*Tmp[ 2][j] - 43*Tmp[ 6][j] - 87*Tmp[10][j] + 9*Tmp[14][j] + 90*Tmp[18][j] + 25*Tmp[22][j] - 80*Tmp[26][j] - 57*Tmp[30][j];
    EO[4] = 57*Tmp[ 2][j] - 80*Tmp[ 6][j] - 25*Tmp[10][j] + 90*Tmp[14][j] - 9*Tmp[18][j] - 87*Tmp[22][j] + 43*Tmp[26][j] + 70*Tmp[30][j];
    EO[5] = 43*Tmp[ 2][j] - 90*Tmp[ 6][j] + 57*Tmp[10][j] + 25*Tmp[14][j] - 87*Tmp[18][j] + 70*Tmp[22][j] + 9*Tmp[26][j] - 80*Tmp[30][j];
    EO[6] = 25*Tmp[ 2][j] - 70*Tmp[ 6][j] + 90*Tmp[10][j] - 80*Tmp[14][j] + 43*Tmp[18][j] + 9*Tmp[22][j] - 57*Tmp[26][j] + 87*Tmp[30][j];
    EO[7] = 9*Tmp[ 2][j] - 25*Tmp[ 6][j] + 43*Tmp[10][j] - 57*Tmp[14][j] + 70*Tmp[18][j] - 80*Tmp[22][j] + 87*Tmp[26][j] - 90*Tmp[30][j];

    EEO[0] = 89*Tmp[4][j] + 75*Tmp[12][j] + 50*Tmp[20][j] + 18*Tmp[28][j];
    EEO[1] = 75*Tmp[4][j] - 18*Tmp[12][j] - 89*Tmp[20][j] - 50*Tmp[28][j];
    EEO[2] = 50*Tmp[4][j] - 89*Tmp[12][j] + 18*Tmp[20][j] + 75*Tmp[28][j];
    EEO[3] = 18*Tmp[4][j] - 50*Tmp[12][j] + 75*Tmp[20][j] - 89*Tmp[28][j];

    EEEO[0] = 83*Tmp[8][j] + 36*Tmp[24][j];
    EEEO[1] = 36*Tmp[8][j] - 83*Tmp[24][j];
    EEEE[0] = 64*Tmp[0][j] + 64*Tmp[16][j];    
    EEEE[1] = 64*Tmp[0][j] - 64*Tmp[16][j];

    EEE[0] = EEEE[0] + EEEO[0];
    EEE[3] = EEEE[0] - EEEO[0];
    EEE[1] = EEEE[1] + EEEO[1];
    EEE[2] = EEEE[1] - EEEO[1];    

    for (int32 k=0;k<4;k++)
    {
      EE[k] = EEE[k] + EEO[k];
      EE[k+4] = EEE[3-k] - EEO[3-k];
    }    
    for (int32 k=0;k<8;k++)
    {
      E[k] = EE[k] + EO[k];
      E[k+8] = EE[7-k] - EO[7-k];
    }    
    for (int32 k=0;k<16;k++)
    {
      Dst[k]    = xClipS16((E[k] + O[k] + Add2nd)>>Shift2nd);
      Dst[k+16] = xClipS16((E[15-k] - O[15-k] + Add2nd)>>Shift2nd);
    }
    Dst += DstStride;
  }
}
void xTransform::InvTransformDCT_32x32_STD_M(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[16],O[16];
  int32 EE[8],EO[8];
  int32 EEE[4],EEO[4];
  int32 EEEE[2],EEEO[2];
  int32 Tmp[32][32];  
  int32 Shift2nd = 20 - BitDepth; 
  int32 Add2nd   = 1<<(Shift2nd-1);

  for(int32 j=0; j<32; j++)
  { 
    for (int32 k=0;k<16;k++)
    {
      O[k] = m_TrM32x32[ 1][k]*Src[ 0*32  ] + m_TrM32x32[ 3][k]*Src[ 3*32  ] + m_TrM32x32[ 5][k]*Src[ 5*32  ] + m_TrM32x32[ 7][k]*Src[ 7*32  ] + 
             m_TrM32x32[ 9][k]*Src[ 9*32  ] + m_TrM32x32[11][k]*Src[ 11*32 ] + m_TrM32x32[13][k]*Src[ 13*32 ] + m_TrM32x32[15][k]*Src[ 15*32 ] + 
             m_TrM32x32[17][k]*Src[ 17*32 ] + m_TrM32x32[19][k]*Src[ 19*32 ] + m_TrM32x32[21][k]*Src[ 21*32 ] + m_TrM32x32[23][k]*Src[ 23*32 ] + 
             m_TrM32x32[25][k]*Src[ 25*32 ] + m_TrM32x32[27][k]*Src[ 27*32 ] + m_TrM32x32[29][k]*Src[ 29*32 ] + m_TrM32x32[31][k]*Src[ 31*32 ];
    }
    for (int32 k=0;k<8;k++)
    {
      EO[k] = m_TrM32x32[ 2][k]*Src[ 2*32  ] + m_TrM32x32[ 6][k]*Src[ 6*32  ] + m_TrM32x32[10][k]*Src[ 10*32 ] + m_TrM32x32[14][k]*Src[ 14*32 ] + 
              m_TrM32x32[18][k]*Src[ 18*32 ] + m_TrM32x32[22][k]*Src[ 22*32 ] + m_TrM32x32[26][k]*Src[ 26*32 ] + m_TrM32x32[30][k]*Src[ 30*32 ];
    }
    for (int32 k=0;k<4;k++)
    {
      EEO[k] = m_TrM32x32[4][k]*Src[ 4*32 ] + m_TrM32x32[12][k]*Src[ 12*32 ] + m_TrM32x32[20][k]*Src[ 20*32 ] + m_TrM32x32[28][k]*Src[ 28*32 ];
    }   

    EEEO[0] = m_TrM32x32[8][0]*Src[ 8*32 ] + m_TrM32x32[24][0]*Src[ 24*32 ];
    EEEO[1] = m_TrM32x32[8][1]*Src[ 8*32 ] + m_TrM32x32[24][1]*Src[ 24*32 ];
    EEEE[0] = m_TrM32x32[0][0]*Src[ 0    ] + m_TrM32x32[16][0]*Src[ 16*32 ];    
    EEEE[1] = m_TrM32x32[0][1]*Src[ 0    ] + m_TrM32x32[16][1]*Src[ 16*32 ];
    Src++;

    EEE[0] = EEEE[0] + EEEO[0];
    EEE[3] = EEEE[0] - EEEO[0];
    EEE[1] = EEEE[1] + EEEO[1];
    EEE[2] = EEEE[1] - EEEO[1];    

    for (int32 k=0;k<4;k++)
    {
      EE[k] = EEE[k] + EEO[k];
      EE[k+4] = EEE[3-k] - EEO[3-k];
    }    
    for (int32 k=0;k<8;k++)
    {
      E[k] = EE[k] + EO[k];
      E[k+8] = EE[7-k] - EO[7-k];
    }    
    for (int32 k=0;k<16;k++)
    {
      Tmp[j][k]    = xClipS16((E[k] + O[k] + 64)>>7);
      Tmp[j][k+16] = xClipS16((E[15-k] - O[15-k] + 64)>>7);
    }   
  }

  for(int32 j=0; j<32; j++)
  {
    for (int32 k=0;k<16;k++)
    {
      O[k] = m_TrM32x32[ 1][k]*Tmp[ 0][j] + m_TrM32x32[ 3][k]*Tmp[ 3][j] + m_TrM32x32[ 5][k]*Tmp[ 5][j] + m_TrM32x32[ 7][k]*Tmp[ 7][j] + 
             m_TrM32x32[ 9][k]*Tmp[ 9][j] + m_TrM32x32[11][k]*Tmp[11][j] + m_TrM32x32[13][k]*Tmp[13][j] + m_TrM32x32[15][k]*Tmp[15][j] + 
             m_TrM32x32[17][k]*Tmp[17][j] + m_TrM32x32[19][k]*Tmp[19][j] + m_TrM32x32[21][k]*Tmp[21][j] + m_TrM32x32[23][k]*Tmp[23][j] + 
             m_TrM32x32[25][k]*Tmp[25][j] + m_TrM32x32[27][k]*Tmp[27][j] + m_TrM32x32[29][k]*Tmp[29][j] + m_TrM32x32[31][k]*Tmp[31][j];
    }
    for (int32 k=0;k<8;k++)
    {
      EO[k] = m_TrM32x32[ 2][k]*Tmp[ 2  ][j] + m_TrM32x32[ 6][k]*Tmp[ 6  ][j] + m_TrM32x32[10][k]*Tmp[ 10 ][j] + m_TrM32x32[14][k]*Tmp[ 14 ][j] + 
              m_TrM32x32[18][k]*Tmp[ 18 ][j] + m_TrM32x32[22][k]*Tmp[ 22 ][j] + m_TrM32x32[26][k]*Tmp[ 26 ][j] + m_TrM32x32[30][k]*Tmp[ 30 ][j];
    }
    for (int32 k=0;k<4;k++)
    {
      EEO[k] = m_TrM32x32[4][k]*Tmp[ 4 ][j] + m_TrM32x32[12][k]*Tmp[ 12 ][j] + m_TrM32x32[20][k]*Tmp[ 20 ][j] + m_TrM32x32[28][k]*Tmp[ 28 ][j];
    }
    EEEO[0] = m_TrM32x32[ 8][0]*Tmp[ 8][j] + m_TrM32x32[24][0]*Tmp[24][j];
    EEEO[1] = m_TrM32x32[ 8][1]*Tmp[ 8][j] + m_TrM32x32[24][1]*Tmp[24][j];
    EEEE[0] = m_TrM32x32[ 0][0]*Tmp[ 0][j] + m_TrM32x32[16][0]*Tmp[16][j];    
    EEEE[1] = m_TrM32x32[ 0][1]*Tmp[ 0][j] + m_TrM32x32[16][1]*Tmp[16][j];   

    EEE[0] = EEEE[0] + EEEO[0];
    EEE[3] = EEEE[0] - EEEO[0];
    EEE[1] = EEEE[1] + EEEO[1];
    EEE[2] = EEEE[1] - EEEO[1];    

    for (int32 k=0;k<4;k++)
    {
      EE[k] = EEE[k] + EEO[k];
      EE[k+4] = EEE[3-k] - EEO[3-k];
    }    
    for (int32 k=0;k<8;k++)
    {
      E[k] = EE[k] + EO[k];
      E[k+8] = EE[7-k] - EO[7-k];
    }    
    for (int32 k=0;k<16;k++)
    {
      Dst[k]    = xClipS16((E[k] + O[k] + Add2nd)>>Shift2nd);
      Dst[k+16] = xClipS16((E[15-k] - O[15-k] + Add2nd)>>Shift2nd);
    }
    Dst += DstStride;
  }
}
void xTransform::InvTransformDCT_32x32_SSE(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth) //transpose->vertical transform->transpose->horizontal transform
{
  static const __m128i xiT32_DCT[128] = {
_mm_setr_epi16(64, 90, 90, 90, 89, 88, 87, 85),    _mm_setr_epi16(83, 82, 80, 78, 75, 73, 70, 67),      _mm_setr_epi16(64, 61, 57, 54, 50, 46, 43, 38),      _mm_setr_epi16(36, 31, 25, 22, 18, 13, 9, 4),
_mm_setr_epi16(64, 90, 87, 82, 75, 67, 57, 46),    _mm_setr_epi16(36, 22, 9, -4, -18, -31, -43, -54),    _mm_setr_epi16(-64, -73, -80, -85, -89, -90, -90, -88),  _mm_setr_epi16(-83, -78, -70, -61, -50, -38, -25, -13),
_mm_setr_epi16(64, 88, 80, 67, 50, 31, 9, -13),    _mm_setr_epi16(-36, -54, -70, -82, -89, -90, -87, -78),  _mm_setr_epi16(-64, -46, -25, -4, 18, 38, 57, 73),    _mm_setr_epi16(83, 90, 90, 85, 75, 61, 43, 22),
_mm_setr_epi16(64, 85, 70, 46, 18, -13, -43, -67),  _mm_setr_epi16(-83, -90, -87, -73, -50, -22, 9, 38),  _mm_setr_epi16(64, 82, 90, 88, 75, 54, 25, -4),      _mm_setr_epi16(-36, -61, -80, -90, -89, -78, -57, -31),
_mm_setr_epi16(64, 82, 57, 22, -18, -54, -80, -90),  _mm_setr_epi16(-83, -61, -25, 13, 50, 78, 90, 85),    _mm_setr_epi16(64, 31, -9, -46, -75, -90, -87, -67),  _mm_setr_epi16(-36, 4, 43, 73, 89, 88, 70, 38),
_mm_setr_epi16(64, 78, 43, -4, -50, -82, -90, -73),  _mm_setr_epi16(-36, 13, 57, 85, 89, 67, 25, -22),    _mm_setr_epi16(-64, -88, -87, -61, -18, 31, 70, 90),  _mm_setr_epi16(83, 54, 9, -38, -75, -90, -80, -46),
_mm_setr_epi16(64, 73, 25, -31, -75, -90, -70, -22),_mm_setr_epi16(36, 78, 90, 67, 18, -38, -80, -90),    _mm_setr_epi16(-64, -13, 43, 82, 89, 61, 9, -46),    _mm_setr_epi16(-83, -88, -57, -4, 50, 85, 87, 54),
_mm_setr_epi16(64, 67, 9, -54, -89, -78, -25, 38),  _mm_setr_epi16(83, 85, 43, -22, -75, -90, -57, 4),    _mm_setr_epi16(64, 90, 70, 13, -50, -88, -80, -31),    _mm_setr_epi16(36, 82, 87, 46, -18, -73, -90, -61),
_mm_setr_epi16(64, 61, -9, -73, -89, -46, 25, 82),  _mm_setr_epi16(83, 31, -43, -88, -75, -13, 57, 90),    _mm_setr_epi16(64, -4, -70, -90, -50, 22, 80, 85),    _mm_setr_epi16(36, -38, -87, -78, -18, 54, 90, 67),
_mm_setr_epi16(64, 54, -25, -85, -75, -4, 70, 88),  _mm_setr_epi16(36, -46, -90, -61, 18, 82, 80, 13),    _mm_setr_epi16(-64, -90, -43, 38, 89, 67, -9, -78),    _mm_setr_epi16(-83, -22, 57, 90, 50, -31, -87, -73),
_mm_setr_epi16(64, 46, -43, -90, -50, 38, 90, 54),  _mm_setr_epi16(-36, -90, -57, 31, 89, 61, -25, -88),  _mm_setr_epi16(-64, 22, 87, 67, -18, -85, -70, 13),    _mm_setr_epi16(83, 73, -9, -82, -75, 4, 80, 78),
_mm_setr_epi16(64, 38, -57, -88, -18, 73, 80, -4),  _mm_setr_epi16(-83, -67, 25, 90, 50, -46, -90, -31),  _mm_setr_epi16(64, 85, 9, -78, -75, 13, 87, 61),    _mm_setr_epi16(-36, -90, -43, 54, 89, 22, -70, -82),
_mm_setr_epi16(64, 31, -70, -78, 18, 90, 43, -61),  _mm_setr_epi16(-83, 4, 87, 54, -50, -88, -9, 82),    _mm_setr_epi16(64, -38, -90, -22, 75, 73, -25, -90),  _mm_setr_epi16(-36, 67, 80, -13, -89, -46, 57, 85),
_mm_setr_epi16(64, 22, -80, -61, 50, 85, -9, -90),  _mm_setr_epi16(-36, 73, 70, -38, -89, -4, 87, 46),    _mm_setr_epi16(-64, -78, 25, 90, 18, -82, -57, 54),    _mm_setr_epi16(83, -13, -90, -31, 75, 67, -43, -88),
_mm_setr_epi16(64, 13, -87, -38, 75, 61, -57, -78),  _mm_setr_epi16(36, 88, -9, -90, -18, 85, 43, -73),    _mm_setr_epi16(-64, 54, 80, -31, -89, 4, 90, 22),    _mm_setr_epi16(-83, -46, 70, 67, -50, -82, 25, 90),
_mm_setr_epi16(64, 4, -90, -13, 89, 22, -87, -31),  _mm_setr_epi16(83, 38, -80, -46, 75, 54, -70, -61),    _mm_setr_epi16(64, 67, -57, -73, 50, 78, -43, -82),    _mm_setr_epi16(36, 85, -25, -88, 18, 90, -9, -90),
_mm_setr_epi16(64, -4, -90, 13, 89, -22, -87, 31),  _mm_setr_epi16(83, -38, -80, 46, 75, -54, -70, 61),    _mm_setr_epi16(64, -67, -57, 73, 50, -78, -43, 82),    _mm_setr_epi16(36, -85, -25, 88, 18, -90, -9, 90),
_mm_setr_epi16(64, -13, -87, 38, 75, -61, -57, 78),  _mm_setr_epi16(36, -88, -9, 90, -18, -85, 43, 73),    _mm_setr_epi16(-64, -54, 80, 31, -89, -4, 90, -22),    _mm_setr_epi16(-83, 46, 70, -67, -50, 82, 25, -90),
_mm_setr_epi16(64, -22, -80, 61, 50, -85, -9, 90),  _mm_setr_epi16(-36, -73, 70, 38, -89, 4, 87, -46),    _mm_setr_epi16(-64, 78, 25, -90, 18, 82, -57, -54),    _mm_setr_epi16(83, 13, -90, 31, 75, -67, -43, 88),
_mm_setr_epi16(64, -31, -70, 78, 18, -90, 43, 61),  _mm_setr_epi16(-83, -4, 87, -54, -50, 88, -9, -82),    _mm_setr_epi16(64, 38, -90, 22, 75, -73, -25, 90),    _mm_setr_epi16(-36, -67, 80, 13, -89, 46, 57, -85),
_mm_setr_epi16(64, -38, -57, 88, -18, -73, 80, 4),  _mm_setr_epi16(-83, 67, 25, -90, 50, 46, -90, 31),    _mm_setr_epi16(64, -85, 9, 78, -75, -13, 87, -61),    _mm_setr_epi16(-36, 90, -43, -54, 89, -22, -70, 82),
_mm_setr_epi16(64, -46, -43, 90, -50, -38, 90, -54),_mm_setr_epi16(-36, 90, -57, -31, 89, -61, -25, 88),  _mm_setr_epi16(-64, -22, 87, -67, -18, 85, -70, -13),  _mm_setr_epi16(83, -73, -9, 82, -75, -4, 80, -78),
_mm_setr_epi16(64, -54, -25, 85, -75, 4, 70, -88),  _mm_setr_epi16(36, 46, -90, 61, 18, -82, 80, -13),    _mm_setr_epi16(-64, 90, -43, -38, 89, -67, -9, 78),    _mm_setr_epi16(-83, 22, 57, -90, 50, 31, -87, 73),
_mm_setr_epi16(64, -61, -9, 73, -89, 46, 25, -82),  _mm_setr_epi16(83, -31, -43, 88, -75, 13, 57, -90),    _mm_setr_epi16(64, 4, -70, 90, -50, -22, 80, -85),    _mm_setr_epi16(36, 38, -87, 78, -18, -54, 90, -67),
_mm_setr_epi16(64, -67, 9, 54, -89, 78, -25, -38),  _mm_setr_epi16(83, -85, 43, 22, -75, 90, -57, -4),    _mm_setr_epi16(64, -90, 70, -13, -50, 88, -80, 31),    _mm_setr_epi16(36, -82, 87, -46, -18, 73, -90, 61),
_mm_setr_epi16(64, -73, 25, 31, -75, 90, -70, 22),  _mm_setr_epi16(36, -78, 90, -67, 18, 38, -80, 90),    _mm_setr_epi16(-64, 13, 43, -82, 89, -61, 9, 46),    _mm_setr_epi16(-83, 88, -57, 4, 50, -85, 87, -54),
_mm_setr_epi16(64, -78, 43, 4, -50, 82, -90, 73),  _mm_setr_epi16(-36, -13, 57, -85, 89, -67, 25, 22),    _mm_setr_epi16(-64, 88, -87, 61, -18, -31, 70, -90),  _mm_setr_epi16(83, -54, 9, 38, -75, 90, -80, 46),
_mm_setr_epi16(64, -82, 57, -22, -18, 54, -80, 90),  _mm_setr_epi16(-83, 61, -25, -13, 50, -78, 90, -85),  _mm_setr_epi16(64, -31, -9, 46, -75, 90, -87, 67),    _mm_setr_epi16(-36, -4, 43, -73, 89, -88, 70, -38),
_mm_setr_epi16(64, -85, 70, -46, 18, 13, -43, 67),  _mm_setr_epi16(-83, 90, -87, 73, -50, 22, 9, -38),    _mm_setr_epi16(64, -82, 90, -88, 75, -54, 25, 4),    _mm_setr_epi16(-36, 61, -80, 90, -89, 78, -57, 31),
_mm_setr_epi16(64, -88, 80, -67, 50, -31, 9, 13),  _mm_setr_epi16(-36, 54, -70, 82, -89, 90, -87, 78),    _mm_setr_epi16(-64, 46, -25, 4, 18, -38, 57, -73),    _mm_setr_epi16(83, -90, 90, -85, 75, -61, 43, -22),
_mm_setr_epi16(64, -90, 87, -82, 75, -67, 57, -46),  _mm_setr_epi16(36, -22, 9, 4, -18, 31, -43, 54),    _mm_setr_epi16(-64, 73, -80, 85, -89, 90, -90, 88),    _mm_setr_epi16(-83, 78, -70, 61, -50, 38, -25, 13),
_mm_setr_epi16(64, -90, 90, -90, 89, -88, 87, -85),  _mm_setr_epi16(83, -82, 80, -78, 75, -73, 70, -67),    _mm_setr_epi16(64, -61, 57, -54, 50, -46, 43, -38),    _mm_setr_epi16(36, -31, 25, -22, 18, -13, 9, -4)};


  const int32 Shift2nd = 20 - BitDepth; 
  const __m128i Add1st  = _mm_set1_epi32(  64);
  const __m128i Add2nd  = _mm_set1_epi32(1<<(Shift2nd-1));
  __m128i tr[32];
  __m128i Tmp_A[128];
  __m128i Tmp_B[128];

  //load
  for(int32 j=0; j<128; j+=4)
  {
    Tmp_B[j  ] = _mm_loadu_si128((__m128i*)(Src  ));
    Tmp_B[j+1] = _mm_loadu_si128((__m128i*)(Src+8));
  Tmp_B[j+2] = _mm_loadu_si128((__m128i*)(Src+16));
    Tmp_B[j+3] = _mm_loadu_si128((__m128i*)(Src+24));
    Src += 32;   
  } 

 //transpose
  for(int32 i=0; i<64; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[64+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[64+i]);
  }

  for(int32 i=0; i<64; i++)
  {
    Tmp_B[2*i  ] = _mm_unpacklo_epi16(Tmp_A[0+i], Tmp_A[64+i]);
    Tmp_B[2*i+1] = _mm_unpackhi_epi16(Tmp_A[0+i], Tmp_A[64+i]);
  }

  for(int32 i=0; i<64; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[64+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[64+i]);
  }

  for(int32 i=0; i<64; i++)
  {
    Tmp_B[2*i  ] = _mm_unpacklo_epi16(Tmp_A[0+i], Tmp_A[64+i]);
    Tmp_B[2*i+1] = _mm_unpackhi_epi16(Tmp_A[0+i], Tmp_A[64+i]);
  }

  for(int32 i=0; i<64; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[64+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[64+i]);
  }
    //vertical transform
  for(int32 j=0; j<32; j++)
  {
    __m128i LineA = Tmp_A[4*j  ];
    __m128i LineB = Tmp_A[4*j+1];
  __m128i LineC = Tmp_A[4*j+2];
  __m128i LineD = Tmp_A[4*j+3];

    for(int32 i=0, k=0; i<128; i+=4, k++)
    {
    __m128i Sum1= _mm_add_epi32(_mm_madd_epi16(LineA, xiT32_DCT[i  ]), _mm_madd_epi16(LineB, xiT32_DCT[i+1]));
    __m128i Sum2= _mm_add_epi32(_mm_madd_epi16(LineC, xiT32_DCT[i+2]), _mm_madd_epi16(LineD, xiT32_DCT[i+3]));
    tr[k] = _mm_add_epi32(Sum1,Sum2);    
  }

    for(int32 i=0; i<16; i++)tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add1st), 7);

    Tmp_B[4*j  ] = _mm_packs_epi32(tr[0], tr[1]);
    Tmp_B[4*j+1] = _mm_packs_epi32(tr[2], tr[3]);
  Tmp_B[4*j+2] = _mm_packs_epi32(tr[4], tr[5]);
  Tmp_B[4*j+3] = _mm_packs_epi32(tr[6], tr[7]);
  }
  
  //transpose
  for(int32 i=0; i<64; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[64+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[64+i]);
  }

  for(int32 i=0; i<64; i++)
  {
    Tmp_B[2*i  ] = _mm_unpacklo_epi16(Tmp_A[0+i], Tmp_A[64+i]);
    Tmp_B[2*i+1] = _mm_unpackhi_epi16(Tmp_A[0+i], Tmp_A[64+i]);
  }

  for(int32 i=0; i<64; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[64+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[64+i]);
  }

  for(int32 i=0; i<64; i++)
  {
    Tmp_B[2*i  ] = _mm_unpacklo_epi16(Tmp_A[0+i], Tmp_A[64+i]);
    Tmp_B[2*i+1] = _mm_unpackhi_epi16(Tmp_A[0+i], Tmp_A[64+i]);
  }

  for(int32 i=0; i<64; i++)
  {
    Tmp_A[2*i  ] = _mm_unpacklo_epi16(Tmp_B[0+i], Tmp_B[64+i]);
    Tmp_A[2*i+1] = _mm_unpackhi_epi16(Tmp_B[0+i], Tmp_B[64+i]);
  }
   //horizontal transform
  for(int32 j=0; j<32; j++)
  {
    __m128i LineA = Tmp_A[4*j  ];
    __m128i LineB = Tmp_A[4*j+1];
  __m128i LineC = Tmp_A[4*j+2];
  __m128i LineD = Tmp_A[4*j+3];

    for(int32 i=0, k=0; i<128; i+=4, k++)
    {
    __m128i Sum1= _mm_add_epi32(_mm_madd_epi16(LineA, xiT32_DCT[i  ]), _mm_madd_epi16(LineB, xiT32_DCT[i+1]));
    __m128i Sum2= _mm_add_epi32(_mm_madd_epi16(LineC, xiT32_DCT[i+2]), _mm_madd_epi16(LineD, xiT32_DCT[i+3]));
    tr[k] = _mm_add_epi32(Sum1,Sum2);    
  }

    for(int32 i=0; i<16; i++)tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
  for(int32 i=0; i<8; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add2nd), Shift2nd);

  _mm_storeu_si128((__m128i*)(Dst   ), _mm_packs_epi32(tr[0], tr[1]));
    _mm_storeu_si128((__m128i*)(Dst+8 ), _mm_packs_epi32(tr[2], tr[3]));
  _mm_storeu_si128((__m128i*)(Dst+16), _mm_packs_epi32(tr[4], tr[5]));
    _mm_storeu_si128((__m128i*)(Dst+24), _mm_packs_epi32(tr[6], tr[7]));
    Dst += DstStride;
  }
}
void xTransform::InvTransformDCT_32x32_SSEv2(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth) //vertical transform 1 symmetry, horizontal transform 0 symmetries
{
  static const __m128i xiT32_DCT[128] = {
_mm_setr_epi16(64, 90, 90, 90, 89, 88, 87, 85),    _mm_setr_epi16(83, 82, 80, 78, 75, 73, 70, 67),      _mm_setr_epi16(64, 61, 57, 54, 50, 46, 43, 38),      _mm_setr_epi16(36, 31, 25, 22, 18, 13, 9, 4),
_mm_setr_epi16(64, 90, 87, 82, 75, 67, 57, 46),    _mm_setr_epi16(36, 22, 9, -4, -18, -31, -43, -54),    _mm_setr_epi16(-64, -73, -80, -85, -89, -90, -90, -88),  _mm_setr_epi16(-83, -78, -70, -61, -50, -38, -25, -13),
_mm_setr_epi16(64, 88, 80, 67, 50, 31, 9, -13),    _mm_setr_epi16(-36, -54, -70, -82, -89, -90, -87, -78),  _mm_setr_epi16(-64, -46, -25, -4, 18, 38, 57, 73),    _mm_setr_epi16(83, 90, 90, 85, 75, 61, 43, 22),
_mm_setr_epi16(64, 85, 70, 46, 18, -13, -43, -67),  _mm_setr_epi16(-83, -90, -87, -73, -50, -22, 9, 38),  _mm_setr_epi16(64, 82, 90, 88, 75, 54, 25, -4),      _mm_setr_epi16(-36, -61, -80, -90, -89, -78, -57, -31),
_mm_setr_epi16(64, 82, 57, 22, -18, -54, -80, -90),  _mm_setr_epi16(-83, -61, -25, 13, 50, 78, 90, 85),    _mm_setr_epi16(64, 31, -9, -46, -75, -90, -87, -67),  _mm_setr_epi16(-36, 4, 43, 73, 89, 88, 70, 38),
_mm_setr_epi16(64, 78, 43, -4, -50, -82, -90, -73),  _mm_setr_epi16(-36, 13, 57, 85, 89, 67, 25, -22),    _mm_setr_epi16(-64, -88, -87, -61, -18, 31, 70, 90),  _mm_setr_epi16(83, 54, 9, -38, -75, -90, -80, -46),
_mm_setr_epi16(64, 73, 25, -31, -75, -90, -70, -22),_mm_setr_epi16(36, 78, 90, 67, 18, -38, -80, -90),    _mm_setr_epi16(-64, -13, 43, 82, 89, 61, 9, -46),    _mm_setr_epi16(-83, -88, -57, -4, 50, 85, 87, 54),
_mm_setr_epi16(64, 67, 9, -54, -89, -78, -25, 38),  _mm_setr_epi16(83, 85, 43, -22, -75, -90, -57, 4),    _mm_setr_epi16(64, 90, 70, 13, -50, -88, -80, -31),    _mm_setr_epi16(36, 82, 87, 46, -18, -73, -90, -61),
_mm_setr_epi16(64, 61, -9, -73, -89, -46, 25, 82),  _mm_setr_epi16(83, 31, -43, -88, -75, -13, 57, 90),    _mm_setr_epi16(64, -4, -70, -90, -50, 22, 80, 85),    _mm_setr_epi16(36, -38, -87, -78, -18, 54, 90, 67),
_mm_setr_epi16(64, 54, -25, -85, -75, -4, 70, 88),  _mm_setr_epi16(36, -46, -90, -61, 18, 82, 80, 13),    _mm_setr_epi16(-64, -90, -43, 38, 89, 67, -9, -78),    _mm_setr_epi16(-83, -22, 57, 90, 50, -31, -87, -73),
_mm_setr_epi16(64, 46, -43, -90, -50, 38, 90, 54),  _mm_setr_epi16(-36, -90, -57, 31, 89, 61, -25, -88),  _mm_setr_epi16(-64, 22, 87, 67, -18, -85, -70, 13),    _mm_setr_epi16(83, 73, -9, -82, -75, 4, 80, 78),
_mm_setr_epi16(64, 38, -57, -88, -18, 73, 80, -4),  _mm_setr_epi16(-83, -67, 25, 90, 50, -46, -90, -31),  _mm_setr_epi16(64, 85, 9, -78, -75, 13, 87, 61),    _mm_setr_epi16(-36, -90, -43, 54, 89, 22, -70, -82),
_mm_setr_epi16(64, 31, -70, -78, 18, 90, 43, -61),  _mm_setr_epi16(-83, 4, 87, 54, -50, -88, -9, 82),    _mm_setr_epi16(64, -38, -90, -22, 75, 73, -25, -90),  _mm_setr_epi16(-36, 67, 80, -13, -89, -46, 57, 85),
_mm_setr_epi16(64, 22, -80, -61, 50, 85, -9, -90),  _mm_setr_epi16(-36, 73, 70, -38, -89, -4, 87, 46),    _mm_setr_epi16(-64, -78, 25, 90, 18, -82, -57, 54),    _mm_setr_epi16(83, -13, -90, -31, 75, 67, -43, -88),
_mm_setr_epi16(64, 13, -87, -38, 75, 61, -57, -78),  _mm_setr_epi16(36, 88, -9, -90, -18, 85, 43, -73),    _mm_setr_epi16(-64, 54, 80, -31, -89, 4, 90, 22),    _mm_setr_epi16(-83, -46, 70, 67, -50, -82, 25, 90),
_mm_setr_epi16(64, 4, -90, -13, 89, 22, -87, -31),  _mm_setr_epi16(83, 38, -80, -46, 75, 54, -70, -61),    _mm_setr_epi16(64, 67, -57, -73, 50, 78, -43, -82),    _mm_setr_epi16(36, 85, -25, -88, 18, 90, -9, -90),
_mm_setr_epi16(64, -4, -90, 13, 89, -22, -87, 31),  _mm_setr_epi16(83, -38, -80, 46, 75, -54, -70, 61),    _mm_setr_epi16(64, -67, -57, 73, 50, -78, -43, 82),    _mm_setr_epi16(36, -85, -25, 88, 18, -90, -9, 90),
_mm_setr_epi16(64, -13, -87, 38, 75, -61, -57, 78),  _mm_setr_epi16(36, -88, -9, 90, -18, -85, 43, 73),    _mm_setr_epi16(-64, -54, 80, 31, -89, -4, 90, -22),    _mm_setr_epi16(-83, 46, 70, -67, -50, 82, 25, -90),
_mm_setr_epi16(64, -22, -80, 61, 50, -85, -9, 90),  _mm_setr_epi16(-36, -73, 70, 38, -89, 4, 87, -46),    _mm_setr_epi16(-64, 78, 25, -90, 18, 82, -57, -54),    _mm_setr_epi16(83, 13, -90, 31, 75, -67, -43, 88),
_mm_setr_epi16(64, -31, -70, 78, 18, -90, 43, 61),  _mm_setr_epi16(-83, -4, 87, -54, -50, 88, -9, -82),    _mm_setr_epi16(64, 38, -90, 22, 75, -73, -25, 90),    _mm_setr_epi16(-36, -67, 80, 13, -89, 46, 57, -85),
_mm_setr_epi16(64, -38, -57, 88, -18, -73, 80, 4),  _mm_setr_epi16(-83, 67, 25, -90, 50, 46, -90, 31),    _mm_setr_epi16(64, -85, 9, 78, -75, -13, 87, -61),    _mm_setr_epi16(-36, 90, -43, -54, 89, -22, -70, 82),
_mm_setr_epi16(64, -46, -43, 90, -50, -38, 90, -54),_mm_setr_epi16(-36, 90, -57, -31, 89, -61, -25, 88),  _mm_setr_epi16(-64, -22, 87, -67, -18, 85, -70, -13),  _mm_setr_epi16(83, -73, -9, 82, -75, -4, 80, -78),
_mm_setr_epi16(64, -54, -25, 85, -75, 4, 70, -88),  _mm_setr_epi16(36, 46, -90, 61, 18, -82, 80, -13),    _mm_setr_epi16(-64, 90, -43, -38, 89, -67, -9, 78),    _mm_setr_epi16(-83, 22, 57, -90, 50, 31, -87, 73),
_mm_setr_epi16(64, -61, -9, 73, -89, 46, 25, -82),  _mm_setr_epi16(83, -31, -43, 88, -75, 13, 57, -90),    _mm_setr_epi16(64, 4, -70, 90, -50, -22, 80, -85),    _mm_setr_epi16(36, 38, -87, 78, -18, -54, 90, -67),
_mm_setr_epi16(64, -67, 9, 54, -89, 78, -25, -38),  _mm_setr_epi16(83, -85, 43, 22, -75, 90, -57, -4),    _mm_setr_epi16(64, -90, 70, -13, -50, 88, -80, 31),    _mm_setr_epi16(36, -82, 87, -46, -18, 73, -90, 61),
_mm_setr_epi16(64, -73, 25, 31, -75, 90, -70, 22),  _mm_setr_epi16(36, -78, 90, -67, 18, 38, -80, 90),    _mm_setr_epi16(-64, 13, 43, -82, 89, -61, 9, 46),    _mm_setr_epi16(-83, 88, -57, 4, 50, -85, 87, -54),
_mm_setr_epi16(64, -78, 43, 4, -50, 82, -90, 73),  _mm_setr_epi16(-36, -13, 57, -85, 89, -67, 25, 22),    _mm_setr_epi16(-64, 88, -87, 61, -18, -31, 70, -90),  _mm_setr_epi16(83, -54, 9, 38, -75, 90, -80, 46),
_mm_setr_epi16(64, -82, 57, -22, -18, 54, -80, 90),  _mm_setr_epi16(-83, 61, -25, -13, 50, -78, 90, -85),  _mm_setr_epi16(64, -31, -9, 46, -75, 90, -87, 67),    _mm_setr_epi16(-36, -4, 43, -73, 89, -88, 70, -38),
_mm_setr_epi16(64, -85, 70, -46, 18, 13, -43, 67),  _mm_setr_epi16(-83, 90, -87, 73, -50, 22, 9, -38),    _mm_setr_epi16(64, -82, 90, -88, 75, -54, 25, 4),    _mm_setr_epi16(-36, 61, -80, 90, -89, 78, -57, 31),
_mm_setr_epi16(64, -88, 80, -67, 50, -31, 9, 13),  _mm_setr_epi16(-36, 54, -70, 82, -89, 90, -87, 78),    _mm_setr_epi16(-64, 46, -25, 4, 18, -38, 57, -73),    _mm_setr_epi16(83, -90, 90, -85, 75, -61, 43, -22),
_mm_setr_epi16(64, -90, 87, -82, 75, -67, 57, -46),  _mm_setr_epi16(36, -22, 9, 4, -18, 31, -43, 54),    _mm_setr_epi16(-64, 73, -80, 85, -89, 90, -90, 88),    _mm_setr_epi16(-83, 78, -70, 61, -50, 38, -25, 13),
_mm_setr_epi16(64, -90, 90, -90, 89, -88, 87, -85),  _mm_setr_epi16(83, -82, 80, -78, 75, -73, 70, -67),    _mm_setr_epi16(64, -61, 57, -54, 50, -46, 43, -38),    _mm_setr_epi16(36, -31, 25, -22, 18, -13, 9, -4)};


  const int32 Shift2nd = 20 - BitDepth; 
  const __m128i Add1st  = _mm_set1_epi32(  64);
  const __m128i Add2nd  = _mm_set1_epi32(1<<(Shift2nd-1));
  __m128i tr[32];
  __m128i Tmp[128];
  __m128i Tmp_B[128];
  
  //load
  for(int32 j=0; j<128; j+=4)
  {
    Tmp[j  ] = _mm_loadu_si128((__m128i*)(Src  ));
    Tmp[j+1] = _mm_loadu_si128((__m128i*)(Src+8));
  Tmp[j+2] = _mm_loadu_si128((__m128i*)(Src+16));
  Tmp[j+3] = _mm_loadu_si128((__m128i*)(Src+24));
    Src += 32;   
  } 
   
  //vertical transform
  for(int32 i=0; i<16; i++)
  { 
  __m128i SumEA = _mm_setzero_si128();
    __m128i SumEB = _mm_setzero_si128();
    __m128i SumOA = _mm_setzero_si128();
    __m128i SumOB = _mm_setzero_si128();
    
  __m128i Sum2EA = _mm_setzero_si128();
    __m128i Sum2EB = _mm_setzero_si128();
  __m128i Sum2OA = _mm_setzero_si128();
    __m128i Sum2OB = _mm_setzero_si128();

  __m128i Sum3EA = _mm_setzero_si128();
    __m128i Sum3EB = _mm_setzero_si128();
    __m128i Sum3OA = _mm_setzero_si128();
    __m128i Sum3OB = _mm_setzero_si128();
    
  __m128i Sum4EA = _mm_setzero_si128();
    __m128i Sum4EB = _mm_setzero_si128();
  __m128i Sum4OA = _mm_setzero_si128();
    __m128i Sum4OB = _mm_setzero_si128();
  
    for(int32 j=0; j<128; j+=8)
    {
      //Even
      {
      __m128i Line1 = Tmp[j  ];
    __m128i Line2 = Tmp[j+1];
    __m128i Line3 = Tmp[j+2];
    __m128i Line4 = Tmp[j+3];

      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2A = _mm_cvtepi16_epi32(Line2);
      __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i Line3A = _mm_cvtepi16_epi32(Line3);
      __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line3,8));

    __m128i Line4A = _mm_cvtepi16_epi32(Line4);
      __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line4,8));
    
      __m128i TX = _mm_set1_epi32(m_TrM32x32[j>>2][i]);
    Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);

    Line2A = _mm_mullo_epi32(Line2A, TX);
      Line2B = _mm_mullo_epi32(Line2B, TX);
    
    Line3A = _mm_mullo_epi32(Line3A, TX);
      Line3B = _mm_mullo_epi32(Line3B, TX);

    Line4A = _mm_mullo_epi32(Line4A, TX);
      Line4B = _mm_mullo_epi32(Line4B, TX);

      SumEA = _mm_add_epi32(SumEA, Line1A);
      SumEB = _mm_add_epi32(SumEB, Line1B); 

    Sum2EA = _mm_add_epi32(Sum2EA, Line2A);
      Sum2EB = _mm_add_epi32(Sum2EB, Line2B); 
    
      Sum3EA = _mm_add_epi32(Sum3EA, Line3A);
      Sum3EB = _mm_add_epi32(Sum3EB, Line3B); 

    Sum4EA = _mm_add_epi32(Sum4EA, Line4A);
      Sum4EB = _mm_add_epi32(Sum4EB, Line4B); 
      }
      //Odd
      {
      __m128i Line1 = Tmp[j+4];
    __m128i Line2 = Tmp[j+5];
    __m128i Line3 = Tmp[j+6];
    __m128i Line4 = Tmp[j+7];

      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));
    
    __m128i Line2A = _mm_cvtepi16_epi32(Line2);
      __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i Line3A = _mm_cvtepi16_epi32(Line3);
      __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line3,8));
    
    __m128i Line4A = _mm_cvtepi16_epi32(Line4);
      __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line4,8));

      __m128i TX = _mm_set1_epi32(m_TrM32x32[(j>>2)+1][i]);

      Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);

    Line2A = _mm_mullo_epi32(Line2A, TX);
      Line2B = _mm_mullo_epi32(Line2B, TX);

    Line3A = _mm_mullo_epi32(Line3A, TX);
      Line3B = _mm_mullo_epi32(Line3B, TX);

    Line4A = _mm_mullo_epi32(Line4A, TX);
      Line4B = _mm_mullo_epi32(Line4B, TX);

      SumOA = _mm_add_epi32(SumOA, Line1A);
      SumOB = _mm_add_epi32(SumOB, Line1B); 

    Sum2OA = _mm_add_epi32(Sum2OA, Line2A);
      Sum2OB = _mm_add_epi32(Sum2OB, Line2B); 

    Sum3OA = _mm_add_epi32(Sum3OA, Line3A);
      Sum3OB = _mm_add_epi32(Sum3OB, Line3B); 

    Sum4OA = _mm_add_epi32(Sum4OA, Line4A);
      Sum4OB = _mm_add_epi32(Sum4OB, Line4B); 
      }
  }
  __m128i EpOA = _mm_add_epi32(SumEA, SumOA);
    __m128i EpOB = _mm_add_epi32(SumEB, SumOB);
    __m128i EmOA = _mm_sub_epi32(SumEA, SumOA);
    __m128i EmOB = _mm_sub_epi32(SumEB, SumOB);
  
  __m128i Ep2OA = _mm_add_epi32(Sum2EA, Sum2OA); 
    __m128i Ep2OB = _mm_add_epi32(Sum2EB, Sum2OB); 
    __m128i Em2OA = _mm_sub_epi32(Sum2EA, Sum2OA);
    __m128i Em2OB = _mm_sub_epi32(Sum2EB, Sum2OB);

  __m128i Ep3OA = _mm_add_epi32(Sum3EA, Sum3OA);
    __m128i Ep3OB = _mm_add_epi32(Sum3EB, Sum3OB);
    __m128i Em3OA = _mm_sub_epi32(Sum3EA, Sum3OA);
    __m128i Em3OB = _mm_sub_epi32(Sum3EB, Sum3OB);
  
  __m128i Ep4OA = _mm_add_epi32(Sum4EA, Sum4OA); 
    __m128i Ep4OB = _mm_add_epi32(Sum4EB, Sum4OB); 
    __m128i Em4OA = _mm_sub_epi32(Sum4EA, Sum4OA);
    __m128i Em4OB = _mm_sub_epi32(Sum4EB, Sum4OB);

    EpOA = _mm_srai_epi32(_mm_add_epi32(EpOA, Add1st),7);
    EpOB = _mm_srai_epi32(_mm_add_epi32(EpOB, Add1st),7);
    EmOA = _mm_srai_epi32(_mm_add_epi32(EmOA, Add1st),7);
    EmOB = _mm_srai_epi32(_mm_add_epi32(EmOB, Add1st),7);

  Ep2OA = _mm_srai_epi32(_mm_add_epi32(Ep2OA, Add1st),7);
    Ep2OB = _mm_srai_epi32(_mm_add_epi32(Ep2OB, Add1st),7);
    Em2OA = _mm_srai_epi32(_mm_add_epi32(Em2OA, Add1st),7);
    Em2OB = _mm_srai_epi32(_mm_add_epi32(Em2OB, Add1st),7);

  Ep3OA = _mm_srai_epi32(_mm_add_epi32(Ep3OA, Add1st),7);
    Ep3OB = _mm_srai_epi32(_mm_add_epi32(Ep3OB, Add1st),7);
    Em3OA = _mm_srai_epi32(_mm_add_epi32(Em3OA, Add1st),7);
    Em3OB = _mm_srai_epi32(_mm_add_epi32(Em3OB, Add1st),7);

  Ep4OA = _mm_srai_epi32(_mm_add_epi32(Ep4OA, Add1st),7);
    Ep4OB = _mm_srai_epi32(_mm_add_epi32(Ep4OB, Add1st),7);
    Em4OA = _mm_srai_epi32(_mm_add_epi32(Em4OA, Add1st),7);
    Em4OB = _mm_srai_epi32(_mm_add_epi32(Em4OB, Add1st),7);

    Tmp_B[4*i   ]= _mm_packs_epi32(EpOA, EpOB);
  Tmp_B[4*i+1 ] = _mm_packs_epi32(Ep2OA, Ep2OB);
  Tmp_B[4*i+2 ]= _mm_packs_epi32(Ep3OA, Ep3OB);
  Tmp_B[4*i+3 ] = _mm_packs_epi32(Ep4OA, Ep4OB);

    Tmp_B[124-4*i  ]= _mm_packs_epi32(EmOA, EmOB);  
  Tmp_B[124-4*i+1] = _mm_packs_epi32(Em2OA, Em2OB);  
  Tmp_B[124-4*i+2]= _mm_packs_epi32(Em3OA, Em3OB);  
  Tmp_B[124-4*i+3] = _mm_packs_epi32(Em4OA, Em4OB); 
   }

   //horizontal transform and store
  for(int32 j=0; j<32; j++)
  {
    __m128i LineA = Tmp_B[4*j];
    __m128i LineB = Tmp_B[4*j+1];
  __m128i LineC = Tmp_B[4*j+2];
    __m128i LineD = Tmp_B[4*j+3];

  /*  for(int32 i=0, int32 k=0; i<32; i+=2, k++)
    {
      tr[k] = _mm_add_epi32(_mm_madd_epi16(LineA, xiT16_DCT[i  ]), _mm_madd_epi16(LineB, xiT16_DCT[i+1]));
    }*/
  
    for(int32 i=0, k=0; i<128; i+=4, k++)
    {
    tr[k ] =_mm_add_epi32( _mm_add_epi32(_mm_madd_epi16(LineA, xiT32_DCT[i]), _mm_madd_epi16(LineB, xiT32_DCT[i+1])),_mm_add_epi32(_mm_madd_epi16(LineC, xiT32_DCT[i+2]), _mm_madd_epi16(LineD, xiT32_DCT[i+3])) );  
    }

    for(int32 i=0; i<16; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add2nd), Shift2nd);
  
    __m128i tr1=  _mm_packs_epi32(tr[0], tr[1]);
  __m128i tr2= _mm_packs_epi32(tr[2], tr[3]);
  __m128i tr3=  _mm_packs_epi32(tr[4], tr[5]);
  __m128i tr4= _mm_packs_epi32(tr[6], tr[7]);
  _mm_storeu_si128((__m128i*)Dst, tr1);
  _mm_storeu_si128((__m128i*)(Dst+8), tr2);
  _mm_storeu_si128((__m128i*)(Dst+16), tr3);
  _mm_storeu_si128((__m128i*)(Dst+24), tr4);
    Dst += DstStride; 
  }
}
void xTransform::InvTransformDCT_32x32_SSEv3(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth)//verical transform 2 symmetries, horizontal transform 0 symmetries
{
  
 static const __m128i xiT32_DCT[128] = {
_mm_setr_epi16(64, 90, 90, 90, 89, 88, 87, 85),    _mm_setr_epi16(83, 82, 80, 78, 75, 73, 70, 67),      _mm_setr_epi16(64, 61, 57, 54, 50, 46, 43, 38),      _mm_setr_epi16(36, 31, 25, 22, 18, 13, 9, 4),
_mm_setr_epi16(64, 90, 87, 82, 75, 67, 57, 46),    _mm_setr_epi16(36, 22, 9, -4, -18, -31, -43, -54),    _mm_setr_epi16(-64, -73, -80, -85, -89, -90, -90, -88),  _mm_setr_epi16(-83, -78, -70, -61, -50, -38, -25, -13),
_mm_setr_epi16(64, 88, 80, 67, 50, 31, 9, -13),    _mm_setr_epi16(-36, -54, -70, -82, -89, -90, -87, -78),  _mm_setr_epi16(-64, -46, -25, -4, 18, 38, 57, 73),    _mm_setr_epi16(83, 90, 90, 85, 75, 61, 43, 22),
_mm_setr_epi16(64, 85, 70, 46, 18, -13, -43, -67),  _mm_setr_epi16(-83, -90, -87, -73, -50, -22, 9, 38),  _mm_setr_epi16(64, 82, 90, 88, 75, 54, 25, -4),      _mm_setr_epi16(-36, -61, -80, -90, -89, -78, -57, -31),
_mm_setr_epi16(64, 82, 57, 22, -18, -54, -80, -90),  _mm_setr_epi16(-83, -61, -25, 13, 50, 78, 90, 85),    _mm_setr_epi16(64, 31, -9, -46, -75, -90, -87, -67),  _mm_setr_epi16(-36, 4, 43, 73, 89, 88, 70, 38),
_mm_setr_epi16(64, 78, 43, -4, -50, -82, -90, -73),  _mm_setr_epi16(-36, 13, 57, 85, 89, 67, 25, -22),    _mm_setr_epi16(-64, -88, -87, -61, -18, 31, 70, 90),  _mm_setr_epi16(83, 54, 9, -38, -75, -90, -80, -46),
_mm_setr_epi16(64, 73, 25, -31, -75, -90, -70, -22),_mm_setr_epi16(36, 78, 90, 67, 18, -38, -80, -90),    _mm_setr_epi16(-64, -13, 43, 82, 89, 61, 9, -46),    _mm_setr_epi16(-83, -88, -57, -4, 50, 85, 87, 54),
_mm_setr_epi16(64, 67, 9, -54, -89, -78, -25, 38),  _mm_setr_epi16(83, 85, 43, -22, -75, -90, -57, 4),    _mm_setr_epi16(64, 90, 70, 13, -50, -88, -80, -31),    _mm_setr_epi16(36, 82, 87, 46, -18, -73, -90, -61),
_mm_setr_epi16(64, 61, -9, -73, -89, -46, 25, 82),  _mm_setr_epi16(83, 31, -43, -88, -75, -13, 57, 90),    _mm_setr_epi16(64, -4, -70, -90, -50, 22, 80, 85),    _mm_setr_epi16(36, -38, -87, -78, -18, 54, 90, 67),
_mm_setr_epi16(64, 54, -25, -85, -75, -4, 70, 88),  _mm_setr_epi16(36, -46, -90, -61, 18, 82, 80, 13),    _mm_setr_epi16(-64, -90, -43, 38, 89, 67, -9, -78),    _mm_setr_epi16(-83, -22, 57, 90, 50, -31, -87, -73),
_mm_setr_epi16(64, 46, -43, -90, -50, 38, 90, 54),  _mm_setr_epi16(-36, -90, -57, 31, 89, 61, -25, -88),  _mm_setr_epi16(-64, 22, 87, 67, -18, -85, -70, 13),    _mm_setr_epi16(83, 73, -9, -82, -75, 4, 80, 78),
_mm_setr_epi16(64, 38, -57, -88, -18, 73, 80, -4),  _mm_setr_epi16(-83, -67, 25, 90, 50, -46, -90, -31),  _mm_setr_epi16(64, 85, 9, -78, -75, 13, 87, 61),    _mm_setr_epi16(-36, -90, -43, 54, 89, 22, -70, -82),
_mm_setr_epi16(64, 31, -70, -78, 18, 90, 43, -61),  _mm_setr_epi16(-83, 4, 87, 54, -50, -88, -9, 82),    _mm_setr_epi16(64, -38, -90, -22, 75, 73, -25, -90),  _mm_setr_epi16(-36, 67, 80, -13, -89, -46, 57, 85),
_mm_setr_epi16(64, 22, -80, -61, 50, 85, -9, -90),  _mm_setr_epi16(-36, 73, 70, -38, -89, -4, 87, 46),    _mm_setr_epi16(-64, -78, 25, 90, 18, -82, -57, 54),    _mm_setr_epi16(83, -13, -90, -31, 75, 67, -43, -88),
_mm_setr_epi16(64, 13, -87, -38, 75, 61, -57, -78),  _mm_setr_epi16(36, 88, -9, -90, -18, 85, 43, -73),    _mm_setr_epi16(-64, 54, 80, -31, -89, 4, 90, 22),    _mm_setr_epi16(-83, -46, 70, 67, -50, -82, 25, 90),
_mm_setr_epi16(64, 4, -90, -13, 89, 22, -87, -31),  _mm_setr_epi16(83, 38, -80, -46, 75, 54, -70, -61),    _mm_setr_epi16(64, 67, -57, -73, 50, 78, -43, -82),    _mm_setr_epi16(36, 85, -25, -88, 18, 90, -9, -90),
_mm_setr_epi16(64, -4, -90, 13, 89, -22, -87, 31),  _mm_setr_epi16(83, -38, -80, 46, 75, -54, -70, 61),    _mm_setr_epi16(64, -67, -57, 73, 50, -78, -43, 82),    _mm_setr_epi16(36, -85, -25, 88, 18, -90, -9, 90),
_mm_setr_epi16(64, -13, -87, 38, 75, -61, -57, 78),  _mm_setr_epi16(36, -88, -9, 90, -18, -85, 43, 73),    _mm_setr_epi16(-64, -54, 80, 31, -89, -4, 90, -22),    _mm_setr_epi16(-83, 46, 70, -67, -50, 82, 25, -90),
_mm_setr_epi16(64, -22, -80, 61, 50, -85, -9, 90),  _mm_setr_epi16(-36, -73, 70, 38, -89, 4, 87, -46),    _mm_setr_epi16(-64, 78, 25, -90, 18, 82, -57, -54),    _mm_setr_epi16(83, 13, -90, 31, 75, -67, -43, 88),
_mm_setr_epi16(64, -31, -70, 78, 18, -90, 43, 61),  _mm_setr_epi16(-83, -4, 87, -54, -50, 88, -9, -82),    _mm_setr_epi16(64, 38, -90, 22, 75, -73, -25, 90),    _mm_setr_epi16(-36, -67, 80, 13, -89, 46, 57, -85),
_mm_setr_epi16(64, -38, -57, 88, -18, -73, 80, 4),  _mm_setr_epi16(-83, 67, 25, -90, 50, 46, -90, 31),    _mm_setr_epi16(64, -85, 9, 78, -75, -13, 87, -61),    _mm_setr_epi16(-36, 90, -43, -54, 89, -22, -70, 82),
_mm_setr_epi16(64, -46, -43, 90, -50, -38, 90, -54),_mm_setr_epi16(-36, 90, -57, -31, 89, -61, -25, 88),  _mm_setr_epi16(-64, -22, 87, -67, -18, 85, -70, -13),  _mm_setr_epi16(83, -73, -9, 82, -75, -4, 80, -78),
_mm_setr_epi16(64, -54, -25, 85, -75, 4, 70, -88),  _mm_setr_epi16(36, 46, -90, 61, 18, -82, 80, -13),    _mm_setr_epi16(-64, 90, -43, -38, 89, -67, -9, 78),    _mm_setr_epi16(-83, 22, 57, -90, 50, 31, -87, 73),
_mm_setr_epi16(64, -61, -9, 73, -89, 46, 25, -82),  _mm_setr_epi16(83, -31, -43, 88, -75, 13, 57, -90),    _mm_setr_epi16(64, 4, -70, 90, -50, -22, 80, -85),    _mm_setr_epi16(36, 38, -87, 78, -18, -54, 90, -67),
_mm_setr_epi16(64, -67, 9, 54, -89, 78, -25, -38),  _mm_setr_epi16(83, -85, 43, 22, -75, 90, -57, -4),    _mm_setr_epi16(64, -90, 70, -13, -50, 88, -80, 31),    _mm_setr_epi16(36, -82, 87, -46, -18, 73, -90, 61),
_mm_setr_epi16(64, -73, 25, 31, -75, 90, -70, 22),  _mm_setr_epi16(36, -78, 90, -67, 18, 38, -80, 90),    _mm_setr_epi16(-64, 13, 43, -82, 89, -61, 9, 46),    _mm_setr_epi16(-83, 88, -57, 4, 50, -85, 87, -54),
_mm_setr_epi16(64, -78, 43, 4, -50, 82, -90, 73),  _mm_setr_epi16(-36, -13, 57, -85, 89, -67, 25, 22),    _mm_setr_epi16(-64, 88, -87, 61, -18, -31, 70, -90),  _mm_setr_epi16(83, -54, 9, 38, -75, 90, -80, 46),
_mm_setr_epi16(64, -82, 57, -22, -18, 54, -80, 90),  _mm_setr_epi16(-83, 61, -25, -13, 50, -78, 90, -85),  _mm_setr_epi16(64, -31, -9, 46, -75, 90, -87, 67),    _mm_setr_epi16(-36, -4, 43, -73, 89, -88, 70, -38),
_mm_setr_epi16(64, -85, 70, -46, 18, 13, -43, 67),  _mm_setr_epi16(-83, 90, -87, 73, -50, 22, 9, -38),    _mm_setr_epi16(64, -82, 90, -88, 75, -54, 25, 4),    _mm_setr_epi16(-36, 61, -80, 90, -89, 78, -57, 31),
_mm_setr_epi16(64, -88, 80, -67, 50, -31, 9, 13),  _mm_setr_epi16(-36, 54, -70, 82, -89, 90, -87, 78),    _mm_setr_epi16(-64, 46, -25, 4, 18, -38, 57, -73),    _mm_setr_epi16(83, -90, 90, -85, 75, -61, 43, -22),
_mm_setr_epi16(64, -90, 87, -82, 75, -67, 57, -46),  _mm_setr_epi16(36, -22, 9, 4, -18, 31, -43, 54),    _mm_setr_epi16(-64, 73, -80, 85, -89, 90, -90, 88),    _mm_setr_epi16(-83, 78, -70, 61, -50, 38, -25, 13),
_mm_setr_epi16(64, -90, 90, -90, 89, -88, 87, -85),  _mm_setr_epi16(83, -82, 80, -78, 75, -73, 70, -67),    _mm_setr_epi16(64, -61, 57, -54, 50, -46, 43, -38),    _mm_setr_epi16(36, -31, 25, -22, 18, -13, 9, -4)};


  const int32 Shift2nd = 20 - BitDepth; 
  const __m128i Add1st  = _mm_set1_epi32(  64);
  const __m128i Add2nd  = _mm_set1_epi32(1<<(Shift2nd-1));

  __m128i Tmp[128];
  __m128i Tmp2[128];
  __m128i tr[32];
  //load
  for(int32 j=0; j<128; j+=4)
  {
    Tmp[j  ] = _mm_loadu_si128((__m128i*)(Src  ));
    Tmp[j+1] = _mm_loadu_si128((__m128i*)(Src+8));
  Tmp[j+2] = _mm_loadu_si128((__m128i*)(Src+16));
  Tmp[j+3] = _mm_loadu_si128((__m128i*)(Src+24));
    Src += 32;   
  } 
  
  //vertical transform  
  for(int32 i=0; i<8; i++) //per O and E
  {   
    __m128i SumEAe = _mm_setzero_si128();//even lines + +
    __m128i SumEBe = _mm_setzero_si128();
  __m128i Sum2EAe = _mm_setzero_si128();
    __m128i Sum2EBe = _mm_setzero_si128();
  __m128i Sum3EAe = _mm_setzero_si128();
    __m128i Sum3EBe = _mm_setzero_si128();
  __m128i Sum4EAe = _mm_setzero_si128();
    __m128i Sum4EBe = _mm_setzero_si128();

    __m128i SumOA = _mm_setzero_si128();//odd lines
    __m128i SumOB = _mm_setzero_si128();
  __m128i Sum2OA = _mm_setzero_si128();
    __m128i Sum2OB = _mm_setzero_si128();
  __m128i Sum3OA = _mm_setzero_si128();
    __m128i Sum3OB = _mm_setzero_si128();
  __m128i Sum4OA = _mm_setzero_si128();
    __m128i Sum4OB = _mm_setzero_si128();

  __m128i SumOAm = _mm_setzero_si128();
    __m128i SumOBm = _mm_setzero_si128();
  __m128i Sum2OAm = _mm_setzero_si128();
    __m128i Sum2OBm = _mm_setzero_si128();
  __m128i Sum3OAm = _mm_setzero_si128();
    __m128i Sum3OBm = _mm_setzero_si128();
  __m128i Sum4OAm = _mm_setzero_si128();
    __m128i Sum4OBm = _mm_setzero_si128();

  __m128i SumEAo = _mm_setzero_si128();//even lines + -
    __m128i SumEBo = _mm_setzero_si128();
  __m128i Sum2EAo = _mm_setzero_si128();
    __m128i Sum2EBo = _mm_setzero_si128();
  __m128i Sum3EAo = _mm_setzero_si128();
    __m128i Sum3EBo = _mm_setzero_si128();
  __m128i Sum4EAo = _mm_setzero_si128();
    __m128i Sum4EBo = _mm_setzero_si128();

    for(int32 j=0; j<128; j+=16)
    {
      //Even Even + +
      {
      __m128i Line1 = Tmp[j  ];
      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+1];
      __m128i Line2A = _mm_cvtepi16_epi32(Line2);
      __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i Line3 = Tmp[j+2];
      __m128i Line3A = _mm_cvtepi16_epi32(Line3);
      __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line3,8));

    __m128i Line4 = Tmp[j+3];
      __m128i Line4A = _mm_cvtepi16_epi32(Line4);
      __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line4,8));

      __m128i TX = _mm_set1_epi32(m_TrM32x32[j>>2][i]);

      Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);

    Line2A = _mm_mullo_epi32(Line2A, TX);
      Line2B = _mm_mullo_epi32(Line2B, TX);

    Line3A = _mm_mullo_epi32(Line3A, TX);
      Line3B = _mm_mullo_epi32(Line3B, TX);

    Line4A = _mm_mullo_epi32(Line4A, TX);
      Line4B = _mm_mullo_epi32(Line4B, TX);

      SumEAe = _mm_add_epi32(SumEAe, Line1A);
      SumEBe = _mm_add_epi32(SumEBe, Line1B);  

    Sum2EAe = _mm_add_epi32(Sum2EAe, Line2A);
      Sum2EBe = _mm_add_epi32(Sum2EBe, Line2B);  

    Sum3EAe = _mm_add_epi32(Sum3EAe, Line3A);
      Sum3EBe = _mm_add_epi32(Sum3EBe, Line3B);  

    Sum4EAe = _mm_add_epi32(Sum4EAe, Line4A);
      Sum4EBe = _mm_add_epi32(Sum4EBe, Line4B);  
      }
      //Odd x y
      {
      __m128i Line1 = Tmp[j+4 ];
      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+5 ];
      __m128i Line2A = _mm_cvtepi16_epi32(Line2);
      __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i Line3 = Tmp[j+6 ];
      __m128i Line3A = _mm_cvtepi16_epi32(Line3);
      __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line3,8));

    __m128i Line4 = Tmp[j+7 ];
      __m128i Line4A = _mm_cvtepi16_epi32(Line4);
      __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line4,8));
      
    __m128i TX = _mm_set1_epi32(m_TrM32x32[(j>>2)+1][i]);
    __m128i TXm= _mm_set1_epi32(m_TrM32x32[(j>>2)+1][15-i]);

    __m128i Line1Am = _mm_mullo_epi32(Line1A, TXm);
      __m128i Line1Bm = _mm_mullo_epi32(Line1B, TXm);
      Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);

    __m128i Line2Am = _mm_mullo_epi32(Line2A, TXm);
      __m128i Line2Bm = _mm_mullo_epi32(Line2B, TXm);
      Line2A = _mm_mullo_epi32(Line2A, TX);
      Line2B = _mm_mullo_epi32(Line2B, TX);

    __m128i Line3Am = _mm_mullo_epi32(Line3A, TXm);
      __m128i Line3Bm = _mm_mullo_epi32(Line3B, TXm);
      Line3A = _mm_mullo_epi32(Line3A, TX);
      Line3B = _mm_mullo_epi32(Line3B, TX);

    __m128i Line4Am = _mm_mullo_epi32(Line4A, TXm);
      __m128i Line4Bm = _mm_mullo_epi32(Line4B, TXm);
      Line4A = _mm_mullo_epi32(Line4A, TX);
      Line4B = _mm_mullo_epi32(Line4B, TX);
    
      SumOA = _mm_add_epi32(SumOA, Line1A);
      SumOB = _mm_add_epi32(SumOB, Line1B); 

    SumOAm = _mm_add_epi32(SumOAm, Line1Am);
      SumOBm = _mm_add_epi32(SumOBm, Line1Bm); 

    Sum2OA = _mm_add_epi32(Sum2OA, Line2A);
      Sum2OB = _mm_add_epi32(Sum2OB, Line2B); 

    Sum2OAm = _mm_add_epi32(Sum2OAm, Line2Am);
      Sum2OBm = _mm_add_epi32(Sum2OBm, Line2Bm); 

    Sum3OA = _mm_add_epi32(Sum3OA, Line3A);
      Sum3OB = _mm_add_epi32(Sum3OB, Line3B); 

    Sum3OAm = _mm_add_epi32(Sum3OAm, Line3Am);
      Sum3OBm = _mm_add_epi32(Sum3OBm, Line3Bm); 

    Sum4OA = _mm_add_epi32(Sum4OA, Line4A);
      Sum4OB = _mm_add_epi32(Sum4OB, Line4B); 

    Sum4OAm = _mm_add_epi32(Sum4OAm, Line4Am);
      Sum4OBm = _mm_add_epi32(Sum4OBm, Line4Bm); 
      }
    //Even odd + -
    {
      __m128i Line1 = Tmp[j+8 ];
      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+9 ];
      __m128i Line2A = _mm_cvtepi16_epi32(Line2);
      __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i Line3 = Tmp[j+10];
      __m128i Line3A = _mm_cvtepi16_epi32(Line3);
      __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line3,8));

    __m128i Line4 = Tmp[j+11];
      __m128i Line4A = _mm_cvtepi16_epi32(Line4);
      __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line4,8));

      __m128i TX = _mm_set1_epi32(m_TrM32x32[(j>>2)+2][i]);
      Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);
      SumEAo = _mm_add_epi32(SumEAo, Line1A);
      SumEBo = _mm_add_epi32(SumEBo, Line1B);  

    Line2A = _mm_mullo_epi32(Line2A, TX);
      Line2B = _mm_mullo_epi32(Line2B, TX);
      Sum2EAo = _mm_add_epi32(Sum2EAo, Line2A);
      Sum2EBo = _mm_add_epi32(Sum2EBo, Line2B);

    Line3A = _mm_mullo_epi32(Line3A, TX);
      Line3B = _mm_mullo_epi32(Line3B, TX);
      Sum3EAo = _mm_add_epi32(Sum3EAo, Line3A);
      Sum3EBo = _mm_add_epi32(Sum3EBo, Line3B);  

    Line4A = _mm_mullo_epi32(Line4A, TX);
      Line4B = _mm_mullo_epi32(Line4B, TX);
      Sum4EAo = _mm_add_epi32(Sum4EAo, Line4A);
      Sum4EBo = _mm_add_epi32(Sum4EBo, Line4B);
    }
    //Odd x y
      {
      __m128i Line1 = Tmp[j+12 ];
      __m128i Line1A = _mm_cvtepi16_epi32(Line1);
      __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+13];
      __m128i Line2A = _mm_cvtepi16_epi32(Line2);
      __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i Line3 = Tmp[j+14];
      __m128i Line3A = _mm_cvtepi16_epi32(Line3);
      __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line3,8));

    __m128i Line4 = Tmp[j+15];
      __m128i Line4A = _mm_cvtepi16_epi32(Line4);
      __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line4,8));

      __m128i TX = _mm_set1_epi32(m_TrM32x32[(j>>2)+3][i]);
    __m128i TXm= _mm_set1_epi32(m_TrM32x32[(j>>2)+3][15-i]);

    __m128i Line1Am = _mm_mullo_epi32(Line1A, TXm);
      __m128i Line1Bm = _mm_mullo_epi32(Line1B, TXm);
      Line1A = _mm_mullo_epi32(Line1A, TX);
      Line1B = _mm_mullo_epi32(Line1B, TX);
    
    __m128i Line2Am = _mm_mullo_epi32(Line2A, TXm);
      __m128i Line2Bm = _mm_mullo_epi32(Line2B, TXm);
      Line2A = _mm_mullo_epi32(Line2A, TX);
      Line2B = _mm_mullo_epi32(Line2B, TX);

    __m128i Line3Am = _mm_mullo_epi32(Line3A, TXm);
      __m128i Line3Bm = _mm_mullo_epi32(Line3B, TXm);
      Line3A = _mm_mullo_epi32(Line3A, TX);
      Line3B = _mm_mullo_epi32(Line3B, TX);
    
    __m128i Line4Am = _mm_mullo_epi32(Line4A, TXm);
      __m128i Line4Bm = _mm_mullo_epi32(Line4B, TXm);
      Line4A = _mm_mullo_epi32(Line4A, TX);
      Line4B = _mm_mullo_epi32(Line4B, TX);

      SumOA = _mm_add_epi32(SumOA, Line1A);
      SumOB = _mm_add_epi32(SumOB, Line1B); 

    SumOAm = _mm_add_epi32(SumOAm, Line1Am);
      SumOBm = _mm_add_epi32(SumOBm, Line1Bm); 

    Sum2OA = _mm_add_epi32(Sum2OA, Line2A);
      Sum2OB = _mm_add_epi32(Sum2OB, Line2B); 

    Sum2OAm = _mm_add_epi32(Sum2OAm, Line2Am);
      Sum2OBm = _mm_add_epi32(Sum2OBm, Line2Bm); 

    Sum3OA = _mm_add_epi32(Sum3OA, Line3A);
      Sum3OB = _mm_add_epi32(Sum3OB, Line3B); 

    Sum3OAm = _mm_add_epi32(Sum3OAm, Line3Am);
      Sum3OBm = _mm_add_epi32(Sum3OBm, Line3Bm); 

    Sum4OA = _mm_add_epi32(Sum4OA, Line4A);
      Sum4OB = _mm_add_epi32(Sum4OB, Line4B); 

    Sum4OAm = _mm_add_epi32(Sum4OAm, Line4Am);
      Sum4OBm = _mm_add_epi32(Sum4OBm, Line4Bm); 
      }
    }
  
  __m128i SumEAeo=_mm_add_epi32(SumEAe,SumEAo);
  __m128i SumEBeo=_mm_add_epi32(SumEBe,SumEBo);
  __m128i Sum2EAeo=_mm_add_epi32(Sum2EAe,Sum2EAo);
  __m128i Sum2EBeo=_mm_add_epi32(Sum2EBe,Sum2EBo);
  __m128i Sum3EAeo=_mm_add_epi32(Sum3EAe,Sum3EAo);
  __m128i Sum3EBeo=_mm_add_epi32(Sum3EBe,Sum3EBo);
  __m128i Sum4EAeo=_mm_add_epi32(Sum4EAe,Sum4EAo);
  __m128i Sum4EBeo=_mm_add_epi32(Sum4EBe,Sum4EBo);

  __m128i DifEAeo=_mm_sub_epi32(SumEAe,SumEAo);
  __m128i DifEBeo=_mm_sub_epi32(SumEBe,SumEBo);
  __m128i Dif2EAeo=_mm_sub_epi32(Sum2EAe,Sum2EAo);
  __m128i Dif2EBeo=_mm_sub_epi32(Sum2EBe,Sum2EBo);
  __m128i Dif3EAeo=_mm_sub_epi32(Sum3EAe,Sum3EAo);
  __m128i Dif3EBeo=_mm_sub_epi32(Sum3EBe,Sum3EBo);
  __m128i Dif4EAeo=_mm_sub_epi32(Sum4EAe,Sum4EAo);
  __m128i Dif4EBeo=_mm_sub_epi32(Sum4EBe,Sum4EBo);

  //line0
    __m128i EpOA = _mm_add_epi32(SumEAeo,SumOA);
    __m128i EpOB = _mm_add_epi32(SumEBeo,SumOB);
  __m128i Ep2OA = _mm_add_epi32(Sum2EAeo,Sum2OA);
    __m128i Ep2OB = _mm_add_epi32(Sum2EBeo,Sum2OB);
  __m128i Ep3OA = _mm_add_epi32(Sum3EAeo,Sum3OA);
    __m128i Ep3OB = _mm_add_epi32(Sum3EBeo,Sum3OB);
  __m128i Ep4OA = _mm_add_epi32(Sum4EAeo,Sum4OA);
    __m128i Ep4OB = _mm_add_epi32(Sum4EBeo,Sum4OB);
   
  //line15
  __m128i ElOA = _mm_add_epi32(DifEAeo,SumOAm);
  __m128i ElOB = _mm_add_epi32(DifEBeo,SumOBm);
  __m128i El2OA = _mm_add_epi32(Dif2EAeo,Sum2OAm);
  __m128i El2OB = _mm_add_epi32(Dif2EBeo,Sum2OBm);
  __m128i El3OA = _mm_add_epi32(Dif3EAeo,Sum3OAm);
  __m128i El3OB = _mm_add_epi32(Dif3EBeo,Sum3OBm);
  __m128i El4OA = _mm_add_epi32(Dif4EAeo,Sum4OAm);
  __m128i El4OB = _mm_add_epi32(Dif4EBeo,Sum4OBm);
  //line16
  __m128i EkOA = _mm_sub_epi32(DifEAeo,SumOAm);
  __m128i EkOB = _mm_sub_epi32(DifEBeo,SumOBm);
  __m128i Ek2OA = _mm_sub_epi32(Dif2EAeo,Sum2OAm);
  __m128i Ek2OB = _mm_sub_epi32(Dif2EBeo,Sum2OBm);
  __m128i Ek3OA = _mm_sub_epi32(Dif3EAeo,Sum3OAm);
  __m128i Ek3OB = _mm_sub_epi32(Dif3EBeo,Sum3OBm);
  __m128i Ek4OA = _mm_sub_epi32(Dif4EAeo,Sum4OAm);
  __m128i Ek4OB = _mm_sub_epi32(Dif4EBeo,Sum4OBm);
   //line31
  __m128i EmOA = _mm_sub_epi32(SumEAeo, SumOA);
    __m128i EmOB = _mm_sub_epi32(SumEBeo, SumOB);
  __m128i Em2OA = _mm_sub_epi32(Sum2EAeo, Sum2OA);
    __m128i Em2OB = _mm_sub_epi32(Sum2EBeo, Sum2OB);
  __m128i Em3OA = _mm_sub_epi32(Sum3EAeo, Sum3OA);
    __m128i Em3OB = _mm_sub_epi32(Sum3EBeo, Sum3OB);
  __m128i Em4OA = _mm_sub_epi32(Sum4EAeo, Sum4OA);
    __m128i Em4OB = _mm_sub_epi32(Sum4EBeo, Sum4OB);

    EpOA = _mm_srai_epi32(_mm_add_epi32(EpOA, Add1st),7);
    EpOB = _mm_srai_epi32(_mm_add_epi32(EpOB, Add1st),7);
  Ep2OA = _mm_srai_epi32(_mm_add_epi32(Ep2OA, Add1st),7);
    Ep2OB = _mm_srai_epi32(_mm_add_epi32(Ep2OB, Add1st),7);
  Ep3OA = _mm_srai_epi32(_mm_add_epi32(Ep3OA, Add1st),7);
    Ep3OB = _mm_srai_epi32(_mm_add_epi32(Ep3OB, Add1st),7);
  Ep4OA = _mm_srai_epi32(_mm_add_epi32(Ep4OA, Add1st),7);
    Ep4OB = _mm_srai_epi32(_mm_add_epi32(Ep4OB, Add1st),7);

  EkOA = _mm_srai_epi32(_mm_add_epi32(EkOA, Add1st),7);
    EkOB = _mm_srai_epi32(_mm_add_epi32(EkOB, Add1st),7);
  Ek2OA = _mm_srai_epi32(_mm_add_epi32(Ek2OA, Add1st),7);
    Ek2OB = _mm_srai_epi32(_mm_add_epi32(Ek2OB, Add1st),7);
  Ek3OA = _mm_srai_epi32(_mm_add_epi32(Ek3OA, Add1st),7);
    Ek3OB = _mm_srai_epi32(_mm_add_epi32(Ek3OB, Add1st),7);
  Ek4OA = _mm_srai_epi32(_mm_add_epi32(Ek4OA, Add1st),7);
    Ek4OB = _mm_srai_epi32(_mm_add_epi32(Ek4OB, Add1st),7);

  ElOA = _mm_srai_epi32(_mm_add_epi32(ElOA, Add1st),7);
    ElOB = _mm_srai_epi32(_mm_add_epi32(ElOB, Add1st),7);
  El2OA = _mm_srai_epi32(_mm_add_epi32(El2OA, Add1st),7);
    El2OB = _mm_srai_epi32(_mm_add_epi32(El2OB, Add1st),7);
  El3OA = _mm_srai_epi32(_mm_add_epi32(El3OA, Add1st),7);
    El3OB = _mm_srai_epi32(_mm_add_epi32(El3OB, Add1st),7);
  El4OA = _mm_srai_epi32(_mm_add_epi32(El4OA, Add1st),7);
    El4OB = _mm_srai_epi32(_mm_add_epi32(El4OB, Add1st),7);

    EmOA = _mm_srai_epi32(_mm_add_epi32(EmOA, Add1st),7);
    EmOB = _mm_srai_epi32(_mm_add_epi32(EmOB, Add1st),7);
  Em2OA = _mm_srai_epi32(_mm_add_epi32(Em2OA, Add1st),7);
    Em2OB = _mm_srai_epi32(_mm_add_epi32(Em2OB, Add1st),7);
  Em3OA = _mm_srai_epi32(_mm_add_epi32(Em3OA, Add1st),7);
    Em3OB = _mm_srai_epi32(_mm_add_epi32(Em3OB, Add1st),7);
  Em4OA = _mm_srai_epi32(_mm_add_epi32(Em4OA, Add1st),7);
    Em4OB = _mm_srai_epi32(_mm_add_epi32(Em4OB, Add1st),7);
  
    Tmp2[4*i  ] = _mm_packs_epi32(EpOA, EpOB);
  Tmp2[4*i+1] = _mm_packs_epi32(Ep2OA, Ep2OB);
  Tmp2[4*i+2] = _mm_packs_epi32(Ep3OA, Ep3OB);
  Tmp2[4*i+3] = _mm_packs_epi32(Ep4OA, Ep4OB);

  Tmp2[60-4*i] = _mm_packs_epi32(ElOA,ElOB);
  Tmp2[60-4*i+1] = _mm_packs_epi32(El2OA,El2OB);
  Tmp2[60-4*i+2] = _mm_packs_epi32(El3OA,El3OB);
  Tmp2[60-4*i+3] = _mm_packs_epi32(El4OA,El4OB);

  Tmp2[64+4*i] = _mm_packs_epi32(EkOA,EkOB);
  Tmp2[64+4*i+1] = _mm_packs_epi32(Ek2OA,Ek2OB);
  Tmp2[64+4*i+2] = _mm_packs_epi32(Ek3OA,Ek3OB);
  Tmp2[64+4*i+3] = _mm_packs_epi32(Ek4OA,Ek4OB);

    Tmp2[124-4*i] = _mm_packs_epi32(EmOA, EmOB);     
  Tmp2[124-4*i+1] = _mm_packs_epi32(Em2OA, Em2OB); 
  Tmp2[124-4*i+2] = _mm_packs_epi32(Em3OA, Em3OB);     
  Tmp2[124-4*i+3] = _mm_packs_epi32(Em4OA, Em4OB); 
  }

  //horizontal transform and store
  for(int32 j=0; j<32; j++)
  {
    __m128i LineA = Tmp2[4*j];
    __m128i LineB = Tmp2[4*j+1];
  __m128i LineC = Tmp2[4*j+2];
    __m128i LineD = Tmp2[4*j+3];
  
    for(int32 i=0, k=0; i<128; i+=4, k++)
    {
    tr[k ] =_mm_add_epi32( _mm_add_epi32(_mm_madd_epi16(LineA, xiT32_DCT[i]), _mm_madd_epi16(LineB, xiT32_DCT[i+1])),_mm_add_epi32(_mm_madd_epi16(LineC, xiT32_DCT[i+2]), _mm_madd_epi16(LineD, xiT32_DCT[i+3])) );  
    }

    for(int32 i=0; i<16; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add2nd), Shift2nd);
  
    __m128i tr1=  _mm_packs_epi32(tr[0], tr[1]);
  __m128i tr2= _mm_packs_epi32(tr[2], tr[3]);
  __m128i tr3=  _mm_packs_epi32(tr[4], tr[5]);
  __m128i tr4= _mm_packs_epi32(tr[6], tr[7]);
  _mm_storeu_si128((__m128i*)Dst, tr1);
  _mm_storeu_si128((__m128i*)(Dst+8), tr2);
  _mm_storeu_si128((__m128i*)(Dst+16), tr3);
  _mm_storeu_si128((__m128i*)(Dst+24), tr4);
    Dst += DstStride; 
  }
}
void xTransform::InvTransformDCT_32x32_SSEv4(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth)//verical transform 2 symmetries, horizontal transform 0 symmetries. slower than iDCT_32x32_SSEv3
{
  
static const __m128i xiT32_DCT[128] = {
_mm_setr_epi16(64, 90, 90, 90, 89, 88, 87, 85),    _mm_setr_epi16(83, 82, 80, 78, 75, 73, 70, 67),      _mm_setr_epi16(64, 61, 57, 54, 50, 46, 43, 38),      _mm_setr_epi16(36, 31, 25, 22, 18, 13, 9, 4),
_mm_setr_epi16(64, 90, 87, 82, 75, 67, 57, 46),    _mm_setr_epi16(36, 22, 9, -4, -18, -31, -43, -54),    _mm_setr_epi16(-64, -73, -80, -85, -89, -90, -90, -88),  _mm_setr_epi16(-83, -78, -70, -61, -50, -38, -25, -13),
_mm_setr_epi16(64, 88, 80, 67, 50, 31, 9, -13),    _mm_setr_epi16(-36, -54, -70, -82, -89, -90, -87, -78),  _mm_setr_epi16(-64, -46, -25, -4, 18, 38, 57, 73),    _mm_setr_epi16(83, 90, 90, 85, 75, 61, 43, 22),
_mm_setr_epi16(64, 85, 70, 46, 18, -13, -43, -67),  _mm_setr_epi16(-83, -90, -87, -73, -50, -22, 9, 38),  _mm_setr_epi16(64, 82, 90, 88, 75, 54, 25, -4),      _mm_setr_epi16(-36, -61, -80, -90, -89, -78, -57, -31),
_mm_setr_epi16(64, 82, 57, 22, -18, -54, -80, -90),  _mm_setr_epi16(-83, -61, -25, 13, 50, 78, 90, 85),    _mm_setr_epi16(64, 31, -9, -46, -75, -90, -87, -67),  _mm_setr_epi16(-36, 4, 43, 73, 89, 88, 70, 38),
_mm_setr_epi16(64, 78, 43, -4, -50, -82, -90, -73),  _mm_setr_epi16(-36, 13, 57, 85, 89, 67, 25, -22),    _mm_setr_epi16(-64, -88, -87, -61, -18, 31, 70, 90),  _mm_setr_epi16(83, 54, 9, -38, -75, -90, -80, -46),
_mm_setr_epi16(64, 73, 25, -31, -75, -90, -70, -22),_mm_setr_epi16(36, 78, 90, 67, 18, -38, -80, -90),    _mm_setr_epi16(-64, -13, 43, 82, 89, 61, 9, -46),    _mm_setr_epi16(-83, -88, -57, -4, 50, 85, 87, 54),
_mm_setr_epi16(64, 67, 9, -54, -89, -78, -25, 38),  _mm_setr_epi16(83, 85, 43, -22, -75, -90, -57, 4),    _mm_setr_epi16(64, 90, 70, 13, -50, -88, -80, -31),    _mm_setr_epi16(36, 82, 87, 46, -18, -73, -90, -61),
_mm_setr_epi16(64, 61, -9, -73, -89, -46, 25, 82),  _mm_setr_epi16(83, 31, -43, -88, -75, -13, 57, 90),    _mm_setr_epi16(64, -4, -70, -90, -50, 22, 80, 85),    _mm_setr_epi16(36, -38, -87, -78, -18, 54, 90, 67),
_mm_setr_epi16(64, 54, -25, -85, -75, -4, 70, 88),  _mm_setr_epi16(36, -46, -90, -61, 18, 82, 80, 13),    _mm_setr_epi16(-64, -90, -43, 38, 89, 67, -9, -78),    _mm_setr_epi16(-83, -22, 57, 90, 50, -31, -87, -73),
_mm_setr_epi16(64, 46, -43, -90, -50, 38, 90, 54),  _mm_setr_epi16(-36, -90, -57, 31, 89, 61, -25, -88),  _mm_setr_epi16(-64, 22, 87, 67, -18, -85, -70, 13),    _mm_setr_epi16(83, 73, -9, -82, -75, 4, 80, 78),
_mm_setr_epi16(64, 38, -57, -88, -18, 73, 80, -4),  _mm_setr_epi16(-83, -67, 25, 90, 50, -46, -90, -31),  _mm_setr_epi16(64, 85, 9, -78, -75, 13, 87, 61),    _mm_setr_epi16(-36, -90, -43, 54, 89, 22, -70, -82),
_mm_setr_epi16(64, 31, -70, -78, 18, 90, 43, -61),  _mm_setr_epi16(-83, 4, 87, 54, -50, -88, -9, 82),    _mm_setr_epi16(64, -38, -90, -22, 75, 73, -25, -90),  _mm_setr_epi16(-36, 67, 80, -13, -89, -46, 57, 85),
_mm_setr_epi16(64, 22, -80, -61, 50, 85, -9, -90),  _mm_setr_epi16(-36, 73, 70, -38, -89, -4, 87, 46),    _mm_setr_epi16(-64, -78, 25, 90, 18, -82, -57, 54),    _mm_setr_epi16(83, -13, -90, -31, 75, 67, -43, -88),
_mm_setr_epi16(64, 13, -87, -38, 75, 61, -57, -78),  _mm_setr_epi16(36, 88, -9, -90, -18, 85, 43, -73),    _mm_setr_epi16(-64, 54, 80, -31, -89, 4, 90, 22),    _mm_setr_epi16(-83, -46, 70, 67, -50, -82, 25, 90),
_mm_setr_epi16(64, 4, -90, -13, 89, 22, -87, -31),  _mm_setr_epi16(83, 38, -80, -46, 75, 54, -70, -61),    _mm_setr_epi16(64, 67, -57, -73, 50, 78, -43, -82),    _mm_setr_epi16(36, 85, -25, -88, 18, 90, -9, -90),
_mm_setr_epi16(64, -4, -90, 13, 89, -22, -87, 31),  _mm_setr_epi16(83, -38, -80, 46, 75, -54, -70, 61),    _mm_setr_epi16(64, -67, -57, 73, 50, -78, -43, 82),    _mm_setr_epi16(36, -85, -25, 88, 18, -90, -9, 90),
_mm_setr_epi16(64, -13, -87, 38, 75, -61, -57, 78),  _mm_setr_epi16(36, -88, -9, 90, -18, -85, 43, 73),    _mm_setr_epi16(-64, -54, 80, 31, -89, -4, 90, -22),    _mm_setr_epi16(-83, 46, 70, -67, -50, 82, 25, -90),
_mm_setr_epi16(64, -22, -80, 61, 50, -85, -9, 90),  _mm_setr_epi16(-36, -73, 70, 38, -89, 4, 87, -46),    _mm_setr_epi16(-64, 78, 25, -90, 18, 82, -57, -54),    _mm_setr_epi16(83, 13, -90, 31, 75, -67, -43, 88),
_mm_setr_epi16(64, -31, -70, 78, 18, -90, 43, 61),  _mm_setr_epi16(-83, -4, 87, -54, -50, 88, -9, -82),    _mm_setr_epi16(64, 38, -90, 22, 75, -73, -25, 90),    _mm_setr_epi16(-36, -67, 80, 13, -89, 46, 57, -85),
_mm_setr_epi16(64, -38, -57, 88, -18, -73, 80, 4),  _mm_setr_epi16(-83, 67, 25, -90, 50, 46, -90, 31),    _mm_setr_epi16(64, -85, 9, 78, -75, -13, 87, -61),    _mm_setr_epi16(-36, 90, -43, -54, 89, -22, -70, 82),
_mm_setr_epi16(64, -46, -43, 90, -50, -38, 90, -54),_mm_setr_epi16(-36, 90, -57, -31, 89, -61, -25, 88),  _mm_setr_epi16(-64, -22, 87, -67, -18, 85, -70, -13),  _mm_setr_epi16(83, -73, -9, 82, -75, -4, 80, -78),
_mm_setr_epi16(64, -54, -25, 85, -75, 4, 70, -88),  _mm_setr_epi16(36, 46, -90, 61, 18, -82, 80, -13),    _mm_setr_epi16(-64, 90, -43, -38, 89, -67, -9, 78),    _mm_setr_epi16(-83, 22, 57, -90, 50, 31, -87, 73),
_mm_setr_epi16(64, -61, -9, 73, -89, 46, 25, -82),  _mm_setr_epi16(83, -31, -43, 88, -75, 13, 57, -90),    _mm_setr_epi16(64, 4, -70, 90, -50, -22, 80, -85),    _mm_setr_epi16(36, 38, -87, 78, -18, -54, 90, -67),
_mm_setr_epi16(64, -67, 9, 54, -89, 78, -25, -38),  _mm_setr_epi16(83, -85, 43, 22, -75, 90, -57, -4),    _mm_setr_epi16(64, -90, 70, -13, -50, 88, -80, 31),    _mm_setr_epi16(36, -82, 87, -46, -18, 73, -90, 61),
_mm_setr_epi16(64, -73, 25, 31, -75, 90, -70, 22),  _mm_setr_epi16(36, -78, 90, -67, 18, 38, -80, 90),    _mm_setr_epi16(-64, 13, 43, -82, 89, -61, 9, 46),    _mm_setr_epi16(-83, 88, -57, 4, 50, -85, 87, -54),
_mm_setr_epi16(64, -78, 43, 4, -50, 82, -90, 73),  _mm_setr_epi16(-36, -13, 57, -85, 89, -67, 25, 22),    _mm_setr_epi16(-64, 88, -87, 61, -18, -31, 70, -90),  _mm_setr_epi16(83, -54, 9, 38, -75, 90, -80, 46),
_mm_setr_epi16(64, -82, 57, -22, -18, 54, -80, 90),  _mm_setr_epi16(-83, 61, -25, -13, 50, -78, 90, -85),  _mm_setr_epi16(64, -31, -9, 46, -75, 90, -87, 67),    _mm_setr_epi16(-36, -4, 43, -73, 89, -88, 70, -38),
_mm_setr_epi16(64, -85, 70, -46, 18, 13, -43, 67),  _mm_setr_epi16(-83, 90, -87, 73, -50, 22, 9, -38),    _mm_setr_epi16(64, -82, 90, -88, 75, -54, 25, 4),    _mm_setr_epi16(-36, 61, -80, 90, -89, 78, -57, 31),
_mm_setr_epi16(64, -88, 80, -67, 50, -31, 9, 13),  _mm_setr_epi16(-36, 54, -70, 82, -89, 90, -87, 78),    _mm_setr_epi16(-64, 46, -25, 4, 18, -38, 57, -73),    _mm_setr_epi16(83, -90, 90, -85, 75, -61, 43, -22),
_mm_setr_epi16(64, -90, 87, -82, 75, -67, 57, -46),  _mm_setr_epi16(36, -22, 9, 4, -18, 31, -43, 54),    _mm_setr_epi16(-64, 73, -80, 85, -89, 90, -90, 88),    _mm_setr_epi16(-83, 78, -70, 61, -50, 38, -25, 13),
_mm_setr_epi16(64, -90, 90, -90, 89, -88, 87, -85),  _mm_setr_epi16(83, -82, 80, -78, 75, -73, 70, -67),    _mm_setr_epi16(64, -61, 57, -54, 50, -46, 43, -38),    _mm_setr_epi16(36, -31, 25, -22, 18, -13, 9, -4)};

  const int32 Shift2nd = 20 - BitDepth; 
  const __m128i Add1st  = _mm_set1_epi32(  64);
  const __m128i Add2nd  = _mm_set1_epi32(1<<(Shift2nd-1));

  __m128i Tmp[128];
  __m128i Tmp2[128];
  __m128i tr[32];
  //load
  for(int32 j=0; j<128; j+=4)
  {
    Tmp[j  ] = _mm_loadu_si128((__m128i*)(Src  ));
    Tmp[j+1] = _mm_loadu_si128((__m128i*)(Src+8));
    Tmp[j+2] = _mm_loadu_si128((__m128i*)(Src+16));
  Tmp[j+3] = _mm_loadu_si128((__m128i*)(Src+24));
    Src += 32;   
  } 
  
  //vertical transform  
  for(int32 i=0; i<4; i++) 
  {   
    // |1A 1B 2A 2B 3A 3B 4A 4B| - 32 samples

    __m128i Sum1QA= _mm_setzero_si128();
    __m128i Sum1QB= _mm_setzero_si128();
    __m128i Sum2QA= _mm_setzero_si128();
    __m128i Sum2QB= _mm_setzero_si128();
    __m128i Sum3QA= _mm_setzero_si128();
    __m128i Sum3QB= _mm_setzero_si128();
    __m128i Sum4QA= _mm_setzero_si128();
    __m128i Sum4QB= _mm_setzero_si128();

    __m128i Sum1WA= _mm_setzero_si128();
    __m128i Sum1WB= _mm_setzero_si128();
    __m128i Sum2WA= _mm_setzero_si128();
    __m128i Sum2WB= _mm_setzero_si128();
    __m128i Sum3WA= _mm_setzero_si128();
    __m128i Sum3WB= _mm_setzero_si128();
    __m128i Sum4WA= _mm_setzero_si128();
    __m128i Sum4WB= _mm_setzero_si128();

    __m128i Sum1EA= _mm_setzero_si128();
    __m128i Sum1EB= _mm_setzero_si128();
    __m128i Sum2EA= _mm_setzero_si128();
    __m128i Sum2EB= _mm_setzero_si128();
    __m128i Sum3EA= _mm_setzero_si128();
    __m128i Sum3EB= _mm_setzero_si128();
    __m128i Sum4EA= _mm_setzero_si128();
    __m128i Sum4EB= _mm_setzero_si128();

    __m128i Sum1RA= _mm_setzero_si128();
    __m128i Sum1RB= _mm_setzero_si128();
    __m128i Sum2RA= _mm_setzero_si128();
    __m128i Sum2RB= _mm_setzero_si128();
    __m128i Sum3RA= _mm_setzero_si128();
    __m128i Sum3RB= _mm_setzero_si128();
    __m128i Sum4RA= _mm_setzero_si128();
    __m128i Sum4RB= _mm_setzero_si128();

    __m128i Sum1TA= _mm_setzero_si128();
    __m128i Sum1TB= _mm_setzero_si128();
    __m128i Sum2TA= _mm_setzero_si128();
    __m128i Sum2TB= _mm_setzero_si128();
    __m128i Sum3TA= _mm_setzero_si128();
    __m128i Sum3TB= _mm_setzero_si128();
    __m128i Sum4TA= _mm_setzero_si128();
    __m128i Sum4TB= _mm_setzero_si128();

    __m128i Sum1YA= _mm_setzero_si128();
    __m128i Sum1YB= _mm_setzero_si128();
    __m128i Sum2YA= _mm_setzero_si128();
    __m128i Sum2YB= _mm_setzero_si128();
    __m128i Sum3YA= _mm_setzero_si128();
    __m128i Sum3YB= _mm_setzero_si128();
    __m128i Sum4YA= _mm_setzero_si128();
    __m128i Sum4YB= _mm_setzero_si128();

    __m128i Sum1UA= _mm_setzero_si128();
    __m128i Sum1UB= _mm_setzero_si128();
    __m128i Sum2UA= _mm_setzero_si128();
    __m128i Sum2UB= _mm_setzero_si128();
    __m128i Sum3UA= _mm_setzero_si128();
    __m128i Sum3UB= _mm_setzero_si128();
    __m128i Sum4UA= _mm_setzero_si128();
    __m128i Sum4UB= _mm_setzero_si128();

    __m128i Sum1IA= _mm_setzero_si128();
    __m128i Sum1IB= _mm_setzero_si128();
    __m128i Sum2IA= _mm_setzero_si128();
    __m128i Sum2IB= _mm_setzero_si128();
    __m128i Sum3IA= _mm_setzero_si128();
    __m128i Sum3IB= _mm_setzero_si128();
    __m128i Sum4IA= _mm_setzero_si128();
    __m128i Sum4IB= _mm_setzero_si128();

    __m128i Sum1OA= _mm_setzero_si128();
    __m128i Sum1OB= _mm_setzero_si128();
    __m128i Sum2OA= _mm_setzero_si128();
    __m128i Sum2OB= _mm_setzero_si128();
    __m128i Sum3OA= _mm_setzero_si128();
    __m128i Sum3OB= _mm_setzero_si128();
    __m128i Sum4OA= _mm_setzero_si128();
    __m128i Sum4OB= _mm_setzero_si128();

    
    for(int32 j=0; j<128; j+=32)
    {
    //line0
    {
    __m128i Line1=Tmp[j+0];
    __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+1];
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i TX = _mm_set1_epi32(m_TrM32x32[j>>2][i]);

    Line1A = _mm_mullo_epi32(Line1A, TX);
        Line1B = _mm_mullo_epi32(Line1B, TX);

      Line2A = _mm_mullo_epi32(Line2A, TX);
        Line2B = _mm_mullo_epi32(Line2B, TX);

    Sum1QA=_mm_add_epi32(Sum1QA,Line1A);
    Sum1QB=_mm_add_epi32(Sum1QB,Line1B);

    Sum2QA=_mm_add_epi32(Sum2QA,Line2A);
    Sum2QB=_mm_add_epi32(Sum2QB,Line2B);

    __m128i Line3=Tmp[j+2];
    __m128i Line3A = _mm_cvtepi16_epi32(Line3);
        __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line3,8));

    __m128i Line4 = Tmp[j+3];
        __m128i Line4A = _mm_cvtepi16_epi32(Line4);
        __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line4,8));

    Line3A = _mm_mullo_epi32(Line3A, TX);
        Line3B = _mm_mullo_epi32(Line3B, TX);

      Line4A = _mm_mullo_epi32(Line4A, TX);
        Line4B = _mm_mullo_epi32(Line4B, TX);

    Sum3QA=_mm_add_epi32(Sum3QA,Line3A);
    Sum3QB=_mm_add_epi32(Sum3QB,Line3B);

    Sum4QA=_mm_add_epi32(Sum4QA,Line4A);
    Sum4QB=_mm_add_epi32(Sum4QB,Line4B);
    }
    //line1
    {
    __m128i Line1=Tmp[j+4];

    __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+5];
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i TX = _mm_set1_epi32(m_TrM32x32[(j>>2)+1][i]);
    __m128i TXR = _mm_set1_epi32(m_TrM32x32[(j>>2)+1][15-i]);
    __m128i TXU = _mm_set1_epi32(m_TrM32x32[(j>>2)+1][7-i]);
    __m128i TXY = _mm_set1_epi32(m_TrM32x32[(j>>2)+1][8+i]);

    __m128i Line1AR = _mm_mullo_epi32(Line1A, TXR);
        __m128i Line1BR = _mm_mullo_epi32(Line1B, TXR);
    __m128i Line2AR = _mm_mullo_epi32(Line2A, TXR);
        __m128i Line2BR = _mm_mullo_epi32(Line2B, TXR);

    __m128i Line1AU = _mm_mullo_epi32(Line1A, TXU);
        __m128i Line1BU = _mm_mullo_epi32(Line1B, TXU);
    __m128i Line2AU = _mm_mullo_epi32(Line2A, TXU);
        __m128i Line2BU = _mm_mullo_epi32(Line2B, TXU);

    __m128i Line1AY = _mm_mullo_epi32(Line1A, TXY);
        __m128i Line1BY = _mm_mullo_epi32(Line1B, TXY);
    __m128i Line2AY = _mm_mullo_epi32(Line2A, TXY);
        __m128i Line2BY = _mm_mullo_epi32(Line2B, TXY);

    Line1A = _mm_mullo_epi32(Line1A, TX);
        Line1B = _mm_mullo_epi32(Line1B, TX);
      Line2A = _mm_mullo_epi32(Line2A, TX);
        Line2B = _mm_mullo_epi32(Line2B, TX);

    Sum1OA=_mm_add_epi32(Sum1OA,Line1A);
    Sum1OB=_mm_add_epi32(Sum1OB,Line1B);
    Sum2OA=_mm_add_epi32(Sum2OA,Line2A);
    Sum2OB=_mm_add_epi32(Sum2OB,Line2B);

    Sum1RA=_mm_add_epi32(Sum1RA,Line1AR);
    Sum1RB=_mm_add_epi32(Sum1RB,Line1BR);
    Sum2RA=_mm_add_epi32(Sum2RA,Line2AR);
    Sum2RB=_mm_add_epi32(Sum2RB,Line2BR);

    Sum1UA=_mm_add_epi32(Sum1UA,Line1AU);
    Sum1UB=_mm_add_epi32(Sum1UB,Line1BU);
    Sum2UA=_mm_add_epi32(Sum2UA,Line2AU);
    Sum2UB=_mm_add_epi32(Sum2UB,Line2BU);

    Sum1YA=_mm_add_epi32(Sum1YA,Line1AY);
    Sum1YB=_mm_add_epi32(Sum1YB,Line1BY);
    Sum2YA=_mm_add_epi32(Sum2YA,Line2AY);
    Sum2YB=_mm_add_epi32(Sum2YB,Line2BY);

    __m128i Line3=Tmp[j+6];
    __m128i Line3A = _mm_cvtepi16_epi32(Line3);
        __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line3,8));

    __m128i Line4 = Tmp[j+7];
        __m128i Line4A = _mm_cvtepi16_epi32(Line4);
        __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line4,8));

    __m128i Line3AR = _mm_mullo_epi32(Line3A, TXR);
        __m128i Line3BR = _mm_mullo_epi32(Line3B, TXR);
    __m128i Line4AR = _mm_mullo_epi32(Line4A, TXR);
        __m128i Line4BR = _mm_mullo_epi32(Line4B, TXR);

    __m128i Line3AU = _mm_mullo_epi32(Line3A, TXU);
        __m128i Line3BU = _mm_mullo_epi32(Line3B, TXU);
    __m128i Line4AU = _mm_mullo_epi32(Line4A, TXU);
        __m128i Line4BU = _mm_mullo_epi32(Line4B, TXU);

    __m128i Line3AY = _mm_mullo_epi32(Line3A, TXY);
        __m128i Line3BY = _mm_mullo_epi32(Line3B, TXY);
    __m128i Line4AY = _mm_mullo_epi32(Line4A, TXY);
        __m128i Line4BY = _mm_mullo_epi32(Line4B, TXY);

    Line3A = _mm_mullo_epi32(Line3A, TX);
        Line3B = _mm_mullo_epi32(Line3B, TX);
      Line4A = _mm_mullo_epi32(Line4A, TX);
        Line4B = _mm_mullo_epi32(Line4B, TX);

    Sum3OA=_mm_add_epi32(Sum3OA,Line3A);
    Sum3OB=_mm_add_epi32(Sum3OB,Line3B);
    Sum4OA=_mm_add_epi32(Sum4OA,Line4A);
    Sum4OB=_mm_add_epi32(Sum4OB,Line4B);

    Sum3RA=_mm_add_epi32(Sum3RA,Line3AR);
    Sum3RB=_mm_add_epi32(Sum3RB,Line3BR);
    Sum4RA=_mm_add_epi32(Sum4RA,Line4AR);
    Sum4RB=_mm_add_epi32(Sum4RB,Line4BR);

    Sum3UA=_mm_add_epi32(Sum3UA,Line3AU);
    Sum3UB=_mm_add_epi32(Sum3UB,Line3BU);
    Sum4UA=_mm_add_epi32(Sum4UA,Line4AU);
    Sum4UB=_mm_add_epi32(Sum4UB,Line4BU);

    Sum3YA=_mm_add_epi32(Sum3YA,Line3AY);
    Sum3YB=_mm_add_epi32(Sum3YB,Line3BY);
    Sum4YA=_mm_add_epi32(Sum4YA,Line4AY);
    Sum4YB=_mm_add_epi32(Sum4YB,Line4BY);
    }
    //line2
    {
    __m128i Line1=Tmp[j+8];
    __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+9];
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i TX  = _mm_set1_epi32(m_TrM32x32[(j>>2)+2][i]);
    __m128i TXI = _mm_set1_epi32(m_TrM32x32[(j>>2)+2][7-i]);

    __m128i Line1IA = _mm_mullo_epi32(Line1A, TXI);
        __m128i Line1IB = _mm_mullo_epi32(Line1B, TXI);
      __m128i Line2IA = _mm_mullo_epi32(Line2A, TXI);
        __m128i Line2IB = _mm_mullo_epi32(Line2B, TXI);

    Line1A = _mm_mullo_epi32(Line1A, TX);
        Line1B = _mm_mullo_epi32(Line1B, TX);
      Line2A = _mm_mullo_epi32(Line2A, TX);
        Line2B = _mm_mullo_epi32(Line2B, TX);

    Sum1EA=_mm_add_epi32(Sum1EA,Line1A);
    Sum1EB=_mm_add_epi32(Sum1EB,Line1B);
    Sum2EA=_mm_add_epi32(Sum2EA,Line2A);
    Sum2EB=_mm_add_epi32(Sum2EB,Line2B);

    Sum1IA=_mm_add_epi32(Sum1IA,Line1IA);
    Sum1IB=_mm_add_epi32(Sum1IB,Line1IB);
    Sum2IA=_mm_add_epi32(Sum2IA,Line2IA);
    Sum2IB=_mm_add_epi32(Sum2IB,Line2IB);

    __m128i Line3=Tmp[j+10];
    __m128i Line3A = _mm_cvtepi16_epi32(Line3);
        __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line3,8));

    __m128i Line4 = Tmp[j+11];
        __m128i Line4A = _mm_cvtepi16_epi32(Line4);
        __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line4,8));

    __m128i Line3IA = _mm_mullo_epi32(Line3A, TXI);
        __m128i Line3IB = _mm_mullo_epi32(Line3B, TXI);
      __m128i Line4IA = _mm_mullo_epi32(Line4A, TXI);
        __m128i Line4IB = _mm_mullo_epi32(Line4B, TXI);

    Line3A = _mm_mullo_epi32(Line3A, TX);
        Line3B = _mm_mullo_epi32(Line3B, TX);
      Line4A = _mm_mullo_epi32(Line4A, TX);
        Line4B = _mm_mullo_epi32(Line4B, TX);

    Sum3EA=_mm_add_epi32(Sum3EA,Line3A);
    Sum3EB=_mm_add_epi32(Sum3EB,Line3B);
    Sum4EA=_mm_add_epi32(Sum4EA,Line4A);
    Sum4EB=_mm_add_epi32(Sum4EB,Line4B);

    Sum3IA=_mm_add_epi32(Sum3IA,Line3IA);
    Sum3IB=_mm_add_epi32(Sum3IB,Line3IB);
    Sum4IA=_mm_add_epi32(Sum4IA,Line4IA);
    Sum4IB=_mm_add_epi32(Sum4IB,Line4IB);

    }
    //line3
    {
    __m128i Line1=Tmp[j+12];
    __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+13];
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i TX = _mm_set1_epi32(m_TrM32x32[(j>>2)+3][i]);
    __m128i TXT = _mm_set1_epi32(m_TrM32x32[(j>>2)+3][15-i]);
    __m128i TXU = _mm_set1_epi32(m_TrM32x32[(j>>2)+3][7-i]);
    __m128i TXY = _mm_set1_epi32(m_TrM32x32[(j>>2)+3][8+i]);

    __m128i Line1TA = _mm_mullo_epi32(Line1A, TXT);
        __m128i Line1TB = _mm_mullo_epi32(Line1B, TXT);
      __m128i Line2TA = _mm_mullo_epi32(Line2A, TXT);
        __m128i Line2TB = _mm_mullo_epi32(Line2B, TXT);

    __m128i Line1AU = _mm_mullo_epi32(Line1A, TXU);
        __m128i Line1BU = _mm_mullo_epi32(Line1B, TXU);
    __m128i Line2AU = _mm_mullo_epi32(Line2A, TXU);
        __m128i Line2BU = _mm_mullo_epi32(Line2B, TXU);

    __m128i Line1AY = _mm_mullo_epi32(Line1A, TXY);
        __m128i Line1BY = _mm_mullo_epi32(Line1B, TXY);
    __m128i Line2AY = _mm_mullo_epi32(Line2A, TXY);
        __m128i Line2BY = _mm_mullo_epi32(Line2B, TXY);

    Line1A = _mm_mullo_epi32(Line1A, TX);
        Line1B = _mm_mullo_epi32(Line1B, TX);
      Line2A = _mm_mullo_epi32(Line2A, TX);
        Line2B = _mm_mullo_epi32(Line2B, TX);

    Sum1OA=_mm_add_epi32(Sum1OA,Line1A);
    Sum1OB=_mm_add_epi32(Sum1OB,Line1B);
    Sum2OA=_mm_add_epi32(Sum2OA,Line2A);
    Sum2OB=_mm_add_epi32(Sum2OB,Line2B);

    Sum1TA=_mm_add_epi32(Sum1TA,Line1TA);
    Sum1TB=_mm_add_epi32(Sum1TB,Line1TB);
    Sum2TA=_mm_add_epi32(Sum2TA,Line2TA);
    Sum2TB=_mm_add_epi32(Sum2TB,Line2TB);

    Sum1UA=_mm_add_epi32(Sum1UA,Line1AU);
    Sum1UB=_mm_add_epi32(Sum1UB,Line1BU);
    Sum2UA=_mm_add_epi32(Sum2UA,Line2AU);
    Sum2UB=_mm_add_epi32(Sum2UB,Line2BU);

    Sum1YA=_mm_add_epi32(Sum1YA,Line1AY);
    Sum1YB=_mm_add_epi32(Sum1YB,Line1BY);
    Sum2YA=_mm_add_epi32(Sum2YA,Line2AY);
    Sum2YB=_mm_add_epi32(Sum2YB,Line2BY);

    __m128i Line3=Tmp[j+14];
    __m128i Line3A = _mm_cvtepi16_epi32(Line3);
        __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line3,8));

    __m128i Line4 = Tmp[j+15];
        __m128i Line4A = _mm_cvtepi16_epi32(Line4);
        __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line4,8));

    __m128i Line3TA = _mm_mullo_epi32(Line3A, TXT);
        __m128i Line3TB = _mm_mullo_epi32(Line3B, TXT);
      __m128i Line4TA = _mm_mullo_epi32(Line4A, TXT);
        __m128i Line4TB = _mm_mullo_epi32(Line4B, TXT);

    __m128i Line3AU = _mm_mullo_epi32(Line3A, TXU);
        __m128i Line3BU = _mm_mullo_epi32(Line3B, TXU);
    __m128i Line4AU = _mm_mullo_epi32(Line4A, TXU);
        __m128i Line4BU = _mm_mullo_epi32(Line4B, TXU);

    __m128i Line3AY = _mm_mullo_epi32(Line3A, TXY);
        __m128i Line3BY = _mm_mullo_epi32(Line3B, TXY);
    __m128i Line4AY = _mm_mullo_epi32(Line4A, TXY);
        __m128i Line4BY = _mm_mullo_epi32(Line4B, TXY);

    Line3A = _mm_mullo_epi32(Line3A, TX);
        Line3B = _mm_mullo_epi32(Line3B, TX);
      Line4A = _mm_mullo_epi32(Line4A, TX);
        Line4B = _mm_mullo_epi32(Line4B, TX);

    Sum3OA=_mm_add_epi32(Sum3OA,Line3A);
    Sum3OB=_mm_add_epi32(Sum3OB,Line3B);
    Sum4OA=_mm_add_epi32(Sum4OA,Line4A);
    Sum4OB=_mm_add_epi32(Sum4OB,Line4B);

    Sum3TA=_mm_add_epi32(Sum3TA,Line3TA);
    Sum3TB=_mm_add_epi32(Sum3TB,Line3TB);
    Sum4TA=_mm_add_epi32(Sum4TA,Line4TA);
    Sum4TB=_mm_add_epi32(Sum4TB,Line4TB);

    Sum3UA=_mm_add_epi32(Sum3UA,Line3AU);
    Sum3UB=_mm_add_epi32(Sum3UB,Line3BU);
    Sum4UA=_mm_add_epi32(Sum4UA,Line4AU);
    Sum4UB=_mm_add_epi32(Sum4UB,Line4BU);

    Sum3YA=_mm_add_epi32(Sum3YA,Line3AY);
    Sum3YB=_mm_add_epi32(Sum3YB,Line3BY);
    Sum4YA=_mm_add_epi32(Sum4YA,Line4AY);
    Sum4YB=_mm_add_epi32(Sum4YB,Line4BY);
    }
    //line4
    {
    __m128i Line1=Tmp[j+16];
    __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+17];
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i TX = _mm_set1_epi32(m_TrM32x32[(j>>2)+4][i]);

    Line1A = _mm_mullo_epi32(Line1A, TX);
        Line1B = _mm_mullo_epi32(Line1B, TX);
      Line2A = _mm_mullo_epi32(Line2A, TX);
        Line2B = _mm_mullo_epi32(Line2B, TX);

    Sum1WA=_mm_add_epi32(Sum1WA,Line1A);
    Sum1WB=_mm_add_epi32(Sum1WB,Line1B);
    Sum2WA=_mm_add_epi32(Sum2WA,Line2A);
    Sum2WB=_mm_add_epi32(Sum2WB,Line2B);

    __m128i Line3=Tmp[j+18];
    __m128i Line3A = _mm_cvtepi16_epi32(Line3);
        __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line3,8));

    __m128i Line4 = Tmp[j+19];
        __m128i Line4A = _mm_cvtepi16_epi32(Line4);
        __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line4,8));

    Line3A = _mm_mullo_epi32(Line3A, TX);
        Line3B = _mm_mullo_epi32(Line3B, TX);
      Line4A = _mm_mullo_epi32(Line4A, TX);
        Line4B = _mm_mullo_epi32(Line4B, TX);

    Sum3WA=_mm_add_epi32(Sum3WA,Line3A);
    Sum3WB=_mm_add_epi32(Sum3WB,Line3B);
    Sum4WA=_mm_add_epi32(Sum4WA,Line4A);
    Sum4WB=_mm_add_epi32(Sum4WB,Line4B);
    }
    //line5
    {
    __m128i Line1=Tmp[j+20];
    __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+21];
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i TX = _mm_set1_epi32(m_TrM32x32[(j>>2)+5][i]);
    __m128i TXR = _mm_set1_epi32(m_TrM32x32[(j>>2)+5][15-i]);
    __m128i TXU = _mm_set1_epi32(m_TrM32x32[(j>>2)+5][7-i]);
    __m128i TXY = _mm_set1_epi32(m_TrM32x32[(j>>2)+5][8+i]);

    __m128i Line1AR = _mm_mullo_epi32(Line1A, TXR);
        __m128i Line1BR = _mm_mullo_epi32(Line1B, TXR);
    __m128i Line2AR = _mm_mullo_epi32(Line2A, TXR);
        __m128i Line2BR = _mm_mullo_epi32(Line2B, TXR);

    __m128i Line1AU = _mm_mullo_epi32(Line1A, TXU);
        __m128i Line1BU = _mm_mullo_epi32(Line1B, TXU);
    __m128i Line2AU = _mm_mullo_epi32(Line2A, TXU);
        __m128i Line2BU = _mm_mullo_epi32(Line2B, TXU);

    __m128i Line1AY = _mm_mullo_epi32(Line1A, TXY);
        __m128i Line1BY = _mm_mullo_epi32(Line1B, TXY);
    __m128i Line2AY = _mm_mullo_epi32(Line2A, TXY);
        __m128i Line2BY = _mm_mullo_epi32(Line2B, TXY);

    Line1A = _mm_mullo_epi32(Line1A, TX);
        Line1B = _mm_mullo_epi32(Line1B, TX);
      Line2A = _mm_mullo_epi32(Line2A, TX);
        Line2B = _mm_mullo_epi32(Line2B, TX);

    Sum1OA=_mm_add_epi32(Sum1OA,Line1A);
    Sum1OB=_mm_add_epi32(Sum1OB,Line1B);
    Sum2OA=_mm_add_epi32(Sum2OA,Line2A);
    Sum2OB=_mm_add_epi32(Sum2OB,Line2B);

    Sum1RA=_mm_add_epi32(Sum1RA,Line1AR);
    Sum1RB=_mm_add_epi32(Sum1RB,Line1BR);
    Sum2RA=_mm_add_epi32(Sum2RA,Line2AR);
    Sum2RB=_mm_add_epi32(Sum2RB,Line2BR);

    Sum1UA=_mm_add_epi32(Sum1UA,Line1AU);
    Sum1UB=_mm_add_epi32(Sum1UB,Line1BU);
    Sum2UA=_mm_add_epi32(Sum2UA,Line2AU);
    Sum2UB=_mm_add_epi32(Sum2UB,Line2BU);

    Sum1YA=_mm_add_epi32(Sum1YA,Line1AY);
    Sum1YB=_mm_add_epi32(Sum1YB,Line1BY);
    Sum2YA=_mm_add_epi32(Sum2YA,Line2AY);
    Sum2YB=_mm_add_epi32(Sum2YB,Line2BY);

    __m128i Line3=Tmp[j+22];
    __m128i Line3A = _mm_cvtepi16_epi32(Line3);
        __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line3,8));

    __m128i Line4 = Tmp[j+23];
        __m128i Line4A = _mm_cvtepi16_epi32(Line4);
        __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line4,8));

    __m128i Line3AR = _mm_mullo_epi32(Line3A, TXR);
        __m128i Line3BR = _mm_mullo_epi32(Line3B, TXR);
    __m128i Line4AR = _mm_mullo_epi32(Line4A, TXR);
        __m128i Line4BR = _mm_mullo_epi32(Line4B, TXR);

    __m128i Line3AU = _mm_mullo_epi32(Line3A, TXU);
        __m128i Line3BU = _mm_mullo_epi32(Line3B, TXU);
    __m128i Line4AU = _mm_mullo_epi32(Line4A, TXU);
        __m128i Line4BU = _mm_mullo_epi32(Line4B, TXU);

    __m128i Line3AY = _mm_mullo_epi32(Line3A, TXY);
        __m128i Line3BY = _mm_mullo_epi32(Line3B, TXY);
    __m128i Line4AY = _mm_mullo_epi32(Line4A, TXY);
        __m128i Line4BY = _mm_mullo_epi32(Line4B, TXY);

    Line3A = _mm_mullo_epi32(Line3A, TX);
        Line3B = _mm_mullo_epi32(Line3B, TX);
      Line4A = _mm_mullo_epi32(Line4A, TX);
        Line4B = _mm_mullo_epi32(Line4B, TX);

    Sum3OA=_mm_add_epi32(Sum3OA,Line3A);
    Sum3OB=_mm_add_epi32(Sum3OB,Line3B);
    Sum4OA=_mm_add_epi32(Sum4OA,Line4A);
    Sum4OB=_mm_add_epi32(Sum4OB,Line4B);

    Sum3RA=_mm_add_epi32(Sum3RA,Line3AR);
    Sum3RB=_mm_add_epi32(Sum3RB,Line3BR);
    Sum4RA=_mm_add_epi32(Sum4RA,Line4AR);
    Sum4RB=_mm_add_epi32(Sum4RB,Line4BR);

    Sum3UA=_mm_add_epi32(Sum3UA,Line3AU);
    Sum3UB=_mm_add_epi32(Sum3UB,Line3BU);
    Sum4UA=_mm_add_epi32(Sum4UA,Line4AU);
    Sum4UB=_mm_add_epi32(Sum4UB,Line4BU);

    Sum3YA=_mm_add_epi32(Sum3YA,Line3AY);
    Sum3YB=_mm_add_epi32(Sum3YB,Line3BY);
    Sum4YA=_mm_add_epi32(Sum4YA,Line4AY);
    Sum4YB=_mm_add_epi32(Sum4YB,Line4BY);
    }

    //line6
    {
    __m128i Line1=Tmp[j+24];
    __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+25];
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i TX = _mm_set1_epi32(m_TrM32x32[(j>>2)+6][i]);
    __m128i TXI = _mm_set1_epi32(m_TrM32x32[(j>>2)+6][7-i]);

    __m128i Line1IA = _mm_mullo_epi32(Line1A, TXI);
        __m128i Line1IB = _mm_mullo_epi32(Line1B, TXI);
      __m128i Line2IA = _mm_mullo_epi32(Line2A, TXI);
        __m128i Line2IB = _mm_mullo_epi32(Line2B, TXI);

    Line1A = _mm_mullo_epi32(Line1A, TX);
        Line1B = _mm_mullo_epi32(Line1B, TX);
      Line2A = _mm_mullo_epi32(Line2A, TX);
        Line2B = _mm_mullo_epi32(Line2B, TX);

    Sum1EA=_mm_add_epi32(Sum1EA,Line1A);
    Sum1EB=_mm_add_epi32(Sum1EB,Line1B);
    Sum2EA=_mm_add_epi32(Sum2EA,Line2A);
    Sum2EB=_mm_add_epi32(Sum2EB,Line2B);

    Sum1IA=_mm_add_epi32(Sum1IA,Line1IA);
    Sum1IB=_mm_add_epi32(Sum1IB,Line1IB);
    Sum2IA=_mm_add_epi32(Sum2IA,Line2IA);
    Sum2IB=_mm_add_epi32(Sum2IB,Line2IB);

    __m128i Line3=Tmp[j+26];
    __m128i Line3A = _mm_cvtepi16_epi32(Line3);
        __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line3,8));

    __m128i Line4 = Tmp[j+27];
        __m128i Line4A = _mm_cvtepi16_epi32(Line4);
        __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line4,8));

    __m128i Line3IA = _mm_mullo_epi32(Line3A, TXI);
        __m128i Line3IB = _mm_mullo_epi32(Line3B, TXI);
      __m128i Line4IA = _mm_mullo_epi32(Line4A, TXI);
        __m128i Line4IB = _mm_mullo_epi32(Line4B, TXI);

    Line3A = _mm_mullo_epi32(Line3A, TX);
        Line3B = _mm_mullo_epi32(Line3B, TX);
      Line4A = _mm_mullo_epi32(Line4A, TX);
        Line4B = _mm_mullo_epi32(Line4B, TX);

    Sum3EA=_mm_add_epi32(Sum3EA,Line3A);
    Sum3EB=_mm_add_epi32(Sum3EB,Line3B);
    Sum4EA=_mm_add_epi32(Sum4EA,Line4A);
    Sum4EB=_mm_add_epi32(Sum4EB,Line4B);

    Sum3IA=_mm_add_epi32(Sum3IA,Line3IA);
    Sum3IB=_mm_add_epi32(Sum3IB,Line3IB);
    Sum4IA=_mm_add_epi32(Sum4IA,Line4IA);
    Sum4IB=_mm_add_epi32(Sum4IB,Line4IB);
    }
    //line7
    {
    __m128i Line1=Tmp[j+28];
    __m128i Line1A = _mm_cvtepi16_epi32(Line1);
        __m128i Line1B = _mm_cvtepi16_epi32(_mm_srli_si128( Line1,8));

    __m128i Line2 = Tmp[j+29];
        __m128i Line2A = _mm_cvtepi16_epi32(Line2);
        __m128i Line2B = _mm_cvtepi16_epi32(_mm_srli_si128( Line2,8));

    __m128i TX = _mm_set1_epi32(m_TrM32x32[(j>>2)+7][i]);
    __m128i TXT = _mm_set1_epi32(m_TrM32x32[(j>>2)+7][15-i]);
    __m128i TXU = _mm_set1_epi32(m_TrM32x32[(j>>2)+7][7-i]);
    __m128i TXY = _mm_set1_epi32(m_TrM32x32[(j>>2)+7][8+i]);

    __m128i Line1TA = _mm_mullo_epi32(Line1A, TXT);
        __m128i Line1TB = _mm_mullo_epi32(Line1B, TXT);
      __m128i Line2TA = _mm_mullo_epi32(Line2A, TXT);
        __m128i Line2TB = _mm_mullo_epi32(Line2B, TXT);

    __m128i Line1AU = _mm_mullo_epi32(Line1A, TXU);
        __m128i Line1BU = _mm_mullo_epi32(Line1B, TXU);
    __m128i Line2AU = _mm_mullo_epi32(Line2A, TXU);
        __m128i Line2BU = _mm_mullo_epi32(Line2B, TXU);

    __m128i Line1AY = _mm_mullo_epi32(Line1A, TXY);
        __m128i Line1BY = _mm_mullo_epi32(Line1B, TXY);
    __m128i Line2AY = _mm_mullo_epi32(Line2A, TXY);
        __m128i Line2BY = _mm_mullo_epi32(Line2B, TXY);

    Line1A = _mm_mullo_epi32(Line1A, TX);
        Line1B = _mm_mullo_epi32(Line1B, TX);
      Line2A = _mm_mullo_epi32(Line2A, TX);
        Line2B = _mm_mullo_epi32(Line2B, TX);

    Sum1OA=_mm_add_epi32(Sum1OA,Line1A);
    Sum1OB=_mm_add_epi32(Sum1OB,Line1B);
    Sum2OA=_mm_add_epi32(Sum2OA,Line2A);
    Sum2OB=_mm_add_epi32(Sum2OB,Line2B);

    Sum1TA=_mm_add_epi32(Sum1TA,Line1TA);
    Sum1TB=_mm_add_epi32(Sum1TB,Line1TB);
    Sum2TA=_mm_add_epi32(Sum2TA,Line2TA);
    Sum2TB=_mm_add_epi32(Sum2TB,Line2TB);

    Sum1UA=_mm_add_epi32(Sum1UA,Line1AU);
    Sum1UB=_mm_add_epi32(Sum1UB,Line1BU);
    Sum2UA=_mm_add_epi32(Sum2UA,Line2AU);
    Sum2UB=_mm_add_epi32(Sum2UB,Line2BU);

    Sum1YA=_mm_add_epi32(Sum1YA,Line1AY);
    Sum1YB=_mm_add_epi32(Sum1YB,Line1BY);
    Sum2YA=_mm_add_epi32(Sum2YA,Line2AY);
    Sum2YB=_mm_add_epi32(Sum2YB,Line2BY);

    __m128i Line3=Tmp[j+30];
    __m128i Line3A = _mm_cvtepi16_epi32(Line3);
        __m128i Line3B = _mm_cvtepi16_epi32(_mm_srli_si128( Line3,8));

    __m128i Line4 = Tmp[j+31];
        __m128i Line4A = _mm_cvtepi16_epi32(Line4);
        __m128i Line4B = _mm_cvtepi16_epi32(_mm_srli_si128( Line4,8));

    __m128i Line3TA = _mm_mullo_epi32(Line3A, TXT);
        __m128i Line3TB = _mm_mullo_epi32(Line3B, TXT);
      __m128i Line4TA = _mm_mullo_epi32(Line4A, TXT);
        __m128i Line4TB = _mm_mullo_epi32(Line4B, TXT);

    __m128i Line3AU = _mm_mullo_epi32(Line3A, TXU);
        __m128i Line3BU = _mm_mullo_epi32(Line3B, TXU);
    __m128i Line4AU = _mm_mullo_epi32(Line4A, TXU);
        __m128i Line4BU = _mm_mullo_epi32(Line4B, TXU);

    __m128i Line3AY = _mm_mullo_epi32(Line3A, TXY);
        __m128i Line3BY = _mm_mullo_epi32(Line3B, TXY);
    __m128i Line4AY = _mm_mullo_epi32(Line4A, TXY);
        __m128i Line4BY = _mm_mullo_epi32(Line4B, TXY);

    Line3A = _mm_mullo_epi32(Line3A, TX);
        Line3B = _mm_mullo_epi32(Line3B, TX);
      Line4A = _mm_mullo_epi32(Line4A, TX);
        Line4B = _mm_mullo_epi32(Line4B, TX);

    Sum3OA=_mm_add_epi32(Sum3OA,Line3A);
    Sum3OB=_mm_add_epi32(Sum3OB,Line3B);
    Sum4OA=_mm_add_epi32(Sum4OA,Line4A);
    Sum4OB=_mm_add_epi32(Sum4OB,Line4B);

    Sum3TA=_mm_add_epi32(Sum3TA,Line3TA);
    Sum3TB=_mm_add_epi32(Sum3TB,Line3TB);
    Sum4TA=_mm_add_epi32(Sum4TA,Line4TA);
    Sum4TB=_mm_add_epi32(Sum4TB,Line4TB);

    Sum3UA=_mm_add_epi32(Sum3UA,Line3AU);
    Sum3UB=_mm_add_epi32(Sum3UB,Line3BU);
    Sum4UA=_mm_add_epi32(Sum4UA,Line4AU);
    Sum4UB=_mm_add_epi32(Sum4UB,Line4BU);

    Sum3YA=_mm_add_epi32(Sum3YA,Line3AY);
    Sum3YB=_mm_add_epi32(Sum3YB,Line3BY);
    Sum4YA=_mm_add_epi32(Sum4YA,Line4AY);
    Sum4YB=_mm_add_epi32(Sum4YB,Line4BY);
    }
    }
  __m128i Sum1QWA= _mm_add_epi32(Sum1QA,Sum1WA);
  __m128i Sum1QWB= _mm_add_epi32(Sum1QB,Sum1WB);
  __m128i Sum2QWA= _mm_add_epi32(Sum2QA,Sum2WA);
  __m128i Sum2QWB= _mm_add_epi32(Sum2QB,Sum2WB);

  __m128i Dif1QWA= _mm_sub_epi32(Sum1QA,Sum1WA);
  __m128i Dif1QWB= _mm_sub_epi32(Sum1QB,Sum1WB);
  __m128i Dif2QWA= _mm_sub_epi32(Sum2QA,Sum2WA);
  __m128i Dif2QWB= _mm_sub_epi32(Sum2QB,Sum2WB);

  __m128i Sum3QWA= _mm_add_epi32(Sum3QA,Sum3WA);
  __m128i Sum3QWB= _mm_add_epi32(Sum3QB,Sum3WB);
  __m128i Sum4QWA= _mm_add_epi32(Sum4QA,Sum4WA);
  __m128i Sum4QWB= _mm_add_epi32(Sum4QB,Sum4WB);

  __m128i Dif3QWA= _mm_sub_epi32(Sum3QA,Sum3WA);
  __m128i Dif3QWB= _mm_sub_epi32(Sum3QB,Sum3WB);
  __m128i Dif4QWA= _mm_sub_epi32(Sum4QA,Sum4WA);
  __m128i Dif4QWB= _mm_sub_epi32(Sum4QB,Sum4WB); 
  
  //line 0
  __m128i Line0_1A= _mm_add_epi32(_mm_add_epi32(Sum1QWA,Sum1EA),Sum1OA);
  __m128i Line0_1B= _mm_add_epi32(_mm_add_epi32(Sum1QWB,Sum1EB),Sum1OB);
  __m128i Line0_2A= _mm_add_epi32(_mm_add_epi32(Sum2QWA,Sum2EA),Sum2OA);
  __m128i Line0_2B= _mm_add_epi32(_mm_add_epi32(Sum2QWB,Sum2EB),Sum2OB);
  __m128i Line0_3A= _mm_add_epi32(_mm_add_epi32(Sum3QWA,Sum3EA),Sum3OA);
  __m128i Line0_3B= _mm_add_epi32(_mm_add_epi32(Sum3QWB,Sum3EB),Sum3OB);
  __m128i Line0_4A= _mm_add_epi32(_mm_add_epi32(Sum4QWA,Sum4EA),Sum4OA);
  __m128i Line0_4B= _mm_add_epi32(_mm_add_epi32(Sum4QWB,Sum4EB),Sum4OB);

  //line 7
  __m128i Line7_1A= _mm_add_epi32(_mm_add_epi32(Dif1QWA,Sum1IA),Sum1UA);
  __m128i Line7_1B= _mm_add_epi32(_mm_add_epi32(Dif1QWB,Sum1IB),Sum1UB);
  __m128i Line7_2A= _mm_add_epi32(_mm_add_epi32(Dif2QWA,Sum2IA),Sum2UA);
  __m128i Line7_2B= _mm_add_epi32(_mm_add_epi32(Dif2QWB,Sum2IB),Sum2UB);
  __m128i Line7_3A= _mm_add_epi32(_mm_add_epi32(Dif3QWA,Sum3IA),Sum3UA);
  __m128i Line7_3B= _mm_add_epi32(_mm_add_epi32(Dif3QWB,Sum3IB),Sum3UB);
  __m128i Line7_4A= _mm_add_epi32(_mm_add_epi32(Dif4QWA,Sum4IA),Sum4UA);
  __m128i Line7_4B= _mm_add_epi32(_mm_add_epi32(Dif4QWB,Sum4IB),Sum4UB);

  //line 8
  __m128i Line8_1A= _mm_add_epi32(_mm_sub_epi32(Dif1QWA,Sum1IA),Sum1YA);
  __m128i Line8_1B= _mm_add_epi32(_mm_sub_epi32(Dif1QWB,Sum1IB),Sum1YB);
  __m128i Line8_2A= _mm_add_epi32(_mm_sub_epi32(Dif2QWA,Sum2IA),Sum2YA);
  __m128i Line8_2B= _mm_add_epi32(_mm_sub_epi32(Dif2QWB,Sum2IB),Sum2YB);
  __m128i Line8_3A= _mm_add_epi32(_mm_sub_epi32(Dif3QWA,Sum3IA),Sum3YA);
  __m128i Line8_3B= _mm_add_epi32(_mm_sub_epi32(Dif3QWB,Sum3IB),Sum3YB);
  __m128i Line8_4A= _mm_add_epi32(_mm_sub_epi32(Dif4QWA,Sum4IA),Sum4YA);
  __m128i Line8_4B= _mm_add_epi32(_mm_sub_epi32(Dif4QWB,Sum4IB),Sum4YB);

  //line 15
  __m128i Line15_1A= _mm_add_epi32(_mm_add_epi32(_mm_sub_epi32(Sum1QWA,Sum1EA),Sum1RA),Sum1TA);
  __m128i Line15_1B= _mm_add_epi32(_mm_add_epi32(_mm_sub_epi32(Sum1QWB,Sum1EB),Sum1RB),Sum1TB);
  __m128i Line15_2A= _mm_add_epi32(_mm_add_epi32(_mm_sub_epi32(Sum2QWA,Sum2EA),Sum2RA),Sum2TA);
  __m128i Line15_2B= _mm_add_epi32(_mm_add_epi32(_mm_sub_epi32(Sum2QWB,Sum2EB),Sum2RB),Sum2TB);
  __m128i Line15_3A= _mm_add_epi32(_mm_add_epi32(_mm_sub_epi32(Sum3QWA,Sum3EA),Sum3RA),Sum3TA);
  __m128i Line15_3B= _mm_add_epi32(_mm_add_epi32(_mm_sub_epi32(Sum3QWB,Sum3EB),Sum3RB),Sum3TB);
  __m128i Line15_4A= _mm_add_epi32(_mm_add_epi32(_mm_sub_epi32(Sum4QWA,Sum4EA),Sum4RA),Sum4TA);
  __m128i Line15_4B= _mm_add_epi32(_mm_add_epi32(_mm_sub_epi32(Sum4QWB,Sum4EB),Sum4RB),Sum4TB);
//-------------------------------------------------------------------------
  //line 16
  __m128i Line16_1A= _mm_sub_epi32(_mm_sub_epi32(_mm_sub_epi32(Sum1QWA,Sum1EA),Sum1RA),Sum1TA);
  __m128i Line16_1B= _mm_sub_epi32(_mm_sub_epi32(_mm_sub_epi32(Sum1QWB,Sum1EB),Sum1RB),Sum1TB);
  __m128i Line16_2A= _mm_sub_epi32(_mm_sub_epi32(_mm_sub_epi32(Sum2QWA,Sum2EA),Sum2RA),Sum2TA);
  __m128i Line16_2B= _mm_sub_epi32(_mm_sub_epi32(_mm_sub_epi32(Sum2QWB,Sum2EB),Sum2RB),Sum2TB);
  __m128i Line16_3A= _mm_sub_epi32(_mm_sub_epi32(_mm_sub_epi32(Sum3QWA,Sum3EA),Sum3RA),Sum3TA);
  __m128i Line16_3B= _mm_sub_epi32(_mm_sub_epi32(_mm_sub_epi32(Sum3QWB,Sum3EB),Sum3RB),Sum3TB);
  __m128i Line16_4A= _mm_sub_epi32(_mm_sub_epi32(_mm_sub_epi32(Sum4QWA,Sum4EA),Sum4RA),Sum4TA);
  __m128i Line16_4B= _mm_sub_epi32(_mm_sub_epi32(_mm_sub_epi32(Sum4QWB,Sum4EB),Sum4RB),Sum4TB);

  //line 23
  __m128i Line23_1A= _mm_sub_epi32(_mm_sub_epi32(Dif1QWA,Sum1IA),Sum1YA);
  __m128i Line23_1B= _mm_sub_epi32(_mm_sub_epi32(Dif1QWB,Sum1IB),Sum1YB);
  __m128i Line23_2A= _mm_sub_epi32(_mm_sub_epi32(Dif2QWA,Sum2IA),Sum2YA);
  __m128i Line23_2B= _mm_sub_epi32(_mm_sub_epi32(Dif2QWB,Sum2IB),Sum2YB);
  __m128i Line23_3A= _mm_sub_epi32(_mm_sub_epi32(Dif3QWA,Sum3IA),Sum3YA);
  __m128i Line23_3B= _mm_sub_epi32(_mm_sub_epi32(Dif3QWB,Sum3IB),Sum3YB);
  __m128i Line23_4A= _mm_sub_epi32(_mm_sub_epi32(Dif4QWA,Sum4IA),Sum4YA);
  __m128i Line23_4B= _mm_sub_epi32(_mm_sub_epi32(Dif4QWB,Sum4IB),Sum4YB);

  //line 24
  __m128i Line24_1A= _mm_sub_epi32(_mm_add_epi32(Dif1QWA,Sum1IA),Sum1UA);
  __m128i Line24_1B= _mm_sub_epi32(_mm_add_epi32(Dif1QWB,Sum1IB),Sum1UB);
  __m128i Line24_2A= _mm_sub_epi32(_mm_add_epi32(Dif2QWA,Sum2IA),Sum2UA);
  __m128i Line24_2B= _mm_sub_epi32(_mm_add_epi32(Dif2QWB,Sum2IB),Sum2UB);
  __m128i Line24_3A= _mm_sub_epi32(_mm_add_epi32(Dif3QWA,Sum3IA),Sum3UA);
  __m128i Line24_3B= _mm_sub_epi32(_mm_add_epi32(Dif3QWB,Sum3IB),Sum3UB);
  __m128i Line24_4A= _mm_sub_epi32(_mm_add_epi32(Dif4QWA,Sum4IA),Sum4UA);
  __m128i Line24_4B= _mm_sub_epi32(_mm_add_epi32(Dif4QWB,Sum4IB),Sum4UB);

  //line 31
  __m128i Line31_1A= _mm_sub_epi32(_mm_add_epi32(Sum1QWA,Sum1EA),Sum1OA);
  __m128i Line31_1B= _mm_sub_epi32(_mm_add_epi32(Sum1QWB,Sum1EB),Sum1OB);
  __m128i Line31_2A= _mm_sub_epi32(_mm_add_epi32(Sum2QWA,Sum2EA),Sum2OA);
  __m128i Line31_2B= _mm_sub_epi32(_mm_add_epi32(Sum2QWB,Sum2EB),Sum2OB);
  __m128i Line31_3A= _mm_sub_epi32(_mm_add_epi32(Sum3QWA,Sum3EA),Sum3OA);
  __m128i Line31_3B= _mm_sub_epi32(_mm_add_epi32(Sum3QWB,Sum3EB),Sum3OB);
  __m128i Line31_4A= _mm_sub_epi32(_mm_add_epi32(Sum4QWA,Sum4EA),Sum4OA);
  __m128i Line31_4B= _mm_sub_epi32(_mm_add_epi32(Sum4QWB,Sum4EB),Sum4OB);
  
  Line0_1A=_mm_srai_epi32(_mm_add_epi32(Line0_1A, Add1st),7);
  Line0_1B=_mm_srai_epi32(_mm_add_epi32(Line0_1B, Add1st),7);
  Line0_2A=_mm_srai_epi32(_mm_add_epi32(Line0_2A, Add1st),7);
  Line0_2B=_mm_srai_epi32(_mm_add_epi32(Line0_2B, Add1st),7);
  Line0_3A=_mm_srai_epi32(_mm_add_epi32(Line0_3A, Add1st),7);
  Line0_3B=_mm_srai_epi32(_mm_add_epi32(Line0_3B, Add1st),7);
  Line0_4A=_mm_srai_epi32(_mm_add_epi32(Line0_4A, Add1st),7);
  Line0_4B=_mm_srai_epi32(_mm_add_epi32(Line0_4B, Add1st),7);

  Line7_1A=_mm_srai_epi32(_mm_add_epi32(Line7_1A, Add1st),7);
  Line7_1B=_mm_srai_epi32(_mm_add_epi32(Line7_1B, Add1st),7);
  Line7_2A=_mm_srai_epi32(_mm_add_epi32(Line7_2A, Add1st),7);
  Line7_2B=_mm_srai_epi32(_mm_add_epi32(Line7_2B, Add1st),7);
  Line7_3A=_mm_srai_epi32(_mm_add_epi32(Line7_3A, Add1st),7);
  Line7_3B=_mm_srai_epi32(_mm_add_epi32(Line7_3B, Add1st),7);
  Line7_4A=_mm_srai_epi32(_mm_add_epi32(Line7_4A, Add1st),7);
  Line7_4B=_mm_srai_epi32(_mm_add_epi32(Line7_4B, Add1st),7);

  Line8_1A=_mm_srai_epi32(_mm_add_epi32(Line8_1A, Add1st),7);
  Line8_1B=_mm_srai_epi32(_mm_add_epi32(Line8_1B, Add1st),7);
  Line8_2A=_mm_srai_epi32(_mm_add_epi32(Line8_2A, Add1st),7);
  Line8_2B=_mm_srai_epi32(_mm_add_epi32(Line8_2B, Add1st),7);
  Line8_3A=_mm_srai_epi32(_mm_add_epi32(Line8_3A, Add1st),7);
  Line8_3B=_mm_srai_epi32(_mm_add_epi32(Line8_3B, Add1st),7);
  Line8_4A=_mm_srai_epi32(_mm_add_epi32(Line8_4A, Add1st),7);
  Line8_4B=_mm_srai_epi32(_mm_add_epi32(Line8_4B, Add1st),7);

  Line15_1A=_mm_srai_epi32(_mm_add_epi32(Line15_1A, Add1st),7);
  Line15_1B=_mm_srai_epi32(_mm_add_epi32(Line15_1B, Add1st),7);
  Line15_2A=_mm_srai_epi32(_mm_add_epi32(Line15_2A, Add1st),7);
  Line15_2B=_mm_srai_epi32(_mm_add_epi32(Line15_2B, Add1st),7);
  Line15_3A=_mm_srai_epi32(_mm_add_epi32(Line15_3A, Add1st),7);
  Line15_3B=_mm_srai_epi32(_mm_add_epi32(Line15_3B, Add1st),7);
  Line15_4A=_mm_srai_epi32(_mm_add_epi32(Line15_4A, Add1st),7);
  Line15_4B=_mm_srai_epi32(_mm_add_epi32(Line15_4B, Add1st),7);

  Line16_1A=_mm_srai_epi32(_mm_add_epi32(Line16_1A, Add1st),7);
  Line16_1B=_mm_srai_epi32(_mm_add_epi32(Line16_1B, Add1st),7);
  Line16_2A=_mm_srai_epi32(_mm_add_epi32(Line16_2A, Add1st),7);
  Line16_2B=_mm_srai_epi32(_mm_add_epi32(Line16_2B, Add1st),7);
  Line16_3A=_mm_srai_epi32(_mm_add_epi32(Line16_3A, Add1st),7);
  Line16_3B=_mm_srai_epi32(_mm_add_epi32(Line16_3B, Add1st),7);
  Line16_4A=_mm_srai_epi32(_mm_add_epi32(Line16_4A, Add1st),7);
  Line16_4B=_mm_srai_epi32(_mm_add_epi32(Line16_4B, Add1st),7);

  Line23_1A=_mm_srai_epi32(_mm_add_epi32(Line23_1A, Add1st),7);
  Line23_1B=_mm_srai_epi32(_mm_add_epi32(Line23_1B, Add1st),7);
  Line23_2A=_mm_srai_epi32(_mm_add_epi32(Line23_2A, Add1st),7);
  Line23_2B=_mm_srai_epi32(_mm_add_epi32(Line23_2B, Add1st),7);
  Line23_3A=_mm_srai_epi32(_mm_add_epi32(Line23_3A, Add1st),7);
  Line23_3B=_mm_srai_epi32(_mm_add_epi32(Line23_3B, Add1st),7);
  Line23_4A=_mm_srai_epi32(_mm_add_epi32(Line23_4A, Add1st),7);
  Line23_4B=_mm_srai_epi32(_mm_add_epi32(Line23_4B, Add1st),7);

  Line24_1A=_mm_srai_epi32(_mm_add_epi32(Line24_1A, Add1st),7);
  Line24_1B=_mm_srai_epi32(_mm_add_epi32(Line24_1B, Add1st),7);
  Line24_2A=_mm_srai_epi32(_mm_add_epi32(Line24_2A, Add1st),7);
  Line24_2B=_mm_srai_epi32(_mm_add_epi32(Line24_2B, Add1st),7);
  Line24_3A=_mm_srai_epi32(_mm_add_epi32(Line24_3A, Add1st),7);
  Line24_3B=_mm_srai_epi32(_mm_add_epi32(Line24_3B, Add1st),7);
  Line24_4A=_mm_srai_epi32(_mm_add_epi32(Line24_4A, Add1st),7);
  Line24_4B=_mm_srai_epi32(_mm_add_epi32(Line24_4B, Add1st),7);

  Line31_1A=_mm_srai_epi32(_mm_add_epi32(Line31_1A, Add1st),7);
  Line31_1B=_mm_srai_epi32(_mm_add_epi32(Line31_1B, Add1st),7);
  Line31_2A=_mm_srai_epi32(_mm_add_epi32(Line31_2A, Add1st),7);
  Line31_2B=_mm_srai_epi32(_mm_add_epi32(Line31_2B, Add1st),7);
  Line31_3A=_mm_srai_epi32(_mm_add_epi32(Line31_3A, Add1st),7);
  Line31_3B=_mm_srai_epi32(_mm_add_epi32(Line31_3B, Add1st),7);
  Line31_4A=_mm_srai_epi32(_mm_add_epi32(Line31_4A, Add1st),7);
  Line31_4B=_mm_srai_epi32(_mm_add_epi32(Line31_4B, Add1st),7);

  Tmp2[4*i  ] = _mm_packs_epi32(Line0_1A, Line0_1B);
  Tmp2[4*i+1] = _mm_packs_epi32(Line0_2A, Line0_2B);
  Tmp2[4*i+2] = _mm_packs_epi32(Line0_3A, Line0_3B);
  Tmp2[4*i+3] = _mm_packs_epi32(Line0_4A, Line0_4B);

  Tmp2[28-4*i  ] = _mm_packs_epi32(Line7_1A, Line7_1B);
  Tmp2[28-4*i+1] = _mm_packs_epi32(Line7_2A, Line7_2B);
  Tmp2[28-4*i+2] = _mm_packs_epi32(Line7_3A, Line7_3B);
  Tmp2[28-4*i+3] = _mm_packs_epi32(Line7_4A, Line7_4B);

  Tmp2[32+4*i  ] = _mm_packs_epi32(Line8_1A, Line8_1B);
  Tmp2[32+4*i+1] = _mm_packs_epi32(Line8_2A, Line8_2B);
  Tmp2[32+4*i+2] = _mm_packs_epi32(Line8_3A, Line8_3B);
  Tmp2[32+4*i+3] = _mm_packs_epi32(Line8_4A, Line8_4B);

  Tmp2[60-4*i]   = _mm_packs_epi32(Line15_1A, Line15_1B);
  Tmp2[60-4*i+1] = _mm_packs_epi32(Line15_2A, Line15_2B);
  Tmp2[60-4*i+2] = _mm_packs_epi32(Line15_3A, Line15_3B);
  Tmp2[60-4*i+3] = _mm_packs_epi32(Line15_4A, Line15_4B);

  Tmp2[64+4*i  ] = _mm_packs_epi32(Line16_1A, Line16_1B);
  Tmp2[64+4*i+1] = _mm_packs_epi32(Line16_2A, Line16_2B);
  Tmp2[64+4*i+2] = _mm_packs_epi32(Line16_3A, Line16_3B);
  Tmp2[64+4*i+3] = _mm_packs_epi32(Line16_4A, Line16_4B);

  Tmp2[92-4*i]   = _mm_packs_epi32(Line23_1A, Line23_1B);
  Tmp2[92-4*i+1] = _mm_packs_epi32(Line23_2A, Line23_2B);
  Tmp2[92-4*i+2] = _mm_packs_epi32(Line23_3A, Line23_3B);
  Tmp2[92-4*i+3] = _mm_packs_epi32(Line23_4A, Line23_4B);

    Tmp2[96+4*i  ] = _mm_packs_epi32(Line24_1A, Line24_1B);     
  Tmp2[96+4*i+1] = _mm_packs_epi32(Line24_2A, Line24_2B); 
  Tmp2[96+4*i+2] = _mm_packs_epi32(Line24_3A, Line24_3B);
  Tmp2[96+4*i+3] = _mm_packs_epi32(Line24_4A, Line24_4B);

  Tmp2[124-4*i]   = _mm_packs_epi32(Line31_1A, Line31_1B);
  Tmp2[124-4*i+1] = _mm_packs_epi32(Line31_2A, Line31_2B);
  Tmp2[124-4*i+2] = _mm_packs_epi32(Line31_3A, Line31_3B);
  Tmp2[124-4*i+3] = _mm_packs_epi32(Line31_4A, Line31_4B);
  }

//horizontal transform and store
  for(int32 j=0; j<32; j++)
  {
    __m128i LineA = Tmp2[4*j];
    __m128i LineB = Tmp2[4*j+1];
  __m128i LineC = Tmp2[4*j+2];
    __m128i LineD = Tmp2[4*j+3];
  
    for(int32 i=0, k=0; i<128; i+=4, k++)
    {
    tr[k ] =_mm_add_epi32( _mm_add_epi32(_mm_madd_epi16(LineA, xiT32_DCT[i]), _mm_madd_epi16(LineB, xiT32_DCT[i+1])),_mm_add_epi32(_mm_madd_epi16(LineC, xiT32_DCT[i+2]), _mm_madd_epi16(LineD, xiT32_DCT[i+3])) );  
    }

    for(int32 i=0; i<16; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_hadd_epi32(tr[2*i], tr[2*i+1]);
    for(int32 i=0; i<8; i++) tr[i] = _mm_srai_epi32(_mm_add_epi32(tr[i], Add2nd), Shift2nd);
  
    __m128i tr1=  _mm_packs_epi32(tr[0], tr[1]);
  __m128i tr2= _mm_packs_epi32(tr[2], tr[3]);
  __m128i tr3=  _mm_packs_epi32(tr[4], tr[5]);
  __m128i tr4= _mm_packs_epi32(tr[6], tr[7]);
  _mm_storeu_si128((__m128i*)Dst, tr1);
  _mm_storeu_si128((__m128i*)(Dst+8), tr2);
  _mm_storeu_si128((__m128i*)(Dst+16), tr3);
  _mm_storeu_si128((__m128i*)(Dst+24), tr4);
    Dst += DstStride; 
  }
}
void xTransform::InvTransformDCT_64x64_STD_M(int16* restrict Src, int16* restrict Dst, uint32 DstStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32 E[32],O[32];
  int32 EE[16],EO[16];
  int32 EEE[8],EEO[8];
  int32 EEEE[4],EEEO[4];
  int32 EEEEE[2],EEEEO[2];
  int32 Tmp[64][64]; 
  int32 Shift2nd = 20 - BitDepth; 
  int32 Add2nd   = 1<<(Shift2nd-1);

  for(int32 j=0; j<64; j++)
  {
    for (int32 k=0;k<32;k++)
    {
      O[k] = m_TrM64x64[ 1][k]*Src[ 1*64] + m_TrM64x64[ 3][k]*Src[ 3*64] + m_TrM64x64[ 5][k]*Src[ 5*64] + m_TrM64x64[ 7][k]*Src[ 7*64] + 
             m_TrM64x64[ 9][k]*Src[ 9*64] + m_TrM64x64[11][k]*Src[11*64] + m_TrM64x64[13][k]*Src[13*64] + m_TrM64x64[15][k]*Src[15*64] + 
             m_TrM64x64[17][k]*Src[17*64] + m_TrM64x64[19][k]*Src[19*64] + m_TrM64x64[21][k]*Src[21*64] + m_TrM64x64[23][k]*Src[23*64] + 
             m_TrM64x64[25][k]*Src[25*64] + m_TrM64x64[27][k]*Src[27*64] + m_TrM64x64[29][k]*Src[29*64] + m_TrM64x64[31][k]*Src[31*64] +
             m_TrM64x64[33][k]*Src[33*64] + m_TrM64x64[35][k]*Src[35*64] + m_TrM64x64[37][k]*Src[37*64] + m_TrM64x64[39][k]*Src[39*64] + 
             m_TrM64x64[41][k]*Src[41*64] + m_TrM64x64[43][k]*Src[43*64] + m_TrM64x64[45][k]*Src[45*64] + m_TrM64x64[47][k]*Src[47*64] + 
             m_TrM64x64[49][k]*Src[49*64] + m_TrM64x64[51][k]*Src[51*64] + m_TrM64x64[53][k]*Src[53*64] + m_TrM64x64[55][k]*Src[55*64] + 
             m_TrM64x64[57][k]*Src[57*64] + m_TrM64x64[59][k]*Src[59*64] + m_TrM64x64[61][k]*Src[61*64] + m_TrM64x64[63][k]*Src[63*64];
    }
    for (int32 k=0;k<16;k++)
    {
      EO[k] = m_TrM64x64[ 2][k]*Src[ 2*64] + m_TrM64x64[ 6][k]*Src[ 6*64] + m_TrM64x64[10][k]*Src[10*64] + m_TrM64x64[14][k]*Src[14*64] + 
              m_TrM64x64[18][k]*Src[18*64] + m_TrM64x64[22][k]*Src[22*64] + m_TrM64x64[26][k]*Src[26*64] + m_TrM64x64[30][k]*Src[30*64] +
              m_TrM64x64[34][k]*Src[34*64] + m_TrM64x64[38][k]*Src[38*64] + m_TrM64x64[42][k]*Src[42*64] + m_TrM64x64[46][k]*Src[46*64] + 
              m_TrM64x64[50][k]*Src[50*64] + m_TrM64x64[54][k]*Src[54*64] + m_TrM64x64[58][k]*Src[58*64] + m_TrM64x64[62][k]*Src[62*64];
    }
    for (int32 k=0;k<8;k++)
    {
      EEO[k] = m_TrM64x64[ 4][k]*Src[ 4*64] + m_TrM64x64[12][k]*Src[12*64] + m_TrM64x64[20][k]*Src[20*64] + m_TrM64x64[28][k]*Src[28*64]+
               m_TrM64x64[36][k]*Src[36*64] + m_TrM64x64[44][k]*Src[44*64] + m_TrM64x64[52][k]*Src[52*64] + m_TrM64x64[60][k]*Src[60*64];
    }

    for (int32 k=0;k<4;k++)
    {
      EEEO[k] = m_TrM64x64[8][k]*Src[8*64] + m_TrM64x64[24][k]*Src[24*64] + m_TrM64x64[40][k]*Src[40*64] + m_TrM64x64[56][k]*Src[56*64];
    }


    EEEEO[0] = m_TrM64x64[16][0]*Src[16*64] + m_TrM64x64[48][0]*Src[48*64];
    EEEEO[1] = m_TrM64x64[16][1]*Src[16*64] + m_TrM64x64[48][1]*Src[48*64];
    EEEEE[0] = m_TrM64x64[ 0][0]*Src[ 0*64] + m_TrM64x64[32][0]*Src[32*64];    
    EEEEE[1] = m_TrM64x64[ 0][1]*Src[ 0*64] + m_TrM64x64[32][1]*Src[32*64];

    Src++;

    EEEE[0] = EEEEE[0] + EEEEO[0];
    EEEE[3] = EEEEE[0] - EEEEO[0];
    EEEE[1] = EEEEE[1] + EEEEO[1];
    EEEE[2] = EEEEE[1] - EEEEO[1];    

    for (int32 k=0;k<4;k++)
    {
      EEE[k] = EEEE[k] + EEEO[k];
      EEE[k+4] = EEEE[3-k] - EEEO[3-k];
    }    
    for (int32 k=0;k<8;k++)
    {
      EE[k] = EEE[k] + EEO[k];
      EE[k+8] = EEE[7-k] - EEO[7-k];
    }    

    for (int32 k=0;k<16;k++)
    {
      E[k] = EE[k] + EO[k];
      E[k+16] = EE[15-k] - EO[15-k];
    }    

    for (int32 k=0;k<32;k++)
    {
      Tmp[j][k] = (E[k] + O[k] + 64)>>7;
      Tmp[j][k+32] = (E[31-k] - O[31-k] + 64)>>7;
    }
  }

  for(int32 j=0; j<64; j++)
  {
    for (int32 k=0;k<32;k++)
    {
      O[k] = m_TrM64x64[ 1][k]*Tmp[ 1][j] + m_TrM64x64[ 3][k]*Tmp[ 3][j] + m_TrM64x64[ 5][k]*Tmp[ 5][j] + m_TrM64x64[ 7][k]*Tmp[ 7][j] + 
        m_TrM64x64[ 9][k]*Tmp[ 9][j] + m_TrM64x64[11][k]*Tmp[11][j] + m_TrM64x64[13][k]*Tmp[13][j] + m_TrM64x64[15][k]*Tmp[15][j] + 
        m_TrM64x64[17][k]*Tmp[17][j] + m_TrM64x64[19][k]*Tmp[19][j] + m_TrM64x64[21][k]*Tmp[21][j] + m_TrM64x64[23][k]*Tmp[23][j] + 
        m_TrM64x64[25][k]*Tmp[25][j] + m_TrM64x64[27][k]*Tmp[27][j] + m_TrM64x64[29][k]*Tmp[29][j] + m_TrM64x64[31][k]*Tmp[31][j] +
        m_TrM64x64[33][k]*Tmp[33][j] + m_TrM64x64[35][k]*Tmp[35][j] + m_TrM64x64[37][k]*Tmp[37][j] + m_TrM64x64[39][k]*Tmp[39][j] + 
        m_TrM64x64[41][k]*Tmp[41][j] + m_TrM64x64[43][k]*Tmp[43][j] + m_TrM64x64[45][k]*Tmp[45][j] + m_TrM64x64[47][k]*Tmp[47][j] + 
        m_TrM64x64[49][k]*Tmp[49][j] + m_TrM64x64[51][k]*Tmp[51][j] + m_TrM64x64[53][k]*Tmp[53][j] + m_TrM64x64[55][k]*Tmp[55][j] + 
        m_TrM64x64[57][k]*Tmp[57][j] + m_TrM64x64[59][k]*Tmp[59][j] + m_TrM64x64[61][k]*Tmp[61][j] + m_TrM64x64[63][k]*Tmp[63][j];
    }
    for (int32 k=0;k<16;k++)
    {
      EO[k] = m_TrM64x64[ 2][k]*Tmp[ 2][j] + m_TrM64x64[ 6][k]*Tmp[ 6][j] + m_TrM64x64[10][k]*Tmp[10][j] + m_TrM64x64[14][k]*Tmp[14][j] + 
        m_TrM64x64[18][k]*Tmp[18][j] + m_TrM64x64[22][k]*Tmp[22][j] + m_TrM64x64[26][k]*Tmp[26][j] + m_TrM64x64[30][k]*Tmp[30][j] +
        m_TrM64x64[34][k]*Tmp[34][j] + m_TrM64x64[38][k]*Tmp[38][j] + m_TrM64x64[42][k]*Tmp[42][j] + m_TrM64x64[46][k]*Tmp[46][j] + 
        m_TrM64x64[50][k]*Tmp[50][j] + m_TrM64x64[54][k]*Tmp[54][j] + m_TrM64x64[58][k]*Tmp[58][j] + m_TrM64x64[62][k]*Tmp[62][j];
    }
    for (int32 k=0;k<8;k++)
    {
      EEO[k] = m_TrM64x64[4][k]*Tmp[4][j] + m_TrM64x64[12][k]*Tmp[12][j] + m_TrM64x64[20][k]*Tmp[20][j] + m_TrM64x64[28][k]*Tmp[28][j]+
        m_TrM64x64[36][k]*Tmp[36][j] + m_TrM64x64[44][k]*Tmp[44][j] + m_TrM64x64[52][k]*Tmp[52][j] + m_TrM64x64[60][k]*Tmp[60][j];
    }

    for (int32 k=0;k<4;k++)
    {
      EEEO[k] = m_TrM64x64[8][k]*Tmp[8][j] + m_TrM64x64[24][k]*Tmp[24][j] + m_TrM64x64[40][k]*Tmp[40][j] + m_TrM64x64[56][k]*Tmp[56][j];
    }

    EEEEO[0] = m_TrM64x64[16][0]*Tmp[16][j] + m_TrM64x64[48][0]*Tmp[48][j];
    EEEEO[1] = m_TrM64x64[16][1]*Tmp[16][j] + m_TrM64x64[48][1]*Tmp[48][j];
    EEEEE[0] = m_TrM64x64[0][0]*Tmp[0][j] + m_TrM64x64[32][0]*Tmp[32][j];    
    EEEEE[1] = m_TrM64x64[0][1]*Tmp[0][j] + m_TrM64x64[32][1]*Tmp[32][j];
    
    EEEE[0] = EEEEE[0] + EEEEO[0];
    EEEE[3] = EEEEE[0] - EEEEO[0];
    EEEE[1] = EEEEE[1] + EEEEO[1];
    EEEE[2] = EEEEE[1] - EEEEO[1];   

    for (int32 k=0;k<4;k++)
    {
      EEE[k] = EEEE[k] + EEEO[k];
      EEE[k+4] = EEEE[3-k] - EEEO[3-k];
    }    
    for (int32 k=0;k<8;k++)
    {
      EE[k] = EEE[k] + EEO[k];
      EE[k+8] = EEE[7-k] - EEO[7-k];
    }    

    for (int32 k=0;k<16;k++)
    {
      E[k] = EE[k] + EO[k];
      E[k+16] = EE[15-k] - EO[15-k];
    }    

    for (int32 k=0;k<32;k++)
    {
      Dst[k] = (E[k] + O[k] + Add2nd)>>Shift2nd;
      Dst[k+32] = (E[31-k] - O[31-k] + Add2nd)>>Shift2nd;
    }
    Dst += DstStride;;
  }
}

void xTransform::TransformDST_4x4_STD_C(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 DstStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32  C[5];
  int32 Tmp[4][4];
  int32 DstStride_x3 = DstStride + (DstStride<<1);
  int32 Shift1st = BitDepth - 7; //+1 per size
  int32 Add1st   = 1<<(Shift1st-1); 

  for(int32 j=0;j<4;j++) //horizontal transform
  {    
    C[0] = Src[0] + Src[3];
    C[1] = Src[1] + Src[3];
    C[2] = Src[0] - Src[1];
    C[3] = 74 * Src[2];
    C[4] = Src[0] + Src[1] - Src[3];
    Src += SrcStride;
    
    Tmp[0][j] = ( 29*C[0] + 55*C[1] + C[3] + Add1st)>>Shift1st; //store with transposition
    Tmp[1][j] = ( 74*C[4]                  + Add1st)>>Shift1st;
    Tmp[2][j] = ( 29*C[2] + 55*C[0] - C[3] + Add1st)>>Shift1st;
    Tmp[3][j] = ( 55*C[2] - 29*C[1] + C[3] + Add1st)>>Shift1st;    
  }

  for(int32 j=0;j<4;j++) //vertical transform
  {
    C[0] = Tmp[j][0] + Tmp[j][3];
    C[1] = Tmp[j][1] + Tmp[j][3];
    C[2] = Tmp[j][0] - Tmp[j][1];
    C[3] = 74 * Tmp[j][2];
    C[4] = Tmp[j][0] + Tmp[j][1] - Tmp[j][3];

    Dst[0           ]  = ( 29*C[0] + 55*C[1] + C[3] + 128)>>8; //store with transposition
    Dst[DstStride   ]  = ( 74*C[4]                  + 128)>>8;
    Dst[DstStride<<1]  = ( 29*C[2] + 55*C[0] - C[3] + 128)>>8;
    Dst[DstStride_x3]  = ( 55*C[2] - 29*C[1] + C[3] + 128)>>8;
    Dst++;
  }
}
void xTransform::TransformDST_4x4_SSE(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 DstStride, uint32 BitDepth) 
{  
  __m128i Tmp_0, Tmp_1;
  const int32 shift_1st = BitDepth - 7;
  const __m128i xT4_0_1 = _mm_setr_epi16(29, 55, 74, 84, 74, 74,  0,-74);
  const __m128i xT4_2_3 = _mm_setr_epi16(84,-29,-74, 55, 55,-84, 74,-29);
  const __m128i add_1st = _mm_set1_epi32(1<<(shift_1st-1));
  const __m128i Add2nd = _mm_set1_epi32(128);
  
  //load and horizontal transform
  {
    __m128i line_0 = _mm_loadl_epi64((__m128i*)Src);
    Src += SrcStride;
    __m128i line_1 = _mm_loadl_epi64((__m128i*)Src);
    Src += SrcStride;
    __m128i line_2 = _mm_loadl_epi64((__m128i*)Src);
    Src += SrcStride;
    __m128i line_3 = _mm_loadl_epi64((__m128i*)Src);

    line_0 = _mm_unpacklo_epi64(line_0, line_0);
    line_1 = _mm_unpacklo_epi64(line_1, line_1);
    line_2 = _mm_unpacklo_epi64(line_2, line_2);
    line_3 = _mm_unpacklo_epi64(line_3, line_3);

    __m128i t_0 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_0, xT4_0_1), _mm_madd_epi16(line_0, xT4_2_3)), add_1st), shift_1st);
    __m128i t_1 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_1, xT4_0_1), _mm_madd_epi16(line_1, xT4_2_3)), add_1st), shift_1st);
    __m128i t_2 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_2, xT4_0_1), _mm_madd_epi16(line_2, xT4_2_3)), add_1st), shift_1st);
    __m128i t_3 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_3, xT4_0_1), _mm_madd_epi16(line_3, xT4_2_3)), add_1st), shift_1st);
  
    Tmp_0 = _mm_packs_epi32(t_0, t_1);
    Tmp_1 = _mm_packs_epi32(t_2, t_3);
  }
  
  //transpose
  {
    __m128i tr_0 = _mm_unpacklo_epi16(Tmp_0, Tmp_1);
    __m128i tr_1 = _mm_unpackhi_epi16(Tmp_0, Tmp_1);  
    Tmp_0 = _mm_unpacklo_epi16(tr_0, tr_1);
    Tmp_1 = _mm_unpackhi_epi16(tr_0, tr_1);  
  }

  //vertical transforms
  {
    __m128i line_0 = _mm_unpacklo_epi64(Tmp_0, Tmp_0);
    __m128i line_1 = _mm_unpackhi_epi64(Tmp_0, Tmp_0);
    __m128i line_2 = _mm_unpacklo_epi64(Tmp_1, Tmp_1);
    __m128i line_3 = _mm_unpackhi_epi64(Tmp_1, Tmp_1);

    __m128i t_0 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_0, xT4_0_1), _mm_madd_epi16(line_0, xT4_2_3)), Add2nd), 8);
    __m128i t_1 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_1, xT4_0_1), _mm_madd_epi16(line_1, xT4_2_3)), Add2nd), 8);
    __m128i t_2 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_2, xT4_0_1), _mm_madd_epi16(line_2, xT4_2_3)), Add2nd), 8);
    __m128i t_3 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_3, xT4_0_1), _mm_madd_epi16(line_3, xT4_2_3)), Add2nd), 8);

    Tmp_0 = _mm_packs_epi32(t_0, t_1);
    Tmp_1 = _mm_packs_epi32(t_2, t_3);
  }

  //transpose
  {
    __m128i tr_0 = _mm_unpacklo_epi16(Tmp_0, Tmp_1);
    __m128i tr_1 = _mm_unpackhi_epi16(Tmp_0, Tmp_1);  
    Tmp_0 = _mm_unpacklo_epi16(tr_0, tr_1);
    Tmp_1 = _mm_unpackhi_epi16(tr_0, tr_1);
  }

  //store in random area with DstStride
  _mm_storel_epi64((__m128i*)Dst, Tmp_0);
  Tmp_0 = _mm_srli_si128(Tmp_0, 8);
  Dst += DstStride;
  _mm_storel_epi64((__m128i*)Dst, Tmp_0);
  Dst += DstStride;
  _mm_storel_epi64((__m128i*)Dst, Tmp_1);
  Tmp_1 = _mm_srli_si128(Tmp_1, 8);
  Dst += DstStride;
  _mm_storel_epi64((__m128i*)Dst, Tmp_1);
}
void xTransform::InvTransformDST_4x4_STD_C(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 DstStride, uint32 BitDepth) //Src and Dst could be the same
{
  int32  j;  
  int32  C[5];
  int32 Tmp[4][4];
  int32 SrcStride_x3 = SrcStride + (SrcStride<<1);
  int32 Shift2nd = 20 - BitDepth; 
  int32 Add2nd   = 1<<(Shift2nd-1); 

  for(j=0;j<4;j++) //vertical transform
  {
    C[0] = Src[0            ] + Src[SrcStride<<1]; //read with transposition
    C[1] = Src[SrcStride<<1] + Src[SrcStride_x3];
    C[2] = Src[0            ] - Src[SrcStride_x3];
    C[3] = 74*Src[SrcStride];
    C[4] = Src[0            ] - Src[SrcStride<<1]  + Src[SrcStride_x3];
    Src++;    

    Tmp[j][0]    = (29*C[0] + 55*C[1] + C[3] + 64)>>7;
    Tmp[j][1]    = (55*C[2] - 29*C[1] + C[3] + 64)>>7;
    Tmp[j][2]    = (74*C[4]                  + 64)>>7;
    Tmp[j][3]    = (55*C[0] + 29*C[2] - C[3] + 64)>>7;    
  }

  for(j=0;j<4;j++) //horizontal transform
  {
    C[0] = Tmp[0][j] + Tmp[2][j]; //read with transposition
    C[1] = Tmp[2][j] + Tmp[3][j];
    C[2] = Tmp[0][j] - Tmp[3][j];
    C[3] = 74*Tmp[1][j];
    C[4] = Tmp[0][j] - Tmp[2][j]  + Tmp[3][j];

    Dst[0]  = (29*C[0] + 55*C[1] + C[3] + Add2nd)>>Shift2nd;
    Dst[1]  = (55*C[2] - 29*C[1] + C[3] + Add2nd)>>Shift2nd;
    Dst[2]  = (74*C[4]                  + Add2nd)>>Shift2nd;
    Dst[3]  = (55*C[0] + 29*C[2] - C[3] + Add2nd)>>Shift2nd;
    Dst += DstStride;
  }
}
void xTransform::InvTransformDST_4x4_SSE(int16* restrict Src, int16* restrict Dst, uint32 SrcStride, uint32 DstStride, uint32 BitDepth) //Src and Dst could be the same
{
  __m128i Tmp_0, Tmp_1;
  const int32 Shift2nd = 20 - BitDepth; 
  const __m128i xiT4_0_1 = _mm_setr_epi16(29, 74, 84, 55, 55, 74,-29,-84);
  const __m128i xiT4_2_3 = _mm_setr_epi16(74,  0,-74, 74, 84,-74, 55,-29);
  const __m128i add_1st  = _mm_set1_epi32(  64);
  const __m128i Add2nd  = _mm_set1_epi32(1<<(Shift2nd-1));

  //load and transpose
  {
    __m128i line_0 = _mm_loadl_epi64((__m128i*)Src);
    Src += SrcStride;
    __m128i line_1 = _mm_loadl_epi64((__m128i*)Src);
    Src += SrcStride;
    __m128i line_2 = _mm_loadl_epi64((__m128i*)Src);
    Src += SrcStride;
    __m128i line_3 = _mm_loadl_epi64((__m128i*)Src);

    Tmp_0 = _mm_unpacklo_epi64(line_0, line_1);
    Tmp_1 = _mm_unpacklo_epi64(line_2, line_3);

    __m128i tr_0 = _mm_unpacklo_epi16(Tmp_0, Tmp_1);
    __m128i tr_1 = _mm_unpackhi_epi16(Tmp_0, Tmp_1);  
    Tmp_0 = _mm_unpacklo_epi16(tr_0, tr_1);
    Tmp_1 = _mm_unpackhi_epi16(tr_0, tr_1);
  }

  //vertical transforms
  {
    __m128i line_0 = _mm_unpacklo_epi64(Tmp_0, Tmp_0);
    __m128i line_1 = _mm_unpackhi_epi64(Tmp_0, Tmp_0);
    __m128i line_2 = _mm_unpacklo_epi64(Tmp_1, Tmp_1);
    __m128i line_3 = _mm_unpackhi_epi64(Tmp_1, Tmp_1);

    __m128i t_0 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_0, xiT4_0_1), _mm_madd_epi16(line_0, xiT4_2_3)), add_1st), 7);
    __m128i t_1 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_1, xiT4_0_1), _mm_madd_epi16(line_1, xiT4_2_3)), add_1st), 7);
    __m128i t_2 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_2, xiT4_0_1), _mm_madd_epi16(line_2, xiT4_2_3)), add_1st), 7);
    __m128i t_3 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_3, xiT4_0_1), _mm_madd_epi16(line_3, xiT4_2_3)), add_1st), 7);

    Tmp_0 = _mm_packs_epi32(t_0, t_1);
    Tmp_1 = _mm_packs_epi32(t_2, t_3);
  }

  //transpose
  {
    __m128i tr_0 = _mm_unpacklo_epi16(Tmp_0, Tmp_1);
    __m128i tr_1 = _mm_unpackhi_epi16(Tmp_0, Tmp_1);  
    Tmp_0 = _mm_unpacklo_epi16(tr_0, tr_1);
    Tmp_1 = _mm_unpackhi_epi16(tr_0, tr_1);  
  }

  //horizontal transforms
  {
    __m128i line_0 = _mm_unpacklo_epi64(Tmp_0, Tmp_0);
    __m128i line_1 = _mm_unpackhi_epi64(Tmp_0, Tmp_0);
    __m128i line_2 = _mm_unpacklo_epi64(Tmp_1, Tmp_1);
    __m128i line_3 = _mm_unpackhi_epi64(Tmp_1, Tmp_1);

    __m128i t_0 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_0, xiT4_0_1), _mm_madd_epi16(line_0, xiT4_2_3)), Add2nd), Shift2nd);
    __m128i t_1 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_1, xiT4_0_1), _mm_madd_epi16(line_1, xiT4_2_3)), Add2nd), Shift2nd);
    __m128i t_2 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_2, xiT4_0_1), _mm_madd_epi16(line_2, xiT4_2_3)), Add2nd), Shift2nd);
    __m128i t_3 = _mm_srai_epi32(_mm_add_epi32(_mm_hadd_epi32(_mm_madd_epi16(line_3, xiT4_0_1), _mm_madd_epi16(line_3, xiT4_2_3)), Add2nd), Shift2nd);

    Tmp_0 = _mm_packs_epi32(t_0, t_1);
    Tmp_1 = _mm_packs_epi32(t_2, t_3);
  }

  //store in random area with DstStride
  _mm_storel_epi64((__m128i*)Dst, Tmp_0);
  Tmp_0 = _mm_srli_si128(Tmp_0, 8);
  Dst += DstStride;
  _mm_storel_epi64((__m128i*)Dst, Tmp_0);
  Dst += DstStride;
  _mm_storel_epi64((__m128i*)Dst, Tmp_1);
  Tmp_1 = _mm_srli_si128(Tmp_1, 8);
  Dst += DstStride;
  _mm_storel_epi64((__m128i*)Dst, Tmp_1); 
}


//float x_cmp_sharpness(sCmp* src)
//{
//  //int32 x,y,dx,dy;
//  //int16 *s;
//  //int32 s_stride;
//
//  //_declspec(align(32)) int16 T[16];
//  //int32 F[2] = {0};
//
//  //dx=src->dx; 
//  //dy=src->dy;
//  //s_stride = src->pel_stride;
//
//  //for(y=0; y<dy; y+=4)
//  //{
//  //  s = src->pel_org + y*s_stride;
//  //  for(x=0; x<dx; x+=4)
//  //  {  
//  //    x_transform_DCT_4x4_SSE(s, T, s_stride, 8);
//  //    s+=4;
//
//  //    int32 DC = x_pow2((int32)T[ 0]);
//  //    int32 F0 = x_pow2((int32)T[ 2]) + x_pow2((int32)T[ 5]) + x_pow2((int32)T[ 8]);
//  //    int32 F1 = x_pow2((int32)T[ 3]) + x_pow2((int32)T[ 6]) + x_pow2((int32)T[ 9]) + x_pow2((int32)T[12]);
//
//  //  }
//  //}  
//  //
//  //F[0] /= (float)P;
//  //F[1] /= (float)P;
//
//  //printf("%f; %f; %d; ", F[0], F[1], P);
//
//  ////
//
//  //float Sharpness;
//  //if(F[0]<0.01f || F[1]<0.01f)
//  //{
//  //  Sharpness = 0.0f;
//  //}
//  //else
//  //{
//  //  Sharpness = F[0]+F[1];
//  //}
//
//  //return Sharpness;
//  return 0;
//}




} //end of namespace AVLib